<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html><head><title>wingolog</title><meta name="Generator" content="An unholy concoction of parenthetical guile" /><meta name="viewport" content="width=device-width" /><link rel="stylesheet" type="text/css" media="screen" href="/base.css" /><link rel="alternate" type="application/rss+xml" title="wingolog" href="/feed/atom" /></head><body><div id="rap"><h1 id="header"><a href="/">wingolog</a></h1><div id="navbar"><a href="/about/">about</a> | <a href="/projects/">projects</a> | <a href="/photos/">photos</a></div><div id="content"><div id="menu"><ul><li><h2><a href="/feed/atom">subscribe <img src="/wp-content/feed-icon-14x14.png" alt="[feed]" /></a></h2></li><li><h2>search</h2><form method="POST" action="/search"><input name="string" type="text" size="15" value="" /></form></li><li><h2>tags <a href="/tags">&gt;&gt;</a></h2><ul><li style="line-height: 150%"><a href="/tags/aikido" rel="tag" style="font-size: 91%">aikido</a> <a href="/tags/america" rel="tag" style="font-size: 94%">america</a> <a href="/tags/barcelona" rel="tag" style="font-size: 88%">barcelona</a> <a href="/tags/bike" rel="tag" style="font-size: 90%">bike</a> <a href="/tags/compilers" rel="tag" style="font-size: 123%">compilers</a> <a href="/tags/computers" rel="tag" style="font-size: 100%">computers</a> <a href="/tags/cps" rel="tag" style="font-size: 88%">cps</a> <a href="/tags/ecmascript" rel="tag" style="font-size: 92%">ecmascript</a> <a href="/tags/gnome" rel="tag" style="font-size: 127%">gnome</a> <a href="/tags/gnu" rel="tag" style="font-size: 116%">gnu</a> <a href="/tags/goops" rel="tag" style="font-size: 89%">goops</a> <a href="/tags/gstreamer" rel="tag" style="font-size: 98%">gstreamer</a> <a href="/tags/guadec" rel="tag" style="font-size: 92%">guadec</a> <a href="/tags/guile" rel="tag" style="font-size: 200%">guile</a> <a href="/tags/hacks" rel="tag" style="font-size: 93%">hacks</a> <a href="/tags/igalia" rel="tag" style="font-size: 133%">igalia</a> <a href="/tags/javascript" rel="tag" style="font-size: 109%">javascript</a> <a href="/tags/meta" rel="tag" style="font-size: 99%">meta</a> <a href="/tags/music" rel="tag" style="font-size: 99%">music</a> <a href="/tags/namibia" rel="tag" style="font-size: 124%">namibia</a> <a href="/tags/profiling" rel="tag" style="font-size: 88%">profiling</a> <a href="/tags/python" rel="tag" style="font-size: 88%">python</a> <a href="/tags/random" rel="tag" style="font-size: 114%">random</a> <a href="/tags/scheme" rel="tag" style="font-size: 198%">scheme</a> <a href="/tags/spain" rel="tag" style="font-size: 98%">spain</a> <a href="/tags/ssa" rel="tag" style="font-size: 89%">ssa</a> <a href="/tags/tekuti" rel="tag" style="font-size: 88%">tekuti</a> <a href="/tags/v8" rel="tag" style="font-size: 104%">v8</a> <a href="/tags/version%20control" rel="tag" style="font-size: 88%">version control</a> <a href="/tags/work" rel="tag" style="font-size: 92%">work</a></li></ul></li></ul></div><h2 class="storytitle"><a href="/archives/2018/02/07/design-notes-on-inline-caches-in-guile">design notes on inline caches in guile</a></h2><div class="post"><h3 class="meta"> 7 February 2018  3:14 PM (<a href="/tags/inline%20caches">inline caches</a> | <a href="/tags/compilers">compilers</a> | <a href="/tags/v8">v8</a> | <a href="/tags/igalia">igalia</a> | <a href="/tags/guile">guile</a> | <a href="/tags/gnu">gnu</a> | <a href="/tags/adaptive%20optimization">adaptive optimization</a> | <a href="/tags/type%20feedback">type feedback</a>)</h3><div class="storycontent"><div><p>Ahoy, programming-language tinkerfolk!  Today's rambling missive chews the gnarly bones of <a href="https://en.wikipedia.org/wiki/Inline_caching">&quot;inline caches&quot;</a>, in general but also with particular respect to the <a href="https://gnu.org/s/guile">Guile</a> implementation of Scheme.  First, a little intro.</p><p><b>inline what?</b></p><p>Inline caches are a language implementation technique used to accelerate polymorphic dispatch.  Let's dive in to that.</p><p>By <i>implementation technique</i>, I mean that the technique applies to the language compiler and runtime, rather than to the semantics of the language itself.  The effects on the language do exist though in an indirect way, in the sense that inline caches can make some operations faster and therefore more common.  Eventually inline caches can affect what users expect out of a language and what kinds of programs they write.</p><p>But I'm getting ahead of myself.  <i>Polymorphic dispatch</i> literally means &quot;choosing based on multiple forms&quot;.  Let's say your language has immutable strings -- like Java, Python, or Javascript.  Let's say your language also has operator overloading, and that it uses <tt>+</tt> to concatenate strings.  Well at that point you have a problem -- while you can specify a terse semantics of some core set of operations on strings (win!), you can't choose one representation of strings that will work well for all cases (lose!).  If the user has a workload where they regularly build up strings by concatenating them, you will want to store strings as trees of substrings.  On the other hand if they want to access <s>characters</s>codepoints by index, then you want an array.  But if the codepoints are all below 256, maybe you should represent them as bytes to save space, whereas maybe instead as 4-byte codepoints otherwise?  Or maybe even UTF-8 with a codepoint index side table.</p><p>The right representation (form) of a string depends on the myriad ways that the string might be used.  The <tt>string-append</tt> operation is <i>polymorphic</i>, in the sense that the precise code for the operator depends on the representation of the operands -- despite the fact that the <i>meaning</i> of <tt>string-append</tt> is monomorphic!</p><p>Anyway, that's the problem.  Before inline caches came along, there were two solutions: callouts and open-coding.  Both were bad in similar ways.  A callout is where the compiler generates a call to a generic runtime routine.  The runtime routine will be able to handle all the myriad forms and combination of forms of the operands.  This works fine but can be a bit slow, as all callouts for a given operator (e.g. <tt>string-append</tt>) dispatch to a single routine for the whole program, so they don't get to optimize for any particular call site.</p><p>One tempting thing for compiler writers to do is to effectively inline the <tt>string-append</tt> operation into each of its call sites.  This is &quot;open-coding&quot; (in the terminology of the early Lisp implementations like MACLISP).  The advantage here is that maybe the compiler knows something about one or more of the operands, so it can eliminate some cases, effectively performing some compile-time specialization.  But this is a limited technique; one could argue that the whole point of polymorphism is to allow for generic operations on generic data, so you rarely have compile-time invariants that can allow you to specialize.  Open-coding of polymorphic operations instead leads to code bloat, as the <tt>string-append</tt> operation is just so many copies of the same thing.</p><p>Inline caches emerged to solve this problem.  They trace their lineage back to Smalltalk 80, gained in complexity and power with Self and finally reached mass consciousness through Javascript.  These languages all share the characteristic of being dynamically typed and object-oriented.  When a user evaluates a statement like <tt>x = y.z</tt>, the language implementation needs to figure out where <tt>y.z</tt> is actually located.  This location depends on the representation of <tt>y</tt>, which is rarely known at compile-time.</p><p>However for any given reference <tt>y.z</tt> in the source code, there is a finite set of concrete representations of <tt>y</tt> that will actually flow to that call site at run-time.  Inline caches allow the language implementation to specialize the <tt>y.z</tt> access for its particular call site.  For example, at some point in the evaluation of a program, <tt>y</tt> may be seen to have representation R1 or R2.  For R1, the <tt>z</tt> property may be stored at offset 3 within the object's storage, and for R2 it might be at offset 4.  The inline cache is a bit of specialized code that compares the type of the object being accessed against R1 , in that case returning the value at offset 3, otherwise R2 and offset r4, and otherwise falling back to a generic routine.  If this isn't clear to you, Vyacheslav Egorov write a <a href="http://mrale.ph/blog/2012/06/03/explaining-js-vms-in-js-inline-caches.html">fine article describing and implementing the object representation optimizations enabled by inline caches</a>.</p><p>Inline caches also serve as input data to later stages of an adaptive compiler, allowing the compiler to selectively inline (open-code) only those cases that are appropriate to values actually seen at any given call site.</p><p><b>but how?</b></p><p>The classic formulation of inline caches from Self and early V8 actually patched the code being executed.  An inline cache might be allocated at address <tt>0xcabba9e5</tt> and the code emitted for its call-site would be <tt>jmp 0xcabba9e5</tt>.  If the inline cache ended up bottoming out to the generic routine, a new inline cache would be generated that added an implementation appropriate to the newly seen &quot;form&quot; of the operands and the call-site.  Let's say that new IC (inline cache) would have the address <tt>0x900db334</tt>.  Early versions of V8 would actually patch the machine code at the call-site to be <tt>jmp 0x900db334</tt> instead of <tt>jmp 0xcabba6e5</tt>.</p><p>Patching machine code has a number of disadvantages, though.  It inherently target-specific: you will need different strategies to patch x86-64 and armv7 machine code.  It's also expensive: you have to flush the instruction cache after the patch, which slows you down.  That is, of course, if you are allowed to patch executable code; on many systems that's impossible.  Writable machine code is a potential vulnerability if the system may be vulnerable to remote code execution.</p><p>Perhaps worst of all, though, patching machine code is not thread-safe.  In the case of early Javascript, this perhaps wasn't so important; but as JS implementations gained parallel garbage collectors and JS-level parallelism via &quot;service workers&quot;, this becomes less acceptable.</p><p>For all of these reasons, the modern take on inline caches is to implement them as a memory location that can be atomically modified.  The call site is just <tt>jmp *<i>loc</i></tt>, as if it were a virtual method call.  Modern CPUs have &quot;branch target buffers&quot; that predict the target of these indirect branches with very high accuracy so that the indirect jump does not become a pipeline stall.  (What does this mean in the face of the Spectre v2 vulnerabilities?  Sadly, God only knows at this point.  Saddest panda.)</p><p><b>cry, the beloved country</b></p><p>I am interested in ICs in the context of the Guile implementation of Scheme, but first I will make a digression.  Scheme is a very monomorphic language.  Yet, this monomorphism is entirely cultural.  It is in no way essential.  Lack of ICs in implementations has actually fed back and encouraged this monomorphism.</p><p>Let us take as an example the case of property access.  If you have a pair in Scheme and you want its first field, you do <tt>(car x)</tt>.  But if you have a vector, you do <tt>(vector-ref x 0)</tt>.</p><p>What's the reason for this nonuniformity?  You could have a generic <tt>ref</tt> procedure, which when invoked as <tt>(ref x 0)</tt> would return the field in <tt>x</tt> associated with 0.  Or <tt>(ref x 'foo)</tt> to return the <tt>foo</tt> property of <tt>x</tt>.  It would be more orthogonal in some ways, and it's completely valid Scheme.</p><p>We don't write Scheme programs this way, though.  From what I can tell, it's for two reasons: one good, and one bad.</p><p>The good reason is that saying <tt>vector-ref</tt> means more to the reader.  You know more about the complexity of the operation and what side effects it might have.  When you call <tt>ref</tt>, who knows?  Using concrete primitives allows for better program analysis and understanding.</p><p>The bad reason is that Scheme implementations, Guile included, tend to compile <tt>(car x)</tt> to much better code than <tt>(ref x 0)</tt>.  Scheme implementations in practice aren't well-equipped for polymorphic data access.  In fact it is standard Scheme practice to abuse the &quot;macro&quot; facility to manually inline code so that that certain performance-sensitive operations get inlined into a closed graph of monomorphic operators with no callouts.  To the extent that this is true, Scheme programmers, Scheme programs, and the Scheme language as a whole are all victims of their implementations.  JavaScript, for example, does not have this problem -- to a small extent, maybe, yes, performance tweaks and tuning are always a thing but JavaScript implementations' ability to burn away polymorphism and abstraction results in an entirely different character in JS programs versus Scheme programs.</p><p><b>it gets worse</b></p><p>On the most basic level, Scheme is the call-by-value lambda calculus.  It's well-studied, well-understood, and eminently flexible.  However the way that the syntax maps to the semantics hides a constrictive monomorphism: that the &quot;callee&quot; of a call refer to a lambda expression.</p><p>Concretely, in an expression like <tt>(a b)</tt>, in which <tt>a</tt> is not a macro, <tt>a</tt> must evaluate to the result of a <tt>lambda</tt> expression.  Perhaps by reference (e.g. <tt>(define a (lambda (x) x))</tt>), perhaps directly; but a lambda nonetheless.  But what if <tt>a</tt> is actually a vector?  At that point the Scheme language standard would declare that to be an error.</p><p>The semantics of Clojure, though, would allow for <tt>((vector 'a 'b 'c) 1)</tt> to evaluate to <tt>b</tt>.  Why not in Scheme?  There are the same good and bad reasons as with <tt>ref</tt>.  Usually, the concerns of the language implementation dominate, regardless of those of the users who generally want to write terse code.  Of course in some cases the implementation concerns <i>should</i> dominate, but not always.  Here, Scheme could be more flexible if it wanted to.</p><p><b>what have you done for me lately</b></p><p>Although inline caches are not a miracle cure for performance overheads of polymorphic dispatch, they are a tool in the box.  But what, precisely, can they do, both in general and for Scheme?</p><p>To my mind, they have five uses.  If you can think of more, please let me know in the comments.</p><p>Firstly, they have the classic named property access optimizations as in JavaScript.  These apply less to Scheme, as we don't have generic property access.  Perhaps this is a deficiency of Scheme, but it's not exactly low-hanging fruit.  Perhaps this would be more interesting if Guile had more generic protocols such as Racket's iteration.</p><p>Next, there are the arithmetic operators: addition, multiplication, and so on.  Scheme's arithmetic is indeed polymorphic; the addition operator <tt>+</tt> can add any number of complex numbers, with a distinction between exact and inexact values.  On a representation level, Guile has fixnums (small exact integers, no heap allocation), bignums (arbitrary-precision heap-allocated exact integers), fractions (exact ratios between integers), flonums (heap-allocated double-precision floating point numbers), and compnums (inexact complex numbers, internally a pair of doubles).  Also in Guile, arithmetic operators are a &quot;primitive generics&quot;, meaning that they can be extended to operate on new types at runtime via GOOPS.</p><p>The usual situation though is that any particular instance of an addition operator only sees fixnums.  In that case, it makes sense to only emit code for fixnums, instead of the product of all possible numeric representations.  This is a clear application where inline caches can be interesting to Guile.</p><p>Third, there is a very specific case related to dynamic linking.  Did you know that most programs compiled for GNU/Linux and related systems have inline caches in them?  It's a bit weird but the <a href="https://www.airs.com/blog/archives/41">&quot;Procedure Linkage Table&quot;</a> (PLT) segment in ELF binaries on Linux systems is set up in a way that when e.g. <tt>libfoo.so</tt> is loaded, the dynamic linker usually doesn't eagerly resolve all of the external routines that <tt>libfoo.so</tt> uses.  The first time that <tt>libfoo.so</tt> calls <tt>frobulate</tt>, it ends up calling a procedure that looks up the location of the <tt>frobulate</tt> procedure, then patches the binary code in the PLT so that the next time <tt>frobulate</tt> is called, it dispatches directly.  To dynamic language people it's the weirdest thing in the world that the C/C++/everything-static universe has at its cold, cold heart a hash table and a dynamic dispatch system that it doesn't expose to any kind of user for instrumenting or introspection -- any user that's not a malware author, of course.</p><p>But I digress!  Guile can use ICs to lazily resolve runtime routines used by compiled Scheme code.  But perhaps this isn't optimal, as the set of primitive runtime calls that Guile will embed in its output is finite, and so resolving these routines eagerly would probably be sufficient.  Guile could use ICs for inter-module references as well, and these should indeed be resolved lazily; but I don't know, perhaps the current strategy of using a call-site cache for inter-module references is sufficient.</p><p>Fourthly (are you counting?), there is a general case of the former:  when you see a call <tt>(a b)</tt> and you don't know what <tt>a</tt> is.  If you put an inline cache in the call, instead of having to emit checks that <tt>a</tt> is a heap object and a procedure and then emit an indirect call to the procedure's code, you might be able to emit simply a check that <tt>a</tt> is the same as <tt>x</tt>, the only callee you ever saw at that site, and in that case you can emit a direct branch to the function's code instead of an indirect branch.</p><p>Here I think the argument is less strong.  Modern CPUs are already very good at indirect jumps and well-predicted branches.  The value of a devirtualization pass in compilers is that it makes the side effects of a virtual method call concrete, allowing for more optimizations; avoiding indirect branches is good but not necessary.  On the other hand, Guile does have polymorphic callees (<a href="https://www.gnu.org/software/guile/docs/docs-2.0/guile-ref/Methods-and-Generic-Functions.html#Methods-and-Generic-Functions">generic functions</a>), and call ICs could help there.  Ideally though we would need to extend the language to allow generic functions to feed back to their inline cache handlers.</p><p>Finally, ICs could allow for cheap tracepoints and breakpoints.  If at every breakable location you included a <tt>jmp *<i>loc</i></tt>, and the initial value of <tt>*<i>loc</i></tt> was the next instruction, then you could patch individual locations with code to run there.  The patched code would be responsible for saving and restoring machine state around the instrumentation.</p><p>Honestly I struggle a lot with the idea of debugging native code.  GDB does the least-overhead, most-generic thing, which is patching code directly; but it runs from a separate process, and in Guile we need in-process portable debugging.  The debugging use case is a clear area where you want adaptive optimization, so that you can emit debugging ceremony from the hottest code, knowing that you can fall back on some earlier tier.  Perhaps Guile should bite the bullet and go this way too.</p><p><b>implementation plan</b></p><p>In Guile, monomorphic as it is in most things, probably only arithmetic is worth the trouble of inline caches, at least in the short term.</p><p>Another question is how much to specialize the inline caches to their call site.  On the extreme side, each call site could have a custom calling convention: if the first operand is in register A and the second is in register B and they are expected to be fixnums, and the result goes in register C, and the continuation is the code at L, well then you generate an inline cache that specializes to all of that.  No need to shuffle operands or results, no need to save the continuation (return location) on the stack.</p><p>The opposite would be to call ICs as if their were normal procedures:  shuffle arguments into fixed operand registers, push a stack frame, and when the IC returns, shuffle the result into place.</p><p>Honestly I am looking mostly to the simple solution.  I am concerned about code and heap bloat if I specify to every last detail of a call site.  Also maximum speed comes with an adaptive optimizer, and in that case simple lower tiers are best.</p><p><b>sanity check</b></p><p>To compare these impressions, I took a look at V8's current source code to see where they use ICs in practice.  When I worked on V8, the compiler was entirely different -- there were two tiers, and both of them generated native code.  Inline caches were everywhere, and they were gnarly; every architecture had its own implementation.  Now in V8 there are two tiers, not the same as the old ones, and the lowest one is a bytecode interpreter.</p><p>As an adaptive optimizer, V8 doesn't need breakpoint ICs.  It can always deoptimize back to the interpreter.  In actual practice, to debug at a source location, V8 will patch the bytecode to insert a &quot;DebugBreak&quot; instruction, which has its own support in the interpreter.  V8 also supports optimized compilation of this operation.  So, no ICs needed here.</p><p>Likewise for generic type feedback, V8 records types as data rather than in the classic formulation of inline caches as in Self.  I think WebKit's JavaScriptCore uses a similar strategy.</p><p>V8 does use inline caches for property access (loads and stores).  Besides that there is an inline cache used in calls which is just used to record callee counts, and not used for direct call optimization.</p><p>Surprisingly, V8 doesn't even seem to use inline caches for arithmetic (any more?).  Fair enough, I guess, given that JavaScript's numbers aren't very polymorphic, and even with a system with fixnums and heap floats like V8, floating-point numbers are rare in cold code.</p><p>The dynamic linking and relocation points don't apply to V8 either, as it doesn't receive binary code from the internet; it always starts from source.</p><p><b>twilight of the inline cache</b></p><p>There was a time when inline caches were recommended to solve all your VM problems, but it would seem now that their heyday is past.</p><p>ICs are still a win if you have named property access on objects whose shape you don't know at compile-time.  But improvements in CPU branch target buffers mean that it's no longer imperative to use ICs to avoid indirect branches (modulo Spectre v2), and creating direct branches via code-patching has gotten more expensive and tricky on today's targets with concurrency and deep cache hierarchies.</p><p>Besides that, the type feedback component of inline caches seems to be taken over by explicit data-driven call-site caches, rather than executable inline caches, and the highest-throughput tiers of an adaptive optimizer burn away inline caches anyway.  The pressure on an inline cache infrastructure now is towards simplicity and ease of type and call-count profiling, leaving the speed component to those higher tiers.</p><p>In Guile the bounded polymorphism on arithmetic combined with the need for ahead-of-time compilation means that ICs are probably a code size and execution time win, but it will take some engineering to prevent the calling convention overhead from dominating cost.</p><p>Time to experiment, then -- I'll let y'all know how it goes.  Thoughts and feedback welcome from the compilerati.  Until then, happy hacking :)</p></div></div><div class="feedback"><a href="/archives/2018/02/07/design-notes-on-inline-caches-in-guile#comments">(2)</a></div></div><h2 class="storytitle"><a href="/archives/2018/02/05/notes-from-the-fosdem-2018-networking-devroom">notes from the fosdem 2018 networking devroom</a></h2><div class="post"><h3 class="meta"> 5 February 2018  5:22 PM (<a href="/tags/fosdem">fosdem</a> | <a href="/tags/networking">networking</a> | <a href="/tags/userspace">userspace</a> | <a href="/tags/snabb">snabb</a> | <a href="/tags/vpp">vpp</a> | <a href="/tags/dpdk">dpdk</a> | <a href="/tags/igalia">igalia</a>)</h3><div class="storycontent"><div><p>Greetings, internet!</p><p>I am on my way back from <a href="https://fosdem.org/">FOSDEM</a> and thought I would share with yall some impressions from talks in the <a href="https://fosdem.org/2018/schedule/track/sdn_and_nfv/">Networking devroom</a>.  I didn't get to go to all that many talks -- FOSDEM's hallway track is the hottest of them all -- but I did hit a select few.  Thanks to Dave Neary at Red Hat for organizing the room.</p><p><b>Ray Kinsella -- Intel -- <a href="https://fosdem.org/2018/schedule/event/dpdk_microservices/">The path to data-plane micro-services</a></b></p><p>The day started with a drum-beating talk that was very light on technical information.</p><p>Essentially Ray was arguing for an evolution of network function virtualization -- that instead of running VNFs on bare metal as was done in the days of yore, that people started to run them in virtual machines, and now they run them in containers -- what's next?  Ray is saying that &quot;cloud-native VNFs&quot; are the next step.</p><p>Cloud-native VNFs to move from &quot;greedy&quot; VNFs that take charge of the cores that are available to them, to some kind of resource sharing.  &quot;Maybe users value flexibility over performance&quot;, says Ray.  It's the Care Bears approach to networking: (resource) sharing is caring.</p><p>In practice he proposed two ways that VNFs can map to cores and cards.</p><p>One was in-process sharing, which if I understood him properly was actually as nodes running within a VPP process.  Basically in this case VPP or DPDK is the scheduler and multiplexes two or more network functions in one process.</p><p>The other was letting Linux schedule separate processes.  In networking, we don't usually do it this way: <a href="https://github.com/snabbco/snabb/blob/master/src/program/lwaftr/doc/performance.md">we run network functions on dedicated cores on which nothing else runs</a>.  Ray was suggesting that perhaps network functions could be more like &quot;normal&quot; Linux services.  Ray doesn't know if Linux scheduling will work in practice.  Also it might mean allowing DPDK to work with 4K pages instead of the 2M hugepages it currently requires.  This obviously has the potential for more latency hazards and would need some tighter engineering, and ultimately would have fewer guarantees than the &quot;greedy&quot; approach.</p><p>Interesting side things I noticed:</p><ul>
<li><p>All the diagrams show Kubernetes managing CPU node allocation and interface assignment.  I guess in marketing diagrams, Kubernetes has completely replaced OpenStack.</p></li>
<li><p>One slide showed guest VNFs differentiated between &quot;virtual network functions&quot; and &quot;socket-based applications&quot;, the latter ones being the legacy services that use kernel APIs.  It's a useful terminology difference.</p></li>
<li><p>The talk identifies user-space networking with DPDK (only!).</p></li>
</ul><p>Finally, I note that <a href="https://www.wingolog.org/archives/2015/11/09/embracing-conways-law">Conway's law</a> is obviously reflected in the performance overheads: because there are organizational isolations between dev teams, vendors, and users, there are big technical barriers between them too.  The least-overhead forms of resource sharing are also those with the highest technical consistency and integration (nodes in a single VPP instance).</p><p><b>Magnus Karlsson -- Intel -- <a href="https://fosdem.org/2018/schedule/event/af_xdp/">AF_XDP</a></b></p><p>This was a talk about getting good throughput from the NIC to userspace, but by using some kernel facilities.  The idea is to get the kernel to set up the NIC and virtualize the transmit and receive ring buffers, but to let the NIC's DMA'd packets go directly to userspace.</p><p>The performance goal is 40Gbps for thousand-byte packets, or 25 Gbps for traffic with only the smallest packets (64 bytes).  The fast path does &quot;zero copy&quot; on the packets if the hardware has the capability to steer the subset of traffic associated with the AF_XDP socket to that particular process.</p><p>The AF_XDP project builds on <a href="https://www.iovisor.org/technology/xdp">XDP</a>, a newish thing where a little kind of bytecode can run on the kernel or possibly on the NIC.  One of the bytecode commands (REDIRECT) causes packets to be forwarded to user-space instead of handled by the kernel's otherwise heavyweight networking stack.  AF_XDP is the bridge between XDP on the kernel side and an interface to user-space using sockets (as opposed to e.g. AF_INET).  The performance goal was to be within 10% or so of DPDK's raw user-space-only performance.</p><p>The benefits of AF_XDP over the current situation would be that you have just one device driver, in the kernel, rather than having to have one driver in the kernel (which you have to have anyway) and one in user-space (for speed).  Also, with the kernel involved, there is a possibility for better isolation between different processes or containers, when compared with raw PCI access from user-space..</p><p>AF_XDP is what was previously known as AF_PACKET v4, and its numbers are looking somewhat OK.  Though it's not upstream yet, it might be interesting to get a Snabb driver here.</p><p>I would note that kernel-userspace cooperation is a bit of a theme these days.  There are other points of potential cooperation or common domain sharing, storage being an obvious one.  However I heard more than once this weekend the kind of &quot;I don't know, that area of the kernel has a different culture&quot; sort of concern as that highlighted by Daniel Vetter in his <a href="https://lwn.net/Articles/745817/">recent LCA talk</a>.</p><p><b>François-Frédéric Ozog -- Linaro -- <a href="https://fosdem.org/2018/schedule/event/netmdev/">Userland Network I/O</a></b></p><p>This talk is hard to summarize.  Like the previous one, it's again about getting packets to userspace with some support from the kernel, but the speaker went really deep and I'm not quite sure what in the talk is new and what is known.</p><p>François-Frédéric is working on a new set of abstractions for relating the kernel and user-space.  He works on OpenDataPlane (ODP), which is kinda like DPDK in some ways.  ARM seems to be a big target for his work; that x86-64 is also a target goes without saying.</p><p>His problem statement was, how should we enable fast userland network I/O, without duplicating drivers?</p><p>François-Frédéric was a bit negative on AF_XDP because (he says) it is so focused on packets that it neglects other kinds of devices with similar needs, such as crypto accelerators.  Apparently the challenge here is accelerating a single large IPsec tunnel -- because the cryptographic operations are serialized, you need good single-core performance, and making use of hardware accelerators seems necessary right now for even a single 10Gbps stream.  (If you had many tunnels, you could parallelize, but that's not the case here.)</p><p>He was also a bit skeptical about standardizing on the &quot;packet array I/O model&quot; which AF_XDP and most NICS use.  What he means here is that most current NICs move packets to and from main memory with the help of a &quot;descriptor array&quot; ring buffer that holds pointers to packets.  A transmit array stores packets ready to transmit; a receive array stores maximum-sized packet buffers ready to be filled by the NIC.  The packet data itself is somewhere else in memory; the descriptor only points to it.  When a new packet is received, the NIC fills the corresponding packet buffer and then updates the &quot;descriptor array&quot; to point to the newly available packet.  This requires at least two memory writes from the NIC to memory: at least one to write the packet data (one per 64 bytes of packet data), and one to update the DMA descriptor with the packet length and possible other metadata.</p><p>Although these writes go directly to cache, there's a limit to the number of DMA operations that can happen per second, and with 100Gbps cards, we can't afford to make one such transaction per packet.</p><p>François-Frédéric promoted an alternative I/O model for high-throughput use cases: the &quot;tape I/O model&quot;, where packets are just written back-to-back in a uniform array of memory.  Every so often a block of memory containing some number of packets is made available to user-space.  This has the advantage of packing in more packets per memory block, as there's no wasted space between packets.  This increases cache density and decreases DMA transaction count for transferring packet data, as we can use each 64-byte DMA write to its fullest.  Additionally there's no side table of descriptors to update, saving a DMA write there.</p><p>Apparently the only cards currently capable of 100 Gbps traffic, the Chelsio and Netcope cards, use the &quot;tape I/O model&quot;.</p><p>Incidentally, the DMA transfer limit isn't the only constraint.  Something I hadn't fully appreciated before was memory write bandwidth.  Before, I had thought that because the NIC would transfer in packet data directly to cache, that this wouldn't necessarily cause any write traffic to RAM.  Apparently that's not the case.  Later over drinks (thanks to Red Hat's networking group for organizing), François-Frédéric asserted that the DMA transfers would eventually use up DDR4 bandwidth as well.</p><p>A NIC-to-RAM DMA transaction will write one cache line (usually 64 bytes) to the socket's last-level cache.  This write will evict whatever was there before.  As far as I can tell, there are three cases of interest here.  The best case is where the evicted cache line is from a previous DMA transfer to the same address.  In that case it's modified in the cache and not yet flushed to main memory, and we can just update the cache instead of flushing to RAM.  (Do I misunderstand the way caches work here?  Do let me know.)</p><p>However if the evicted cache line is from some other address, we might have to flush to RAM if the cache line is dirty.  That causes a memory write traffic.  But if the cache line is clean, that means it was probably loaded as part of a memory read operation, and then that means we're evicting part of the network function's working set, which will later cause memory read traffic as the data gets loaded in again, and write traffic to flush out the DMA'd packet data cache line.</p><p>François-Frédéric simplified the whole thing to equate packet bandwidth with memory write bandwidth, that yes, the packet goes directly to cache but it is also written to RAM.  I can't convince myself that that's the case for all packets, but I need to look more into this.</p><p>Of course the cache pressure and the memory traffic is worse if the packet data is less compact in memory; and worse still if there is any need to copy data.  Ultimately, processing small packets at 100Gbps is still a huge challenge for user-space networking, and it's no wonder that there are only a couple devices on the market that can do it reliably, not that I've seen either of them operate first-hand :)</p><p>Talking with Snabb's Luke Gorrie later on, he thought that it could be that we can still stretch the packet array I/O model for a while, given that PCIe gen4 is coming soon, which will increase the DMA transaction rate.  So that's a possibility to keep in mind.</p><p>At the same time, apparently there are some <a href="https://www.ccixconsortium.com/">&quot;coherent interconnects&quot;</a> coming too which will allow the NIC's memory to be mapped into the &quot;normal&quot; address space available to the CPU.  In this model, instead of having the NIC transfer packets to the CPU, the NIC's memory will be directly addressable from the CPU, as if it were part of RAM.  The latency to pull data in from the NIC to cache is expected to be slightly longer than a RAM access; for comparison, RAM access takes about 70 nanoseconds.</p><p>For a user-space networking workload, coherent interconnects don't change much.  You still need to get the packet data into cache.  True, you do avoid the writeback to main memory, as the packet is already in addressable memory before it's in cache.  But, if it's possible to keep the packet on the NIC -- like maybe you are able to add some kind of inline classifier on the NIC that could directly shunt a packet towards an on-board IPSec accelerator -- in that case you could avoid a lot of memory transfer.  That appears to be the driving factor for coherent interconnects.</p><p>At some point in François-Frédéric's talk, my brain just died.  I didn't quite understand all the complexities that he was taking into account.  Later, after he kindly took the time to dispell some more of my ignorance, I understand more of it, though not yet all :)  The concrete &quot;deliverable&quot; of the talk was a model for kernel modules and user-space drivers that uses the paradigms he was promoting.  It's a work in progress from Linaro's networking group, with some support from NIC vendors and CPU manufacturers.</p><p><b>Luke Gorrie and Asumu Takikawa -- SnabbCo and Igalia -- <a href="https://fosdem.org/2018/schedule/event/lua_snabb/">How to write your own NIC driver, and why</a></b></p><p>This talk had the most magnificent beginning: a sort of &quot;repent now ye sinners&quot; sermon from Luke Gorrie, a seasoned veteran of software networking.  Luke started by describing the path of righteousness leading to &quot;driver heaven&quot;, a world in which all vendors have publically accessible datasheets which parsimoniously describe what you need to get packets flowing.  In this blessed land it's easy to write drivers, and for that reason there are many of them.  Developers choose a driver based on their needs, or they write one themselves if their needs are quite specific.</p><p>But there is another path, says Luke, that of &quot;driver hell&quot;: a world of wickedness and proprietary datasheets, where even when you buy the hardware, you can't program it unless you're buying a hundred thousand units, and even then you are smitten with the cursed non-disclosure agreements.  In this inferno, only a vendor is practically empowered to write drivers, but their poor driver developers are only incentivized to get the driver out the door deployed on all nine architectural circles of driver hell.  So they include some kind of circle-of-hell abstraction layer, resulting in a hundred thousand lines of code like a tangled frozen beard.  We all saw the abyss and repented.</p><p>Luke described the process that led to Mellanox releasing the specification for its ConnectX line of cards, something that was warmly appreciated by the entire audience, users and driver developers included.  Wonderful stuff.</p><p>My Igalia colleague Asumu Takikawa took the last half of the presentation, showing some code for the driver for the Intel i210, i350, and 82599 cards.  For more on that, I recommend his recent <a href="https://www.asumu.xyz/blog/2018/01/15/supporting-both-vmdq-and-rss-in-snabb">blog post on user-space driver development</a>.  It was truly a ray of sunshine in dark, dark Brussels.</p><p><b>Ole Trøan -- Cisco -- <a href="https://fosdem.org/2018/schedule/event/vnf_vpp/">Fast dataplanes with VPP</a></b></p><p>This talk was a delightful introduction to <a href="https://wiki.fd.io/view/VPP">VPP</a>, but without all of the marketing; the sort of talk that makes FOSDEM worthwhile.  Usually at more commercial, vendory events, you can't really get close to the technical people unless you have a vendor relationship: they are surrounded by a phalanx of salesfolk.  But in FOSDEM it is clear that we are all comrades out on the open source networking front.</p><p>The speaker expressed great personal pleasure on having being able to work on open source software; his relief was palpable.  A nice moment.</p><p>He also had some kind words about Snabb, too, saying at one point that &quot;of course you can do it on snabb as well -- Snabb and VPP are quite similar in their approach to life&quot;.  He trolled the horrible complexity diagrams of many &quot;NFV&quot; stacks whose components reflect the org charts that produce them more than the needs of the network functions in question (service chaining anyone?).</p><p>He did get to drop some numbers as well, which I found interesting.  One is that recently they have been working on carrier-grade NAT, aiming for 6 terabits per second.  Those are pretty big boxes and I hope they are getting paid appropriately for that :)  For context he said that for a 4-unit server, these days you can build one that does a little less than a terabit per second.  I assume that's with ten dual-port 40Gbps cards, and I would guess to power that you'd need around 40 cores or so, split between two sockets.</p><p>Finally, he finished with a long example on lightweight 4-over-6.  Incidentally this is the same network function my group at Igalia has been building in Snabb over the last couple years, so it was interesting to see the comparison.  I enjoyed his commentary that although all of these technologies (carrier-grade NAT, MAP, lightweight 4-over-6) have the ostensible goal of keeping IPv4 running, in reality &quot;we're day by day making IPv4 work worse&quot;, mainly by breaking the assumption that just because you get traffic from port P on IP M, doesn't mean you can send traffic to M from another port or another protocol and have it reach the target.</p><p>All of these technologies also have problems with IPv4 fragmentation.  Getting it right is possible but expensive.  Instead, Ole mentions that he and a cross-vendor cabal of dataplane people have a &quot;dark RFC&quot; in the works to deprecate IPv4 fragmentation entirely :)</p><p>OK that's it.  If I get around to writing up the couple of interesting Java talks I went to (I know right?) I'll let yall know.  Happy hacking!</p></div></div><div class="feedback"><a href="/archives/2018/02/05/notes-from-the-fosdem-2018-networking-devroom#comments">(1)</a></div></div><h2 class="storytitle"><a href="/archives/2018/01/17/instruction-explosion-in-guile">instruction explosion in guile</a></h2><div class="post"><h3 class="meta">17 January 2018 10:30 AM (<a href="/tags/guile">guile</a> | <a href="/tags/compilers">compilers</a> | <a href="/tags/instruction%20explosion">instruction explosion</a> | <a href="/tags/loop%20optimizations">loop optimizations</a> | <a href="/tags/cps">cps</a>)</h3><div class="storycontent"><div><p>Greetings, fellow Schemers and compiler nerds: I bring fresh nargery!</p><p><b>instruction explosion</b></p><p>A couple years ago I made a list of <a href="https://wingolog.org/archives/2016/02/04/guile-compiler-tasks">compiler tasks for Guile</a>.  Most of these are still open, but I've been chipping away at the one labeled &quot;instruction explosion&quot;:</p><blockquote><p> Now we get more to the compiler side of things. Currently in Guile's VM there are instructions like <a href="https://www.gnu.org/software/guile/docs/master/guile.html/Inlined-Scheme-Instructions.html#Inlined-Scheme-Instructions">vector-ref</a>.  This is a little silly: there are also instructions to branch on the type of an object (<a href="https://www.gnu.org/software/guile/docs/master/guile.html/Branch-Instructions.html#Branch-Instructions">br-if-tc7</a> in this case), to get the vector's length, and to do a branching integer comparison. Really we should replace vector-ref with a combination of these test-and-branches, with real control flow in the function, and then the actual ref should use some more primitive unchecked memory reference instruction. Optimization could end up hoisting everything but the primitive unchecked memory reference, while preserving safety, which would be a win. But probably in most cases optimization wouldn't manage to do this, which would be a lose overall because you have more instruction dispatch.</p><p>Well, this transformation is something we need for native compilation anyway. I would accept a patch to do this kind of transformation on the master branch, after version 2.2.0 has forked. In theory this would remove most all high level instructions from the VM, making the bytecode closer to a virtual CPU, and likewise making it easier for the compiler to emit native code as it's working at a lower level.  </p></blockquote><p>Now that I'm getting close to finished I wanted to share some thoughts.  <a href="https://lists.gnu.org/archive/html/guile-devel/2018-01/msg00003.html">Previous progress reports on the mailing list</a>.</p><p><b>a simple loop</b></p><p>As an example, consider this loop that sums the 32-bit floats in a bytevector.  I've annotated the code with lines and columns so that you can correspond different pieces to the assembly.</p><pre>
   0       8   12     19
 +-v-------v---v------v-
 |
1| (use-modules (rnrs bytevectors))
2| (define (f32v-sum bv)
3|   (let lp ((n 0) (sum 0.0))
4|     (if (&lt; n (bytevector-length bv))
5|         (lp (+ n 4)
6|             (+ sum (bytevector-ieee-single-native-ref bv n)))
7|          sum)))
</pre><p>The assembly for the loop before instruction explosion went like this:</p><pre>
L1:
  17    (handle-interrupts)     at (unknown file):5:12
  18    (uadd/immediate 0 1 4)
  19    (bv-f32-ref 1 3 1)      at (unknown file):6:19
  20    (fadd 2 2 1)            at (unknown file):6:12
  21    (s64&lt;? 0 4)             at (unknown file):4:8
  22    (jnl 8)                ;; -&gt; L4
  23    (mov 1 0)               at (unknown file):5:8
  24    (j -7)                 ;; -&gt; L1
</pre><p>So, already Guile's compiler has hoisted the <tt>(bytevector-length bv)</tt> and unboxed the loop index <i>n</i> and accumulator <i>sum</i>.  This work aims to simplify further by exploding <tt>bv-f32-ref</tt>.</p><p><b>exploding the loop</b></p><p>In practice, instruction explosion happens in CPS conversion, as we are converting the Scheme-like <a href="https://www.gnu.org/software/guile/docs/master/guile.html/Tree_002dIL.html#Tree_002dIL">Tree-IL</a> language down to the <a href="https://www.gnu.org/software/guile/docs/master/guile.html/Continuation_002dPassing-Style.html#Continuation_002dPassing-Style">CPS soup</a> language.  When we see a Tree-Il primcall (a call to a known primitive), instead of lowering it to a corresponding CPS primcall, we inline a whole blob of code.</p><p>In the concrete case of <tt>bv-f32-ref</tt>, we'd inline it with something like the following:</p><pre>
(unless (and (heap-object? bv)
             (eq? (heap-type-tag bv) %bytevector-tag))
  (error &quot;not a bytevector&quot; bv))
(define len (word-ref bv 1))
(define ptr (word-ref bv 2))
(unless (and (&lt;= 4 len)
             (&lt;= idx (- len 4)))
  (error &quot;out of range&quot; idx))
(f32-ref ptr len)
</pre><p>As you can see, there are four branches hidden in the <tt>bv-f32-ref</tt>: two to check that the object is a bytevector, and two to check that the index is within range.  In this explanation we assume that the offset <i>idx</i> is already unboxed, but actually unboxing the index ends up being part of this work as well.</p><p>One of the goals of instruction explosion was that by breaking the operation into a number of smaller, more orthogonal parts, native code generation would be easier, because the compiler would only have to know about those small bits.  However without an optimizing compiler, it would be better to <a href="https://docs.google.com/document/d/15hmBrCrmMZzra8ekhl7meQZBodeahsauilNDImif5Xg/edit#heading=h.8zevmmfemvcv">reify a call out to a specialized <tt>bv-f32-ref</tt> runtime routine</a> instead of inlining all of this code -- probably whatever language you write your runtime routine in (C, rust, whatever) will do a better job optimizing than your compiler will.</p><p>But with an optimizing compiler, there is the possibility of removing possibly everything but the <tt>f32-ref</tt>.  Guile doesn't quite get there, but almost; here's the post-explosion optimized assembly of the inner loop of f32v-sum:</p><pre>
L1:
  27    (handle-interrupts)
  28    (tag-fixnum 1 2)
  29    (s64&lt;? 2 4)             at (unknown file):4:8
  30    (jnl 15)               ;; -&gt; L5
  31    (uadd/immediate 0 2 4)  at (unknown file):5:12
  32    (u64&lt;? 2 7)             at (unknown file):6:19
  33    (jnl 5)                ;; -&gt; L2
  34    (f32-ref 2 5 2)
  35    (fadd 3 3 2)            at (unknown file):6:12
  36    (mov 2 0)               at (unknown file):5:8
  37    (j -10)                ;; -&gt; L1
</pre><p><b>good things</b></p><p>The first thing to note is that unlike the &quot;before&quot; code, there's no instruction in this loop that can throw an exception.  Neat.</p><p>Next, note that there's no type check on the bytevector; the <a href="https://wingolog.org/archives/2015/07/28/loop-optimizations-in-guile">peeled iteration</a> preceding the loop already proved that the bytevector is a bytevector.</p><p>And indeed there's no reference to the bytevector at all in the loop!  The value being dereferenced in <tt>(f32-ref 2 5 2)</tt> is a raw pointer.  (Read this instruction as, &quot;sp[2] = *(float*)((byte*)sp[5] + (uptrdiff_t)sp[2])&quot;.)  The compiler does something interesting; the <tt>f32-ref</tt> CPS primcall actually takes three arguments: the garbage-collected object protecting the pointer, the pointer itself, and the offset.  The object itself doesn't appear in the residual code, but including it in the <tt>f32-ref</tt> primcall's inputs keeps it alive as long as the <tt>f32-ref</tt> itself is alive.</p><p><b>bad things</b></p><p>Then there are the limitations.  Firstly, instruction 28 tags the u64 loop index as a fixnum, but never uses the result.  Why is this here?  Sadly it's because the value is used in the bailout at L2.  Recall this pseudocode:</p><pre>
(unless (and (&lt;= 4 len)
             (&lt;= idx (- len 4)))
  (error &quot;out of range&quot; idx))
</pre><p>Here the <a>error</a> ends up lowering to a <a href="https://lists.gnu.org/archive/html/guile-devel/2018-01/msg00003.html"><tt>throw</tt> CPS term</a> that the compiler recognizes as a bailout and renders out-of-line; cool.  But it uses <i>idx</i> as an argument, as a tagged SCM value.  The compiler untags the loop index, but has to keep a tagged version around for the error cases.</p><p>The right fix is probably some kind of allocation sinking pass that sinks the <tt>tag-fixnum</tt> to the bailouts.  Oh well.</p><p>Additionally, there are two tests in the loop.  Are both necessary?  Turns out, yes :( Imagine you have a bytevector of length 102<b>5</b>.  The loop continues until the last ref at offset 1024, which is within bounds of the bytevector but there's one one byte available at that point, so we need to throw an exception at this point.  The compiler did as good a job as we could expect it to do.</p><p><b>is is worth it?  where to now?</b></p><p>On the one hand, instruction explosion is a step sideways.  The code is more optimal, but it's more instructions.  Because Guile currently has a bytecode VM, that means more total interpreter overhead.  Testing on a 40-megabyte bytevector of 32-bit floats, the exploded <tt>f32v-sum</tt> completes in 115 milliseconds compared to around 97 for the earlier version.</p><p>On the other hand, it is very easy to imagine how to compile these instructions to native code, either ahead-of-time or via a simple template JIT.  You practically just have to look up the instructions in the corresponding ISA reference, is all.  The result should perform quite well.</p><p>I will probably take a whack at a simple template JIT first that does no register allocation, then ahead-of-time compilation with register allocation.  Getting the AOT-compiled artifacts to dynamically link with runtime routines is a sufficient pain in my mind that I will put it off a bit until later.  I also need to figure out a good strategy for truly polymorphic operations like general integer addition; probably involving inline caches.</p><p>So that's where we're at :)  Thanks for reading, and happy hacking in Guile in 2018!</p></div></div><div class="feedback"><a href="/archives/2018/01/17/instruction-explosion-in-guile#comments">(7)</a></div></div><h2 class="storytitle"><a href="/archives/2018/01/11/spectre-and-the-end-of-langsec">spectre and the end of langsec</a></h2><div class="post"><h3 class="meta">11 January 2018  1:44 PM (<a href="/tags/compilers">compilers</a> | <a href="/tags/security">security</a> | <a href="/tags/spectre">spectre</a> | <a href="/tags/meltdown">meltdown</a>)</h3><div class="storycontent"><div><p>I remember in 2008 seeing Gerald Sussman, creator of the Scheme language, resignedly describing a sea change in the MIT computer science curriculum.  In response to a question from the audience, <a href="https://wingolog.org/archives/2009/03/24/international-lisp-conference-day-two">he said</a>:</p><blockquote><p>The work of engineers used to be about taking small parts that they understood entirely and using simple techniques to compose them into larger things that do what they want.</p><p>But programming now isn't so much like that.  Nowadays you muck around with incomprehensible or nonexistent man pages for software you don't know who wrote.  <i>You have to do basic science on your libraries to see how they work</i>, trying out different inputs and seeing how the code reacts.  This is a fundamentally different job.  </p></blockquote><p>Like many I was profoundly saddened by this analysis.  I want to believe in constructive correctness, in math and in proofs.  And so with the rise of functional programming, I thought that this historical slide from reason towards observation was just that, historical, and that the &quot;safe&quot; languages had a compelling value that would be evident eventually: that &quot;another world is possible&quot;.</p><p>In particular I found solace in &quot;langsec&quot;, an approach to assessing and ensuring system security in terms of constructively correct programs.  One obvious application is parsing of untrusted input, and indeed the <a href="http://langsec.org/">langsec.org website</a> appears to emphasize this domain as one in which a programming languages approach can be fruitful.  It is, after all, <a href="https://www.gnu.org/software/guile/manual/html_node/Types-and-the-Web.html#Types-and-the-Web">a truth universally acknowledged, that a program with good use of data types, will be free from many common bugs.</a> So far so good, and so far so successful.</p><p>The basis of language security is starting from a programming language with a well-defined, easy-to-understand semantics.  From there you can prove (formally or informally) interesting security properties about particular programs.  For example, if a program has a secret <i>k</i>, but some untrusted subcomponent <i>C</i> of it should not have access to <i>k</i>, one can prove if <i>k</i> can or cannot leak to <i>C</i>.  This approach is taken, for example, by <a href="https://developers.google.com/caja/">Google's Caja compiler</a> to isolate components from each other, even when they run in the context of the same web page.</p><p>But the <a href="https://spectreattack.com/">Spectre</a> and <a href="https://meltdownattack.com/">Meltdown</a> attacks have seriously set back this endeavor.  One manifestation of the Spectre vulnerability is that code running in a process can now read the entirety of its address space, bypassing invariants of the language in which it is written, even if it is written in a &quot;safe&quot; language.  This is currently being used by JavaScript programs to exfiltrate passwords from a browser's password manager, or bitcoin wallets.</p><p>Mathematically, in terms of the semantics of e.g. JavaScript, these attacks should not be possible.  But practically, they work.  Spectre shows us that the building blocks provided to us by Intel, ARM, and all the rest are no longer &quot;small parts understood entirely&quot;; that instead now we have to do &quot;basic science&quot; on our CPUs and memory hierarchies to know what they do.</p><p>What's worse, we need to do basic science to come up with adequate mitigations to the Spectre vulnerabilities (side-channel exfiltration of results of speculative execution).  <a href="https://support.google.com/faqs/answer/7625886">Retpolines</a>, <a href="https://webkit.org/blog/8048/what-spectre-and-meltdown-mean-for-webkit/">poisons and masks</a>, et cetera: none of these are <i>proven</i> to work.  They are simply <i>observed</i> to be effective on current hardware.  Indeed mitigations are anathema to the correctness-by-construction: if you can prove that a problem doesn't exist, what is there to mitigate?</p><p>Spectre is not the first crack in the edifice of practical program correctness.  In particular, <a href="https://wingolog.org/archives/2014/12/02/there-are-no-good-constant-time-data-structures">timing side channels</a> are rarely captured in language semantics.  But I think it's fair to say that Spectre is the most devastating vulnerability in the langsec approach to security that has ever been uncovered.</p><p>Where do we go from here?  I see but two options.  One is to attempt to make the behavior of the machines targetted by secure language implementations behave rigorously as architecturally specified, and in no other way.  This is the approach taken by all of the deployed mitigations (retpolines, poisoned pointers, masked accesses): modify the compiler and runtime to prevent the CPU from speculating through vulnerable indirect branches (prevent speculative execution), or from using fetched values in further speculative fetches (prevent this particular side channel).  I think we are missing a model and a proof that these mitigations restore target architectural semantics, though.</p><p>However if we did have a model of what a CPU does, we have another opportunity, which is to incorporate that model in a semantics of the target language of a compiler (e.g. micro-x86 versus x86).  It could be that this model produces a co-evolution of the target architectures as well, whereby Intel decides to disclose and expose more of its microarchitecture to user code.  Cacheing and other microarchitectural side-effects would then become explicit rather than transparent.</p><p>Rich Hickey has this thing where he talks about &quot;simple versus easy&quot;.  Both of them sound good but for him, only &quot;simple&quot; is good whereas &quot;easy&quot; is bad.  It's the sort of subjective distinction that can lead to an endless string of <a href="https://www.dreamsongs.com/Files/worse-is-worse.pdf">Worse Is Better Is Worse</a> Bourbaki papers, according to the perspective of the author.  Anyway transparent caching in the CPU has been marvelously easy for most application developers and fantastically beneficial from a performance perspective.  People needing constant-time operations have complained, of course, but that kind of person always complains.  Could it be, though, that actually there is some other, better-is-better kind of simplicity that should replace the all-pervasive, now-treacherous transparent cacheing?</p><p>I don't know.  All I will say is that an ad-hoc approach to determining which branches and loads are safe and which are not is not a plan that inspires confidence.  Godspeed to the langsec faithful in these dark times.</p></div></div><div class="feedback"><a href="/archives/2018/01/11/spectre-and-the-end-of-langsec#comments">(8)</a></div></div><h2 class="storytitle"><a href="/archives/2017/09/05/a-new-interview-question">a new interview question</a></h2><div class="post"><h3 class="meta"> 5 September 2017  1:09 PM (<a href="/tags/hiring">hiring</a> | <a href="/tags/diversity">diversity</a> | <a href="/tags/inclusion">inclusion</a> | <a href="/tags/interview%20questions">interview questions</a>)</h3><div class="storycontent"><div><p>I have a new interview question, and you can have it too:</p><p>&quot;The industry has a gender balance problem.  Why is this?&quot; <i>[ed: see postscript]</i></p><p>This question is designed to see if how a potential collaborator is going to fit into a diverse team.  Are they going to behave well to their coworkers, or will they be a reason why people leave the group or the company?</p><p>You then follow up with a question about how you would go about improving gender balance, what the end result would look like, what's doable in what amount of time, and so on.  Other versions of the question could talk about the composition of the industry (or of academia) in terms of race, sexual orientation, gender identity, and so on.</p><p>I haven't tried this test yet but am looking forward to it.  I am hoping that it can shed some light on someone's ability to empathize with people from another group, and in that sense I think it's probably best to ask about a group that the candidate does not belong to (in as much as it's possible to know).  The candidate will also show who they listen to and who they trust -- how do they know what they know?  Do they repeat stories told about people of that group or by people of that group?</p><p>Some candidates will simply be weak in this area, and won't be able to say very much.  The person would require more training, and after an eventual hire it would be expected that this person says more ignorant things.  Unfortunately unlike ignorance about compilers, say, ignorance about the lived experience of women compiler writers, say, can lead to hurtful behavior, even if unintentional.  For this reason, ignorance is a negative point about a candidate, though not a no-hire signal as such.</p><p>Obviously if you discover a person that thinks that gender imbalance is just the way it is and that nothing can or should be done about it, or that women don't program well, or the like, then that's a great result:  clear no-hire.  This person is likely to make life unpleasant for their female colleagues, and your company just avoided the problem.  High fives, interview team!</p><p>Alternately, if you find a candidate who deeply understands the experience of a group that they aren't a part of and who can even identify measures to improve those peoples' experience in your company and industry, then you've found a gem.  I think it can easily outweigh a less strong technical background, especially if the person has a history of being able to learn and train on the job (or on the open-source software project, or in the university course, etc).</p><p>Successfully pulling off this question looks tricky to me but I am hopeful.  If you give it a go, let me know!  Likewise if you know of other approaches that work at interview-time, they are very welcome :)</p><p><i>Postscript: The question is a template -- ideally you ask about a group the person is not in.  A kind commenter correctly pointed out that the article looks like I would only interview men and that definitely wasn't what I was going for!</i></p></div></div><div class="feedback"><a href="/archives/2017/09/05/a-new-interview-question#comments">(30)</a></div></div><h2 class="storytitle"><a href="/archives/2017/09/04/the-hardest-thing-about-hiring-is-avoiding-the-fash">the hardest thing about hiring is avoiding the fash</a></h2><div class="post"><h3 class="meta"> 4 September 2017 11:25 PM (<a href="/tags/hiring">hiring</a> | <a href="/tags/fash">fash</a> | <a href="/tags/icfp">icfp</a> | <a href="/tags/fascism">fascism</a> | <a href="/tags/antifa">antifa</a>)</h3><div class="storycontent"><div><p>Most of the literature on hiring is about technical ability, and indeed that's important.  But no job is solely technical: there are communication and cooperation aspects as well.  The output of a group is not simply the sum of its components.</p><p>In that regard I have a thesis: the most important thing about hiring is to avoid the fash.  Fascist sympathisers are toxic to the collective organism and you do not want them on your team.  The tricky thing is honing your fashdar to suit the purpose.</p><p>I am currently in Oxford for ICFP, one of the most prestigious conferences of my field (programming languages).  I came to be refreshed with new ideas and to maintain contacts, for collaboration and also for future hires.  This evening I was out with a group of folks, and the topic of conversation for much of the evening was the abstract value of free speech versus anti-fascist activity.</p><p>Unfortunately, one participant was stuck on free speech side of the debate.  All day the conference has been doing logical programs and term equivalence, but somehow this person didn't deduce the result &quot;free speech fundamentalism == fash&quot;, at least within the current evaluation context.</p><p>In the US we are raised to see speech as an axiom.  As such it must be taken on faith, and such faith is only present in  people not physically threatened by the axiom.  For example, as an immigrant to France, speech against immigrants in France is not simply an abstract topic to me.  It means that my existence does not have the same value as someone that was born there.  More broadly it also applies to second-generation immigrants, or to people of color, as whiteness is also part of what it means to be a native French person.  Treating the speech of all equally is probably a good ideal among speakers of equal power, but in the current context, enshrining &quot;free speech&quot; as a Trump card of social value is fash.  Saying &quot;Latino people are lazy&quot; is obviously more threatening to Latinos than &quot;white people are lazy&quot; is to whites <i>(ed: not that these sets are disjoint!)</i>; but under the &quot;free speech&quot; evaluator, these terms are equivalent.  &quot;Free speech&quot; as an abstract interpreter of value is too abstract.</p><p>I thought with this person we had a good discussion trying to get them closer to a justice-affirming perspective, but in the end they turned out to also believe that women were predisposed to not be good computer scientists -- &quot;genetic differences&quot;, they said.  Never mind that CS was women's work until it became high-status, and this after the whole discussion of the effects of speech; it does not take an abstract interpretation specialist to predict the results of this perspective on the future gender composition of the industry, not to mention the concrete evaluation of any given woman.</p><p>I will admit that I was surprised at the extent to which men's-rights-activist and racist talking points had made it into their discourse, and the way the individualist edifice of their world-view enabled their pre-fascist ideas.  I use the term advisedly; the effects of advocating these views are prior to fascism.</p><p>My conclusion from the interaction is that, now more than ever, when it comes to future collaborators, in any context, it is important for anyone for whom justice matters to probe candidates deeply for fascist tendencies.  If they show a sign, pass.  Anti-immigrant, anti-woman, anti-queer, and anti-black behaviors and actions often correlate: if you see one sign, often there are more underneath.  Even if the person claims to value egalitarianism, the perspectives that guide their actions may not favor justice in your organization.  In retrospect I am glad that I had this interaction, simply for the purpose of knowing what to filter out the next time I am in a hiring decision.</p></div></div><div class="feedback"><a href="/archives/2017/09/04/the-hardest-thing-about-hiring-is-avoiding-the-fash#comments">(3)</a></div></div><h2 class="storytitle"><a href="/archives/2017/06/29/a-new-concurrent-ml">a new concurrent ml</a></h2><div class="post"><h3 class="meta">29 June 2017  2:37 PM (<a href="/tags/guile">guile</a> | <a href="/tags/scheme">scheme</a> | <a href="/tags/gnu">gnu</a> | <a href="/tags/igalia">igalia</a> | <a href="/tags/compilers">compilers</a> | <a href="/tags/delimited%20continuations">delimited continuations</a> | <a href="/tags/prompts">prompts</a> | <a href="/tags/fibers">fibers</a> | <a href="/tags/concurrency">concurrency</a> | <a href="/tags/concurrent%20ml">concurrent ml</a> | <a href="/tags/curry%20on">curry on</a>)</h3><div class="storycontent"><div><p>Good morning all!</p><p>In my <a href="https://wingolog.org/archives/2017/06/27/growing-fibers">last article</a> I talked about how we composed a lightweight &quot;fibers&quot; facility in Guile out of lower-level primitives.  What we implemented there is enough to be useful, but it is missing an important aspect of concurrency: communication.  Sure, being able to spawn off fibers is nice, but you have to be able to actually talk to them.</p><p>Fibers had just gotten to the state described above about a year ago as I caught a train from Geneva to Rome for <a href="http://curry-on.org/2016/">Curry On 2016</a>.  Train rides are magnificent for organizing thoughts, and I was in dire need of clarity.  I had tentatively settled on Go-style channels by the time I got there, but when I saw that Matthias Felleisen and Matthew Flatt there, I had to take advantage of the opportunity to ask them what they thought.  Would they recommend Racket-like threads and channels?  Had that been a good experience?</p><p>The answer that I got in return was a &quot;yes, that's what you should do&quot;, but also a <a href="https://wingolog.org/archives/2016/09/20/concurrent-ml-versus-go">&quot;you should look at Concurrent ML&quot;</a>.  Concurrent ML?  What's that?  I looked and was a bit skeptical.  It seemed old and hoary and maybe channels were just as expressive.  I looked more deeply into this issue and it seemed <a href="https://wingolog.org/archives/2016/09/21/is-go-an-acceptable-cml">CML is a bit more expressive than just channels</a> but damn, it looked complicated to implement.</p><p>I was wrong.  This article shows that what you need to do to implement multi-core CML is actually the same as what you need to do to implement channels in a multi-core environment.  By building CML first and channels and whatever later, you get more power for the same amount of work.</p><p>Note that this article has an associated talk!  If video is your thing, see my Curry On 2017 talk here:</p><p><video width="640" preload="none" poster="//wingolog.org/pub/curry-on-2017-wingo-cml.png" height="360" controls="controls"><source type="video/webm" src="//wingolog.org/pub/curry-on-2017-wingo-cml.webm" /><source type="video/mp4" src="//wingolog.org/pub/curry-on-2017-wingo-cml.mp4" /><a href="//wingolog.org/pub/curry-on-2017-wingo-cml.webm">Click to download video</a></video></p><p>Or, watch <a href="https://www.youtube.com/watch?v=7IcI6sl5oBc">on the youtube</a> if the inline video above doesn't work; <a href="https://wingolog.org/pub/curry-on-2017-slides.pdf">slides here as well</a>.</p><p><b>on channels</b></p><p>Let's first have a crack at implementing channels.  Before we begin, we should be a bit more explicit about what a channel is.  My first hack in this area did the wrong thing: I was used to asynchronous queues, and I thought that's what a channel was.  Besides ignorance, apparently that's what Erlang does; a process's inbox is an unbounded queue of messages with only very slight back-pressure.</p><p>But an asynchronous queue is not a channel, at least in its classic sense.  As they were originally formulated in &quot;Communicating Sequential Processes&quot; by Tony Hoare, adopted into David May's occam, and from there into many other languages, channels are meeting-places.  Processes meet at a channel to exchange values; whichever party arrives first has to wait for the other party to show up.  The message that is handed off in a channel send/receive operation is never &quot;owned&quot; by the channel; it is either owned by a sender who is waiting at the meeting point for a receiver, or it's accepted by a receiver.  After the transaction is complete, both parties continue on.</p><p>You'd think this is a fine detail, but meeting-place channels are strictly more expressive than buffered channels.  I was actually called out for this because my first implementation of channels for Fibers had effectively a minimum buffer size of 1.  In Go, whose channels are unbuffered by default, you can use a channel for RPC:</p><pre>
package main

func double(ch chan int) {
  for { ch &lt;- (&lt;-ch * 2) }
}

func main() {
  ch := make(chan int)
  go double(ch)
  ch &lt;- 2
  x := &lt;-ch
  print(x)
}
</pre><p>Here you see that the main function sent a value on <tt>ch</tt>, then immediately read a response from the same channel.  If the channel were buffered, then we'd probably read the value we sent instead of the doubled value supplied by the <tt>double</tt> goroutine.  I say &quot;probably&quot; because it's not deterministic!  Likewise the <tt>double</tt> routine could read its responses as its inputs.</p><p>Anyway, the channels we are looking to build are meeting-place channels.  If you are interested in the broader design questions, you might enjoy the <a href="https://wingolog.org/archives/2016/10/12/an-incomplete-history-of-language-facilities-for-concurrency">incomplete history of language facilities for concurrency</a> article I wrote late last year.</p><p>With that prelude out of the way, here's a first draft at the implementation of the &quot;receive&quot; operation on a channel.</p><pre>
(define (recv ch)
  (match ch
    (($ $channel recvq sendq)
     (match (try-dequeue! sendq)
       (#(value resume-sender)
        (resume-sender)
        value)
       (#f
        (suspend
         (lambda (k)
           (define (resume val)
             (schedule (lambda () (k val)))
           (enqueue! recvq resume))))))))

;; Note: this code has a race!  Fixed later.
</pre><p>A channel is a record with two fields, its <tt>recvq</tt> and <tt>sendq</tt>.  The receive queue (<tt>recvq</tt>) holds a FIFO queue of continuations that are waiting to receive values, and the send queue holds continuations that are waiting to send values, along with the value that they are sending.  Both the <tt>recvq</tt> and the <tt>sendq</tt> are lockless queues.</p><p>To receive a value from a meeting-place channel, there are two possibilities: either there's a sender already there and waiting, or we have to wait for a sender.  Those two cases are handled above, in that order.  We use the <tt>suspend</tt> primitive from the last article to arrange for the fiber to wait; presumably the sender will resume us when they arrive later at the meeting-point.</p><p><b>an aside on lockless data structures</b></p><p>We'll go more deeply into the channel receive mechanics later, but first, a general question: what's the right way to implement a data structure that can be accessed and modified concurrently without locks?  Though I am full of hubris, I don't have enough to answer this question definitively.  I know many ways, but none that's optimal in all ways.</p><p>For what I needed in Fibers, I chose to err on the side of simplicity.</p><p>Some data in Fibers is never modified; this immutable data is safe to access concurrently from any code.  This is the best, obviously :)</p><p>Some mutable data is only ever mutated from an &quot;owner&quot; core; it's safe to read without a lock from that owner core, and in Fibers we do not access this data from other cores.  An example of this kind of data structure is the <tt>i/o</tt> map from file descriptors to continuations; it's core-local.  I say &quot;core-local&quot; because in fibers we typically run one scheduler per core, with each core having a pinned POSIX thread; it's really thread-local but I don't want to use the word &quot;thread&quot; too much here as it's confusing.</p><p>Some mutable data needs to be read and written from many cores.  An example of this is the <tt>recvq</tt> of a channel; many receivers and senders can be wanting to read and write there at once.  The approach we take in Fibers is just to use immutable data stored inside an <a href="https://www.gnu.org/software/guile/manual/html_node/Atomics.html">&quot;atomic box&quot;</a>.  An atomic box holds a single value, and exposes operations to read, write, swap, and compare-and-swap (<tt>CAS</tt>) the value.  To read a value, just fetch it from the box; you then have immutable data that you can analyze without locks.  Having read a value, you can to compute a new state and use CAS on the atomic box to publish that change.  If the CAS succeeds, then great; otherwise the state changed in the meantime, so you typically want to loop and try again.</p><p>Single-word CAS suffices for Guile when every value stored into an atomic box will be unique, a property that freshly-allocated objects have and of which GC ensures us an endless supply.  Note that for this to work, the values can share structure internally but the outer reference has to be freshly allocated.</p><p>The combination of freshly-allocated data structures and atomic variables is a joy to use: no hassles about multi-word compare-and-swap or the <a href="https://en.wikipedia.org/wiki/ABA_problem">ABA problem</a>.  Assuming your GC can keep up (Guile's does about 700 MB/s), it can be an effective strategy, and is certainly less error-prone than others.</p><p><b>back at the channel recv ranch</b></p><p>Now, the theme here is <a href="https://www.youtube.com/watch?v=_ahvzDzKdB0">&quot;growing a language&quot;</a>: taking primitives and using them to compose more expressive abstractions.  In that regard, sure, channel send and receive are nice, but what about <tt>select</tt>, which allows us to wait on any channel in a set of channels?  How do we take what we have and built non-determinism on top?</p><p>I think we should begin by noting that <tt>select</tt> in Go for example isn't just about receiving messages.  You can select on the first channel that can send, or between send and receive operations.</p><pre>
select {
case c &lt;- x:
  x, y = y, x+y
case &lt;-quit:
  return
}
</pre><p>As you can see, Go provides special syntax for <tt>select</tt>.  Although in Guile we can of course provide macros, usually those macros expand out to a procedure call; the macro is sugar for a function.  So we want <tt>select</tt> as a function.  But because we need to be able to select over receiving and sending at the same time, the function needs to take some kind of annotation on what we are going to do with the channels:</p><pre>
(select (<i>recv A</i>) (<i>send B v</i>))
</pre><p>So what we do is to introduce the concept of an <i>operation</i>, which is simply data describing some event which may occur in the future.  The arguments to <tt>select</tt> are now operations.</p><pre>
(select (recv-op A) (send-op B v))
</pre><p>Here <tt>recv-op</tt> is obviously a constructor for the channel-receive operation, and likewise for <tt>send-op</tt>.  And actually, given that we've just made an abstraction over sending or receiving on a channel, we might as well make an abstraction over choosing the first available op among a set of operations.  The implementation of <tt>select</tt> now creates such a <tt>choice-op</tt>, then <i>performs</i> it.</p><pre>
(define (select . ops)
  (perform (apply choice-op ops)))
</pre><p>But what we're missing here is the ability to know which operation actually happened.  In Go, <tt>select</tt>'s special syntax associates a clause of code with each sub-operation.  In Scheme a clause of code is just a function, and so what we want to do is to be able to annotate an operation with a function that will get run if the operation succeeds.</p><p>So we define a <tt>(wrap-op op k)</tt>, which makes an operation that itself annotates <i>op</i>, associating it with <i>k</i>.  If <i>op</i> occurs, its result values will be passed to <i>k</i>.  For example, if we make a fiber that tries to perform this operation:</p><pre>
(perform
 (wrap-op
  (recv-op A)
  (lambda (v)
    (string-append &quot;hello, &quot; v))))
</pre><p>If we send the string <tt>&quot;world&quot;</tt> on the channel <tt>A</tt>, then the result of this <tt>perform</tt> invocation will be <tt>&quot;hello, world&quot;</tt>.  Providing &quot;wrapped&quot; operations to <tt>select</tt> allows us to handle the various cases in separate, appropriate ways.</p><p><b>we just made concurrent ml</b></p><p>Hey, we just reinvented Concurrent ML!  In his PLDI 1988 paper &quot;Synchronous operations as first-class values&quot;, John Reppy proposes just this abstraction.  I like to compare it to the relationship between an expression (<tt><i>exp</i></tt>) and wrapping that expression in a lambda (<tt>(lambda () <i>exp</i>)</tt>); evaluating an expression gives its value, and the expression just goes away, whereas evaluating a lambda gives a procedure that you can call in the future to evaluate the expression.  You can call the lambda many times, or no times.  In the same way, a channel-receive operation is an abstraction over receiving a value from a channel.  You can perform that operation many times, once, or not at all.</p><p>Reppy consolidated this work in his PLDI 1991 paper, &quot;CML: A higher-order concurrent language&quot;.  Note that he uses the term &quot;event&quot; instead of &quot;operation&quot;.  To me the name &quot;event&quot; to apply to this abstraction never felt quite right; I guess I wrote too much code in the past against event loops.  I see &quot;events&quot; as single instances in time and not an abstraction over the possibility of a, well, of an event.  Indeed I wish I could refer to an instantiation of an operation as an event, but better not to muddy the waters.  Likewise Reppy uses &quot;synchronize&quot; where I use &quot;perform&quot;.  As you like, really, it's still Concurrent ML; I just prefer to explain to my users using terms that make sense to me.</p><p><b>what's an op?</b></p><p>Let's return to that channel <tt>recv</tt> implementation.  It had basically two parts: an optimistic part, where the operation could complete immediately, and a pessimistic part, where we had to wait for the other party to arrive.  However, there was a race condition, as I noted in the comment.  If a sender and a receiver concurrently arrive at a channel, it could be that they concurrently do the optimistic check, don't notice that the other is there, then they both suspend, waiting for each other to arrive: deadlock.  To fix this for <tt>recv</tt>, we have to recheck the <tt>sendq</tt> after publishing our presence to the <tt>recvq</tt>.</p><p>I'll get to the details in a bit for channels, but it turns out that this is a general pattern.  All kinds of ops have optimistic and pessimistic behavior.</p><pre>
(define (perform op)
  (match op
    (($ $op try block wrap)
     (define (do-op)
       ;; Return a thunk that has result values.
       (or <i>optimistic</i>
           <i>pessimistic</i>)))
     ;; Return values, passed through wrap function.
     ((compose wrap do-op)))))
</pre><p>In the optimistic phase, the calling fiber will try to commit the operation directly.  If that succeeds, then the calling fiber resumes any other fibers that are part of the transaction, and the calling fiber continues.  In the pessimistic phase, we park the calling fiber, publish the fact that we're ready and waiting for the operation, then to resolve the race condition we have to try <i>again</i> to complete the operation.  In either case we pass the result(s) through the <tt>wrap</tt> function.</p><p>Given that the pessimistic phase has to include a re-check for operation completability, the optimistic phase is purely an optimization.  It's a good optimization that everyone will want to implement, but it's not strictly necessary.  It's OK for a <tt>try</tt> function to always return <tt>#f</tt>.</p><p>As shown in the above function, an operation is a plain old data structure with three fields: a <tt>try</tt>, a <tt>block</tt>, and a <tt>wrap</tt> function.  The optimistic behavior is implemented by the <tt>try</tt> function; the pessimistic side is partly implemented by <tt>perform</tt>, which handles the fiber suspension part, and by the operation's <tt>block</tt> function.  The <tt>wrap</tt> function implements the <tt>wrap-op</tt> behavior described above, and is applied to the result(s) of a successful operation.</p><p>Now, it was about at this point that I was thinking &quot;jeebs, this CML thing is complicated&quot;.  I was both wrong and right -- there's some complication inherent in multicore lockless communication, yes, but I believe CML captures something close to the minimum, and certainly it's just as much work as with a direct implementation of channels.  In that spirit, I continue on with the implementation of channel operations in Fibers.</p><p><b>channel receive operation</b></p><p>Here's an implementation of a <tt>try</tt> function for a channel.</p><pre>
(define (try-recv ch)
  (match ch
    (($ $channel recvq sendq)
     (let ((q (atomic-ref sendq)))
       (match q
         (() #f)
         ((head . tail)
          (match head
            (#(val resume-sender state)
             (match (CAS! state 'W 'S)
               ('W
                (resume-sender (lambda () (values)))
                (CAS! sendq q tail) ; *
                (lambda () val))
               (_ #f))))))))))
</pre><p>In Fibers, a <tt>try</tt> function either succeeds and returns a thunk, or fails and returns <tt>#f</tt>.  For channel receive, we only succeed if there is a sender already in the queue: the sender has arrived, suspended itself, and published its availability.  The <tt>state</tt> variable is an atomic box that holds the operation state, which initially starts as <tt>W</tt> and when complete is <tt>S</tt>.  More on that in a minute.  If the <tt>CAS!</tt> compare-and-swap operation managed to change the state from <tt>W</tt> to <tt>S</tt>, then the optimistic phase suceeded -- yay!  We resume the sender with no values, take the value that the sender gave us, and keep on trucking, returning that value wrapped in a thunk.</p><p>Additionally the sender's entry on the <tt>sendq</tt> is now stale, as the operation is already complete; we try to pop it off the queue at the line indicated with <tt>*</tt>, but that could fail due to concurrent queue modification.  In that case, no biggie, someone else will do the collect our garbage for us.</p><p>The pessimistic case is a bit more involved.  It's the last bit of code though; almost done here!  I express the pessimistic phase as a function of the operation's <tt>block</tt> function.</p><pre>
(define (pessimistic block)
  ;; For consistency with optimistic phase, result of
  ;; pessimistic phase is a thunk that &quot;perform&quot; will
  ;; apply.
  (lambda ()
    ;; 1. Suspend the thread.  Expect to be resumed
    ;; with a thunk, which we arrange to invoke directly.
    ((suspend
       (lambda (k)
        (define (resume values-thunk)
          (schedule (lambda () (k values-thunk))))
        ;; 2. Make a fresh opstate.
        (define state (make-atomic-box 'W))
        ;; 3. Call op's block function.
        (block resume state))))))
</pre><p>So what about that state variable?  Well basically, once we publish the fact that we're ready to perform an operation, fibers from other cores might concurrently try to complete our operation.  We need for this <tt>perform</tt> invocation to complete at most once!  So we introduce a state variable, the &quot;opstate&quot;, held in an atomic box.  It has three states:</p><ul> <li><p><tt>W</tt>: &quot;Waiting&quot;; initial state</p></li> <li><p><tt>C</tt>: &quot;Claimed&quot;; temporary state</p></li> <li><p><tt>S</tt>: &quot;Synched&quot;; final state</p></li> </ul><p>There are four possible state transitions, of two kinds.  Firstly there are the &quot;local&quot; transitions <tt>W-&gt;C</tt>, <tt>C-&gt;W</tt>, and <tt>C-&gt;S</tt>.  These transitions may only ever occur as part of the &quot;retry&quot; phase a <tt>block</tt> function; notably, no remote fiber will cause these transitions on &quot;our&quot; state variable.  Remote fibers can only make the <tt>W-&gt;S</tt> transition, committing an operation.  The <tt>W-&gt;S</tt> transition can also be made locally of course.</p><p>Every time an operation is instantiated via the <tt>perform</tt> function, we make a new opstate.  Operations themselves don't hold any state; only their instantiations do.</p><p>The need for the <tt>C</tt> state wasn't initially obvious to me, but after seeing the <tt>recv-op</tt> <tt>block</tt> function below, it will be clear to you I hope.</p><p><b>block functions</b></p><p>The <tt>block</tt> function itself has two jobs to do.  Recall that it's called after the calling fiber was suspended, and is passed two arguments: a procedure that can be called to resume the fiber with some number of values, and the fresh opstate for this instantiation.  The <tt>block</tt> function has two jobs: it needs to publish the resume function and the opstate to the channel's <tt>recvq</tt>, and then it needs to try again to receive.  That's the &quot;retry&quot; phase I was mentioning before.</p><p>Retrying the <tt>recv</tt> can have three possible results:</p><ol> <li><p>If the retry succeeds, we resume the sender.  We also have to resume the calling fiber, as it has been suspended already.  In general, whatever code manages to commit an operation has to resume any fibers that were waiting on it to complete.</p></li> <li><p>If the operation was already in the <tt>S</tt> state, that means some other party concurrently completed our operation on our behalf.  In that case there's nothing to do; the other party resumed us already.</p></li> <li><p>Otherwise if the operation couldn't proceed, then when the other party or parties arrive, they will be responsible for completing the operation and ultimately resuming our fiber in the future.</p></li> </ol><p>With that long prelude out of the way, here's the gnarlies!</p><pre>
(define (block-recv ch resume-recv recv-state)
  (match ch
    (($ $channel recvq sendq)
     ;; Publish -- now others can resume us!
     (enqueue! recvq (vector resume-recv recv-state))
     ;; Try again to receive.
     (let retry ()
       (let ((q (atomic-ref sendq)))
         (match q
           ((head . tail)
            (match head
              (#(val resume-send send-state)
               (match (CAS! recv-state 'W 'C)   ; Claim txn.
                 ('W
                  (match (CAS! send-state 'W 'S)
                    ('W                         ; Case (1): yay!
                     (atomic-set! recv-state 'S)
                     (CAS! sendq q tail)        ; Maybe GC.
                     (resume-send (lambda () (values)))
                     (resume-recv (lambda () val)))
                    ('C                         ; Conflict; retry.
                     (atomic-set! recv-state 'W)
                     (retry))
                    ('S                         ; GC and retry.
                     (atomic-set! recv-state 'W)
                     (CAS! sendq q tail)
                     (retry))))
                 ('S #f)))))                    ; Case (2): cool!
           (() #f)))))))                        ; Case (3): we wait.
</pre><p>As we said, first we publish, then we retry.  If there is a sender on the queue, we will try to complete their operation, but before we do that we have to prevent other fibers from completing ours; that's the purpose of going into the <tt>C</tt> state.  If we manage to commit the sender's operation, then we commit ours too, going from <tt>C</tt> to <tt>S</tt>; otherwise we roll back to <tt>W</tt>.  If the sender itself was in <tt>C</tt> then we had a conflict, and we spin to retry.  We also try to GC off any completed operations from the <tt>sendq</tt> via unchecked <tt>CAS</tt>.  If there's no sender on the queue, we just wait.</p><p>And that's it for the code!  Thank you for suffering through this all.  I only left off a few details: the <tt>try</tt> function can loop if sender is in the <tt>C</tt> state, and the <tt>block</tt> function needs to avoid a <tt>(choice-op (send-op A v) (recv-op A))</tt> from sending <i>v</i> to itself.  But because opstates are fresh allocations, we can know if a sender is actually ourself by comparing its opstate to ours (with <tt>eq?</tt>).</p><p><b>what about select?</b></p><p>I started about all this &quot;op&quot; business because I needed to annotate the arguments to <tt>select</tt>.  Did I actually get anywhere?  Good news, everyone: it turns out that <tt>select</tt> doesn't have to be a primitive!</p><p>Firstly, note that the <tt>choose-op</tt> <tt>try</tt> function just needs to run all <tt>try</tt> functions of sub-operations (possibly in random order), returning early if one succeeds.  Pretty straightforward.  And actually the story with the <tt>block</tt> function is the same: we just run the sub-operation <tt>block</tt> functions, knowing that the operation will commit at most one time.  The only complication is plumbing through the respective <tt>wrap</tt> functions to all of the sub-operations, but of course that's the point of the <tt>wrap</tt> facility, so we pay the cost willingly.</p><pre>
(define (choice-op . ops)
  (define (try)
    (or-map
     (match-lambda
      (($ $op sub-try sub-block sub-wrap)
       (define thunk (sub-try))
       (and thunk (compose sub-wrap thunk))))
     ops))
  (define (block resume opstate)
    (for-each
     (match-lambda
      (($ $op sub-try sub-block sub-wrap)
       (define (wrapped-resume results-thunk)
         (resume (compose sub-wrap results-thunk)))
       (sub-block wrapped-resume opstate)))
     ops))
  (define wrap values)
  (make-op try block wrap))
</pre><p>There are optimizations possible, for example to randomize the order of visiting the sub-operations for more less deterministic behavior, but this is really all there is.</p><p><b>concurrent ml is inevitable</b></p><p>As far as I understand things, the protocol to implement CML-style operations on channels in a lock-free environment are <i>exactly</i> the same as what's needed if you wrote out the <tt>recv</tt> function by hand, without abstracting it to a <tt>recv-op</tt>.</p><p>You still need the ability to park a fiber in the <tt>block</tt> function, and you still need to retry the operation after parking.  Although <tt>try</tt> is just an optimization, it's an optimization that you'll want.</p><p>So given that the cost of parallel CML is necessary, you might as well get what you pay for and have your language expose the more expressive CML interface in addition to the more &quot;standard&quot; channel operations.</p><p><b>concurrent ml between pthreads and fibers</b></p><p>One really cool aspect about implementing CML is that the bit that suspends the current thread is isolated in the <tt>perform</tt> function.  Of course if you're in a fiber, you suspend the current fiber as we have described above.  But what if you're not?  What if you want to use CML to communicate between POSIX threads?  You can do that, just create a mutex/cond pair and pass a procedure that will signal the cond as the resume argument to the <tt>block</tt> function.  It just works!  The channels implementation doesn't need to know anything about pthreads, or even fibers for that matter.</p><p>In fact, you can actually use CML operations to communicate between fibers and full pthreads.  This can be really useful if you need to run some truly blocking operation in a side pthread, but you want most of your program to be in fibers.</p><p><b>a meta-note for a meta-language</b></p><p>This implementation was based on the Parallel CML paper from Reppy et al, describing the protocol implemented in <a href="http://manticore.cs.uchicago.edu/">Manticore</a>.  Since then there's been a lot of development there; you should check out Manticore!  I also hear that Reppy has a new version of his &quot;Concurrent Programming in ML&quot; book coming out soon (not sure though).</p><p>This work is in <a href="https://github.com/wingo/fibers/">Fibers</a>, a concurrency facility for Guile Scheme, built as a library.  Check out the <a href="https://github.com/wingo/fibers/wiki/Manual">manual</a> for full details.  Relative to the Parallel CML paper, this work has a couple differences beyond the superficial operation/perform event/sync name change.</p><p>Most significantly, Reppy's CML operations have three phases: <tt>poll</tt>, <tt>do</tt>, and <tt>block</tt>.  Fibers uses just two, as in a concurrent context it doesn't make sense to check-then-do.  There is no <tt>do</tt>, only <tt>try</tt> :)</p><p>Additionally the Fibers channel implementation is lockless, with an atomic <tt>sendq</tt> and <tt>recvq</tt>.  In contrast, Manticore uses a spinlock and hence needs to mask/unmask interrupts at times.</p><p>On the other hand, the Parallel CML paper included some model checking work, which Fibers doesn't have.  It would be nice to have some more confidence on correctness!</p><p><b>but what about perf</b></p><p>Performance!  Does it scale?  Let's poke it.  Here I'm going to try to isolate my tests to measure the overhead of communication of channels as implemented in terms of Parallel CML ops.  I have more real benchmarks for Fibers on a web server workload where it does well, but here I am really trying to focus on CML.</p><p>My test system is a 2 x E5-2620v3, which is two sockets each having 6 2.6GHz cores, hyperthreads off, performance governor on all cores.  This is a system we use for <a href="https://github.com/snabbco/snabb/blob/master/src/doc/performance-tuning.md">Snabb testing</a>, so the first core on each socket handles interrupts and all others are reserved; Linux won't schedule anything on them.  When you run a fibers system, it will spawn a thread per available core, then set the thread's affinity to that core.  In these tests, I'll give benchmarks progressively more cores and see how they do with the workload.</p><p><center><img style="max-width:100%;" src="//wingolog.org/pub/curry-on-2017-chain.png" /></center></p><p>So this is a benchmark measuring total message sends per second on a chain of fibers communicating over channels.  For 0 links, that means that there's just a sender and a receiver and no intermediate links.  For 10 links, each message is relayed 10 times, for 11 total sends in the chain and 12 total fibers.  For 0 links we expect pretty much no parallel speedup, and no slowdown, and that's what we see; but when we get to more links, we should expect more throughput.  The fibers are allocated to cores at random (a randomized round-robin initial scheduling, then after that fibers have core affinity; though there is a limited work-stealing phase).</p><p>You would think that the 1-core case would be the same for all of them.  Unfortunately it seems that currently there is a fixed cost for bouncing through epoll to pick up new I/O tasks, even though there are no I/O runnables in this test and the timeout is 0, so it will return immediately.  It's definitely something to look into as it's a cost that all cores are paying.</p><p>Initially I expected a linear speedup but that's not what we're seeing.  But then I thought about it and revised my expectations :) As we add more cores, we add more communication; we should see sublinear speedups as we have to do more cross-core wakeups and synchronizations.  After all, we aren't measuring a nice parallelizable computational workload: we're measuring overhead.</p><p>On the other hand, the diminishing returns effect is pretty bad, and then we hit the NUMA cliff: as we cross from 6 to 7 cores, we start talking to the other CPU socket and everything goes to shit.</p><p>But here it's hard to isolate the test from three external factors, whose impact I don't understand: firstly, that Fibers itself has a significant wakeup cost for remote schedulers.  I haven't measured contention on scheduler inboxes, but I suspect one issue is that when a remote scheduler has decided it has no runnables, it will sleep in epoll; and to wake it up we need to write on a socketpair.  Guile can avoid that when there are lots of runnables and we see the remote scheduler isn't sleeping, but it's not perfect.</p><p>Secondly, Guile is a bytecode VM.  I measured that Guile retires about 0.4 billion instructions per second per core on the test machine, whereas a 4 IPC native program will retire about 10 billion.  There's overhead at various points, some of which will go away with native compilation in Guile but some might not for a while, given that Go (for example) has baked-in support for channels.  So to what extent is it the protocol and to what extent the implementation overhead?  I don't know.</p><p>Finally, and perhaps most importantly, we can't isolate this test from the garbage collector.  Guile still uses the Boehm GC, which is just OK I think.  It does have a nice parallel mark phase, but it uses POSIX signals to pause program threads instead of having those threads reach safepoints; and it's completely NUMA-unaware.</p><p>So, with all of those caveats mentioned, let's see a couple more graphs :) Firstly, similar to the previous one, here's total message send rate for N pairs of fibers that ping-pong their message back and forth.  Core allocation was randomized round-robin.</p><p><center><img style="max-width:100%;" src="//wingolog.org/pub/curry-on-2017-ping-pong.png" /></center></p><p>My conclusion here is that when more fibers are runnable per scheduler turn, the overhead of the epoll phase is less.</p><p>Here's a test where there's one fiber producer, and N fibers competing to consume the messages sent.  Ultimately we expect that the rate will be limited on the producer side, but there's still a nice speedup.</p><p><center><img style="max-width:100%;" src="//wingolog.org/pub/curry-on-2017-fan-out.png" /></center></p><p>Next is a pretty weak-sauce benchmark where we're computing diagonal lengths on an N-dimensional cube; the squares of the dimensions happen in parallel fibers, then one fiber collects those lengths, sums and makes a square root.</p><p><center><img style="max-width:100%;" src="//wingolog.org/pub/curry-on-2017-diagonal.png" /></center></p><p>The workload on that one is just very low, and the serial components become a bottleneck quickly.  I think I need to rework that test case.</p><p>Finally, there's a false sieve of Erastothenes, in which every time we find a prime, we add another fiber onto the sieve chain that filters out multiples of that prime.</p><p><center><img style="max-width:100%;" src="//wingolog.org/pub/curry-on-2017-sieve.png" /></center></p><p>Even though the workload is really small, we still see speedups, which is somewhat satisfying.  Still, on all of these, the NUMA cliff is something fierce.</p><p>For me what these benchmarks show is that there are still some bottlenecks to work on.  We do OK in the handful-of-cores scenario, but the system as a whole doesn't really scale past that.  On more real benchmarks with bigger workloads and proportionally much less communication, I get much more satisfactory results; but those tend to be I/O heavy anyway, so the bottleneck is elsewhere.</p><p><b>closing notes</b></p><p>There are other parts to CML events, namely guard functions and withNack functions.  My understanding is that these are implementable in terms of this &quot;primitive&quot; CML as described here; that was a result of earlier work by Matthew Fluet.  I haven't actually implemented these yet!  A to-do item, truly.</p><p>There are other event types in CML systems of course!  Besides being able to implement operations yourself, there are built-in condition variables (cvars), timeouts, thread join events, and so on.  The Fibers manual mentions some of these, but it's an open set.</p><p>Finally and perhaps most significantly, Aaron Turon did some work a few years ago on <a href="https://www.mpi-sws.org/~turon/reagents.pdf">&quot;Reagents&quot;</a>, a pattern library for composing parallel and concurrent operations, initially in Scala.  It's claimed that Reagents generalizes CML.  Is this the case?  I am looking forward to finding out.</p><p>OK, that's it for this verrrrry long post :) I hope that you found that this made parallel CML seem a bit more approachable and interesting, whether as a language implementor, a library implementor, or a user.  Comments and corrections welcome.  Check out <a href="https://github.com/wingo/fibers">Fibers</a> and give it a go!</p></div></div><div class="feedback"><a href="/archives/2017/06/29/a-new-concurrent-ml#comments">(25)</a></div></div><h2 class="storytitle"><a href="/archives/2017/06/27/growing-fibers">growing fibers</a></h2><div class="post"><h3 class="meta">27 June 2017 10:17 AM (<a href="/tags/guile">guile</a> | <a href="/tags/scheme">scheme</a> | <a href="/tags/gnu">gnu</a> | <a href="/tags/igalia">igalia</a> | <a href="/tags/compilers">compilers</a> | <a href="/tags/delimited%20continuations">delimited continuations</a> | <a href="/tags/prompts">prompts</a> | <a href="/tags/fibers">fibers</a> | <a href="/tags/concurrency">concurrency</a>)</h3><div class="storycontent"><div><p>Good day, Schemers!</p><p>Over the last 12 to 18 months, as we were preparing for the <a href="https://wingolog.org/archives/2017/03/15/guile-2-2-omg">Guile 2.2</a> release, I was growing increasingly dissatisfied at not having a good concurrency story in Guile.</p><p>I wanted to be able to spawn a million threads on a core, to support highly-concurrent I/O servers, and <a href="https://www.gnu.org/software/guile/manual/html_node/Threads.html">Guile's POSIX threads</a> are just not the answer.  I needed something different, and this article is about the search for and the implementation of that thing.</p><p><b>on pthreads</b></p><p>It's worth being specific why POSIX threads are not a great abstraction.  One is that they don't compose: two pieces of code that use mutexes won't necessarily compose together.  A correct component A that takes locks might call a correct component B that takes locks, and the other way around, and if both happen concurrently you get the classic deadly-embrace deadlock.</p><p>POSIX threads are also terribly low-level.  Asking someone to build a system with mutexes and cond vars is like building a house with exploding toothpicks.</p><p>I want to program network services in a straightforward way, and POSIX threads don't help me here either.  I'd like to spawn a million &quot;threads&quot; (scare-quotes!), one for each client, each one just just looping reading a request, computing and writing the response, and so on.  POSIX threads aren't the concrete implementation of this abstraction though, as in most systems you can't have more than a few thousand of them.</p><p>Finally as a Guile maintainer I have a duty to tell people the good ways to make their programs, but I can't in good conscience recommend POSIX threads to anyone.  If someone is a responsible programmer, then yes we can discuss details of POSIX threads.  But for a new Schemer?  Never.  Recommending POSIX threads is malpractice.</p><p><b>on scheme</b></p><p>In Scheme we claim to be minimalists.  Whether we actually are that or not is another story, but it's true that we have a culture of trying to grow expressive systems from minimal primitives.</p><p>It's sometimes claimed that in Scheme, we don't need threads because we have <tt>call-with-current-continuation</tt>, an ultrapowerful primitive that lets us implement any kind of control structure we want.  (The name screams for an abbreviation, so the alias <tt>call/cc</tt> is blessed; minimalism is whatever we say it is, right?)  Unfortunately it turned out that while <tt>call/cc</tt> can implement any control abstraction, it can't implement any two.  <a href="http://okmij.org/ftp/continuations/against-callcc.html#traps">Abstractions built on <tt>call/cc</tt> don't compose!</a></p><p>Fortunately, there is a way to build powerful control abstractions that do compose.  This article covers the first half of composing a concurrency facility out of a set of more basic primitives.</p><p>Just to be concrete, I have to start with a simple implementation of an event loop.  We're going to build on it later, but for now, here we go:</p><pre>
(define (run sched)
  (match sched
    (($ $sched inbox i/o)
     (define (dequeue-tasks)
       (append (dequeue-all! inbox)
               (poll-for-tasks i/o)))
     (let lp ()
       (for-each (lambda (task) (task))
                 (dequeue-tasks))
       (lp)))))
</pre><p>This is a scheduler that is a record with two fields, <i>inbox</i> and <i>i/o</i>.</p><p>The <i>inbox</i> holds a queue of pending tasks, as thunks (procedure of no arguments).  When something wants to enqueue a task, it posts a thunk to the inbox.</p><p>On the other hand, when a task needs to wait in some external input or output being available, it will register an event with <i>i/o</i>.  Typically <i>i/o</i> will be a simple combination of an <tt>epollfd</tt> and a mapping of tasks to enqueue when a file descriptor becomes readable or writable.  <tt>poll-for-tasks</tt> does the underlying <tt>epoll_wait</tt> call that pulls new I/O events from the kernel.</p><p>There are some details I'm leaving out, like when to have <tt>epoll_wait</tt> return directly, and when to have it wait for some time, and how to wake it up if it's sleeping while a task is posted to the scheduler's inbox, but ultimately this is the core of an event loop.</p><p><b>a long digression</b></p><p>Now you might think that I'm getting a little far afield from what my goal was, which was threads or fibers or something.  But that's OK, let's go a little farther and talk about &quot;prompts&quot;.  The term &quot;prompt&quot; comes from the experience you get when you work on the command-line:</p><pre>
/home/wingo% <i>./prog</i>
</pre><p>I don't know about you all, but I have the feeling that the <tt>/home/wingo%</tt> has a kind of solid reality, that my screen is not just an array of characters but there is a left-hand-side that belongs to the system, and a right-hand-side that's mine.  The two parts are delimited by a prompt.  Well prompts in Scheme allow you to provide this abstraction within your program: you can establish a program part that's a &quot;system&quot; facility, for whatever definition of &quot;system&quot; suits your purposes, and a part that's for the &quot;user&quot;.</p><p>In a way, prompts generalize a pattern of system/user division that has special facilities in other programming languages, such as a try/catch block.</p><pre>
try {
  <i>foo()</i>;
} catch (e) {
  bar();
}
</pre><p>Here again I put the &quot;user&quot; code in italics.  Some other examples of control flow patterns that prompts generalize would be early exit of a subcomputation, coroutines, and nondeterminitic choice like SICP's <tt>amb</tt> operator.  Coroutines is obviously where I'm headed here in the context of this article, but still there are some details to go over.</p><p>To make a prompt in Guile, you can use the <tt>%</tt> operator, which is pronounced &quot;prompt&quot;:</p><pre>
(use-modules (ice-9 control))

(% <i>expr</i>
   (lambda (k . args) #f))
</pre><p>The name for this operator comes from Dorai Sitaram's 1993 paper, <a href="http://www.ccs.neu.edu/scheme/pubs/pldi93-sitaram.pdf">Handling Control</a>; it's actually a pun on the <tt>tcsh</tt> prompt, if you must know.  Anyway the basic idea in this example is that we run <i>expr</i>, but if it aborts we run the <tt>lambda</tt> handler instead, which just returns <tt>#f</tt>.</p><p>Really <tt>%</tt> is just syntactic sugar for <tt>call-with-prompt</tt> though.  The previous example desugars to something like this:</p><pre>
(let ((tag (make-prompt-tag)))
  (call-with-prompt tag
    ;; Body:
    (lambda () <i>expr</i>)
    ;; Escape handler:
    (lambda (k . args) #f)))
</pre><p>(It's not quite the same; <a href="https://www.gnu.org/software/guile/manual/html_node/Shift-and-Reset.html"><tt>%</tt> uses a &quot;default prompt tag&quot;</a>.  This is just a detail though.)</p><p>You see here that <tt>call-with-prompt</tt> is really the primitive.  It will call the <i>body thunk</i>, but if an abort occurs within the body to the given <i>prompt tag</i>, then the body aborts and the <i>handler</i> is run instead.</p><p>So if you want to define a primitive that runs a function but allows early exit, we can do that:</p><pre>
(define-module (my-module)
  #:export (with-return))

(define-syntax-rule (with-return return body ...)
  (let ((t (make-prompt-tag)))
    (define (return . args)
      (apply abort-to-prompt t args))
    (call-with-prompt t
      (lambda () body ...)
      (lambda (k . rvals)
        (apply values rvals)))))
</pre><p>Here we define a module with a little <tt>with-return</tt> macro.  We can use it like this:</p><pre>
(use-modules (my-module))

(with-return return
  (+ 3 (return 42)))
;; =&gt; 42
</pre><p>As you can see, calling <tt>return</tt> within the body will abort the computation and cause the <tt>with-return</tt> expression to evaluate to the arguments passed to <tt>return</tt>.</p><p>But what's up with the handler?  Let's look again at the form of the <tt>call-with-prompt</tt> invocations we've been making.</p><pre>
(let ((tag (make-prompt-tag)))
  (call-with-prompt tag
    (lambda () ...)
    (lambda (k . args) ...)))
</pre><p>With the <tt>with-return</tt> macro, the handler took a first <i>k</i> argument, threw it away, and returned the remaining values.  But the first argument to the handler is pretty cool: it is the <i>continuation</i> of the computation that was aborted, <i>delimited</i> by the prompt: meaning, it's the part of the computation between the <tt>abort-to-prompt</tt> and the <tt>call-with-prompt</tt>, packaged as a function that you can call.</p><p>If you call the <i>k</i>, the delimited continuation, you reinstate it:</p><pre>
(define (f)
  (define tag (make-prompt-tag))
  (call-with-prompt tag
   (lambda ()
     (+ 3
        (abort-to-prompt tag)))
   (lambda (k) k)))

(let ((k (f)))
  (k 1))
;; =&amp; 4
</pre><p>Here, the <tt>abort-to-prompt</tt> invocation behaved simply like a &quot;suspend&quot; operation, returning the suspended computation <tt>k</tt>.  Calling that continuation resumes it, supplying the value 1 to the saved continuation <tt>(+ 3 [])</tt>, resulting in 4.</p><p>Basically, when a delimited continuation suspends, the first argument to the handler is a function that can resume the continuation.</p><p><b>tasks to fibers</b></p><p>And with that, we just built coroutines in terms of delimited continuations.  We can turn our scheduler inside-out, giving the illusion that each task runs in its own isolated fiber.</p><pre>
(define tag (make-prompt-tag))

(define (call/susp thunk)
  (define (handler k on-suspend) (on-suspend k))
  (call-with-prompt tag thunk handler))

(define (suspend on-suspend)
  (abort-to-prompt tag on-suspend))

(define (schedule thunk)
  (match (current-scheduler)
    (($ $sched inbox i/o)
     (enqueue! inbox (lambda () (call/susp thunk))))))
</pre><p>So!  Here we have a system that can run a thunk in a scheduler.  Fine.  No big deal.  But if the thunk calls <tt>suspend</tt>, then it causes an abort back to a prompt.  <tt>suspend</tt> takes a procedure as an argument, the <tt>on-suspend</tt> procedure, which will be called with one argument: the suspended continuation of the thunk.  We've layered coroutines on top of the event loop.</p><p>Guile's virtual machine is a normal register virtual machine with a stack composed of function frames.  It's not necessary to do full CPS conversion to implement delimited control, but if you don't, then your virtual machine needs primitive support for <tt>call-with-prompt</tt>, as Guile's VM does.  In Guile then, a suspended continuation is an object composed of the slice of the stack captured between the prompt and the abort, and also the slice of the dynamic stack.  (Guile keeps a parallel stack for dynamic bindings.  Perhaps we should unify these; dunno.)  This object is wrapped in a little procedure that uses VM primitives to push those stack frames back on, and continue.</p><p>I say all this just to give you a mental idea of what it costs to suspend a fiber.  It will allocate storage proportional to the stack depth between the prompt and the abort.  Usually this is a few dozen words, if there are 5 or 10 frames on the stack in the fiber.</p><p>We've gone from prompts to coroutines, and from here to fibers there's just a little farther to go.  First, note that spawning a new fiber is simply scheduling a thunk:</p><pre>
(define (spawn-fiber thunk)
  (schedule thunk))
</pre><p>Many threading libraries provide a &quot;yield&quot; primitive, which simply suspends the current thread, allowing others to run.  We can do this for fibers directly:</p><pre>
(define (yield)
  (suspend schedule))
</pre><p>Note that the <tt>on-suspend</tt> procedure here is just <tt>schedule</tt>, which re-schedules the continuation (but presumably at the back of the queue).</p><p>Similarly if we are reading on a non-blocking file descriptor and detect that we need more input before we can continue, but none is available, we can suspend and arrange for the <tt>epollfd</tt> to resume us later:</p><pre>
(define (wait-for-readable fd)
  (suspend
   (lambda (k)
     (match (current-scheduler)
       (($ $sched inbox i/o)
        (add-read-fd! i/o fd
                      (lambda () (schedule k))))))))
</pre><p>In Guile you can arrange to install this function as the &quot;current read waiter&quot;, causing it to run whenever a port would block.  The details are a little gnarly currently; see the <a href="https://www.gnu.org/software/guile/manual/html_node/Non_002dBlocking-I_002fO.html">Non-blocking I/O</a> manual page for more.</p><p>Anyway the cool thing is that I can run any thunk within a <tt>spawn-fiber</tt>, without modification, and it will run as if in a new thread of some sort.</p><p><b>solid abstractions?</b></p><p>I admit that although I am very happy with Emacs, I never really took to using the shell from within Emacs.  I always have a terminal open with a bunch of tabs.  I think the reason for that is that I never quite understood why I could move the cursor over the bash prompt, or into previous expressions or results; it seemed like I was waking up groggily from some kind of dream where nothing was real.  I like the terminal, where the only bit that's &quot;mine&quot; is the current command.  All the rest is immutable text in the scrollback.</p><p>Similarly when you make a UI, you want to design things so that people perceive the screen as being composed of buttons and so on, not just lines.  In essence you trick the user, a willing user who is ready to be tricked, into seeing buttons and text and not just weird pixels.</p><p>In the same way, with fibers we want to provide the illusion that fibers actually exist.  To solidify this illusion, we're still missing a few elements.</p><p>One point relates to error handling.  As it is, if an error happens in a fiber and the fiber doesn't handle it, the exception propagates out of the fiber, through the scheduler, and might cause the whole program to error out.  So we need to wrap fibers in a catch-all.</p><pre>
(define (spawn-fiber thunk)
  (schedule
   (lambda ()
     (catch #t thunk
       (lambda (key . args)
         (print-exception (current-error-port) #f key args))))))
</pre><p>Well, OK.  Exceptions won't propagate out of fibers, yay.  In fact in Guile we add another catch inside the print-exception, in case the print-exception throws an exception...  Anyway.  Cool.</p><p>Another point relates to fiber-local variables.  In an operating system, each process has a number of variables that are local to it, notably in UNIX we have the umask, the current effective user, the current directory, the open files and what file descriptors they are associated with, and so on.  In Scheme we have similar facilities in the form of <i>parameters</i>.</p><p>Now the usual way that parameters are used is to bind a new value within the extent of some call:</p><pre>
(define (with-output-to-string thunk)
  (let ((p (open-output-string)))
    (parameterize ((current-output-port p))
      (thunk))
    (get-output-string p)))
</pre><p>Here the <tt>parameterize</tt> invocation established <tt>p</tt> as the current output port during the call to <tt>thunk</tt>.  Parameters already compose quite well with prompts; Guile, like Racket, implements the protocol described by Kiselyov, Shan, and Sabry in their <a href="http://okmij.org/ftp/papers/DDBinding.pdf">Delimited Dynamic Binding</a> paper (well worth a read!).</p><p>The one missing piece is that parameters in Scheme are mutable (by default).  Normally if you call <tt>(current-input-port)</tt>, you just get the current value of the current input port parameter.  But if you pass an argument, like <tt>(current-input-port p)</tt>, then you actually set the current input port to that new value.  This value will be in place until we leave some <tt>parameterize</tt> invocation that parameterizes the current input port.</p><p>The problem here is that it could be that there's an interesting parameter which some piece of Scheme code will want to just mutate, so that all further Scheme code will use the new value.  This is fine if you have no concurrency: there's just one thing running.  But when you have many fibers, you want to avoid mutations in one fiber from affecting others.  You want some isolation with regards to parameters.  In Guile, we do this with the <a href="https://www.gnu.org/software/guile/manual/html_node/Fluids-and-Dynamic-States.html">with-dynamic-state</a> facility, which isolates changes to the dynamic state (parameters and so on) within the extent of the <tt>with-dynamic-state</tt> call.</p><pre>
(define (spawn-fiber thunk)
  (let ((state (current-dynamic-state)))
    (schedule
     (lambda ()
       (catch #t
         (lambda ()
           (with-dynamic-state state thunk))
         (lambda (key . args)
           (print-exception (current-error-port) #f key args))))))
</pre><p>Interestingly, <tt>with-dynamic-state</tt> solves another problem as well.  You would like for newly spawned fibers to inherit the parameters from the point at which they were spawned.</p><pre>
(parameterize ((current-output-port p))
  (spawn-fiber
   ;; New fiber should inherit current-output-port
   ;; binding as &quot;p&quot;
   (lambda () ...)))
</pre><p>Capturing the <tt>(current-dynamic-state)</tt> outside the thunk does this for us.</p><p>When I made this change in Guile, making sure that <tt>with-dynamic-state</tt> did not impose a <a href="https://github.com/wingo/fibers/wiki/Manual#Barriers">continuation barrier</a>, I ran into a problem.  In Guile we implemented <a href="https://wingolog.org/archives/2010/02/14/sidelong-glimpses">exceptions in terms of delimited continuations and dynamic binding</a>.  The current stack of exception handlers was a list, and each element included the exceptions handled by that handler, and what prompt to which to abort before running the exception handler.  See where the problem is?  If we ship this exception handler stack over to a new fiber, then an exception propagating out of the new fiber would be looking up handlers from another fiber, for prompts that probably aren't even on the stack any more.</p><p>The problem here is that if you store a heap-allocated stack of current exception handlers in a dynamic variable, and that dynamic variable is captured somehow (say, by a delimited continuation), then you capture the whole stack of handlers, not (in the case of delimited continuations) the delimited set of handlers that were active within the prompt.  To fix this, we had to change Guile's exceptions to instead make <tt>catch</tt> just rebind the exception handler parameter to hold the handler installed by the <tt>catch</tt>.  If Guile needs to walk the chain of exception handlers, we introduced a new primitive <tt>fluid-ref*</tt> to do so, building the chain from the current stack of parameterizations instead of some representation of that stack on the heap.  It's O(<i>n</i>), but life is that way sometimes.  This way also, delimited continuations capture the right set of exception handlers.</p><p>Finally, Guile also supports <a href="https://www.gnu.org/software/guile/manual/html_node/Asyncs.html">asynchronous interrupts</a>.  We can arrange to interrupt a Guile process (or POSIX thread) every so often, as measured in wall-clock or process time.  It used to be that interrupt handlers caused a continuation barrier, but this is no longer the case, so now we can add pre-emption to a fibers using interrupts.</p><p><b>summary and reflections</b></p><p>In Guile we were able to create a solid-seeming abstraction for fibers by composing other basic building blocks from the Scheme toolkit.  Guile users can take an abstraction that's implemented in terms of an event loop (any event loop) and layer fibers on top in a way that feels &quot;real&quot;.  We were able to do this because we have prompts (delimited continuation) and parameters (dynamic binding), and we were able to compose the two.  Actually getting it all to work required fixing a few bugs.</p><p>In Fibers, we just use delimited continuations to implement coroutines, and then our fibers are coroutines.  If we had coroutines as a primitive, that would work just as well.  As it is, each suspension of a fiber will allocate a new continuation.  Perhaps this is unimportant, given the average continuation size, but it would be comforting in a way to be able to re-use the allocation from the previous suspension (if any).  <a href="https://wingolog.org/archives/2013/02/25/on-generators">Other languages with coroutine primitives</a> might have an advantage here, though delimited dynamic binding is still relatively uncommon.</p><p>Another point is that because we use prompts to suspend fiberss, we effectively are always unwinding and rewinding the dynamic state.  In practice this should be transparent to the user and the implementor should make this transparent from a performance perspective, with the exception of <tt>dynamic-wind</tt>.  Basically any fiber suspension will be run the &quot;out&quot; guard of any enclosing <tt>dynamic-wind</tt>, and resumption will run the &quot;in&quot; guard.  In practice we find that we defer &quot;finalization&quot; issues to <tt>with-throw-handler</tt> / <tt>catch</tt>, which unlike <tt>dynamic-wind</tt> don't run on every entry or exit of a dynamic extent and rather just run on exceptional exits.  We will see over time if this situation is acceptable.  It's certainly another nail in the coffin of <tt>dynamic-wind</tt> though.</p><p>This article started with pthreads malaise, and although we've solved the problem of having a million fibers, we haven't solved the communications problem.  How should fibers communicate with each other?  This is the topic for my next article.  Until then, happy hacking :)</p></div></div><div class="feedback"><a href="/archives/2017/06/27/growing-fibers#comments">(13)</a></div></div><h2 class="storytitle"><a href="/archives/2017/06/26/an-early-look-at-p4-for-software-networking">an early look at p4 for software networking</a></h2><div class="post"><h3 class="meta">26 June 2017  2:00 PM (<a href="/tags/igalia">igalia</a> | <a href="/tags/networking">networking</a> | <a href="/tags/p4">p4</a> | <a href="/tags/compilers">compilers</a> | <a href="/tags/snabb">snabb</a> | <a href="/tags/vpp">vpp</a>)</h3><div class="storycontent"><div><p>Happy midsummer, hackfriends!</p><p>As you know <a href="https://www.igalia.com/networking">at work</a> we have been trying to find ways to apply compilers technology to the networking space.  We will compile <a href="https://github.com/snabbco/snabb/blob/master/src/program/config/README.md">high-level configurations into low-level network processing graphs</a>, <a href="https://github.com/snabbco/snabb/blob/master/src/lib/binary_search.dasl">search algorithms into lookup routines optimized for the target data structures</a>, <a href="https://github.com/snabbco/snabb/tree/master/lib/pflua">packet filters into code ready to be further trace-compiled</a>, or <a href="https://github.com/snabbco/snabb/pull/1155">hash functions into parallel AVX2 code</a>.</p><p>On one side, we try to provide fast implementations of existing &quot;languages&quot;; on the other side we can't help but try out new co-designed domain-specific languages that can be expressive and run fast.  As an example, with <a href="https://wingolog.org/archives/2015/07/03/pfmatch-a-packet-filtering-language-embedded-in-lua">pfmatch</a> we extended pflang, the tcpdump language, with a more match-action kind of semantics.  <a href="https://www.asumu.xyz/blog/2017/01/13/making-a-snabb-app-with-pfmatch/">It worked fine but the embedding between pfmatch and the host language could have been more smooth</a>; in the end the abstractions it offers don't really apply to what we have needed to build.  For a long time we have been wondering if indeed there is a better domain-specific programming language to apply to the networking domain.</p><p><a href="http://p4.org/">P4</a> claims to be this language, and I think it's worth a look.  P4's goal is be able to define switches and other networking equipment in software, with the specific goal that they would like to be able for P4 programs to be synthesized to ASICs, or installed in the FPGA of a <a href="https://www.google.fr/search?q=smart+nic">&quot;Smart NIC&quot;</a>, or compiled to CPUs.  It's a wide target domain and the silicon-bakery side of things definitely constrains what is possible.  Indeed P4 explicitly disclaims any ambition to be a general-purpose programming language.  Still, I think they manage to achieve an admirable balance between declarative programming and transparent low-level compilability.</p><p>The best, most current intro to P4 out there is probably <a href="http://p4.org/wp-content/uploads/2017/05/p4_d2_2017_p4_16_tutorial.pdf">Vladimir Gurevich's slides from last month's P4 &quot;developer day&quot; in California</a>.  I think it does a good job linking the language's syntax and semantics with how they are intended to be applied to the target domain.  For a more PL-friendly and abstract introduction, the <a href="https://p4lang.github.io/p4-spec/docs/P4-16-v1.0.0-spec.html">P4<sub>16</sub> specification</a> is a true delight.</p><p>Like I said, at work we build software switches and other network functions, and our target is commodity hardware.  We write most of our work in <a href="https://github.com/snabbco/snabb">Snabb</a>, a powerful network toolkit built on LuaJIT, though we are branching out now to <a href="https://wiki.fd.io/view/VPP">VPP/fd.io</a> as well, just to broaden the offering a bit.  Generally we try to build solutions that don't have any dependencies other than a commodity Xeon server and a commodity NIC like Intel's 82599.  So how could P4 help us in what we're doing?</p><p>My first thought in this regard was that if there is a library of P4 building blocks out there, that it would be really convenient to be able to incorporate a functional block written in P4 within the graph of a Snabb program.  For example, if we have an IPFIX collector written in Snabb (and we do!), it would be cool to stick that in the middle of a P4 traffic conditioner.</p><p>(Immediately I run into the problem that I am straining my mind to think of a network function that we wouldn't rather just write in Snabb -- something valuable enough that we wouldn't want to &quot;own&quot; it and instead we would import someone else's black box into our data-plane.  Maybe this interesting <a href="http://p4.org/wp-content/uploads/2017/06/p4-ws-2017-netcache.pdf">in-network key-value cache</a> counts?  But I digress, let's assume that something exists here.)</p><p>One question is, why bother doing P4 in software?  I can understand that if you have 1Tbps ports that you definitely need custom silicon pushing your packets around.  You would like to be able to program that silicon, so P4 looks to be a compelling step forward.  But if your needs are satisfied with 40Gbps ports and you have chosen a software networking solution for its low cost, low lock-in, high flexibility, and sufficient performance -- well does P4 buy you something?</p><p>Right now it would seem that the answer is &quot;no&quot;.  A Cisco group wrote <a href="http://www.cs.princeton.edu/~mshahbaz/papers/sosr17demos-pvpp.pdf">a custom P4 compiler to VPP</a>, which is architecturally pretty much the same as Snabb, and they had to do some work to get the performance within a couple percent of the hand-coded application.  The only win I can see is if people start building up libraries of reusable P4 components that can be linked together -- but the language itself currently doesn't support any more global composition primitive than <tt>#include</tt> (yes, it uses CPP :).</p><p>Additionally, at least as things are now, it doesn't seem that there's a library of reusable, open source P4 components out there to take advantage of.  If this changes, I'll have to have another look.  And of course it's worth keeping an eye on what kinds of <a href="http://p4.org/wp-content/uploads/2017/06/p4-ws-2017-netcache.pdf">cool</a> <a href="http://p4.org/wp-content/uploads/2017/06/02-p4-to-wireshark.pdf">things</a> people are building :)</p><p><i>Thanks to Luke Gorrie for conversations leading to this blog post.  All opinions and errors mine, of course!</i></p></div></div><div class="feedback"><a href="/archives/2017/06/26/an-early-look-at-p4-for-software-networking#comments">(4)</a></div></div><h2 class="storytitle"><a href="/archives/2017/03/15/guile-2-2-omg">guile 2.2 omg!!!</a></h2><div class="post"><h3 class="meta">15 March 2017 10:56 PM (<a href="/tags/guile">guile</a> | <a href="/tags/scheme">scheme</a> | <a href="/tags/gnu">gnu</a> | <a href="/tags/igalia">igalia</a> | <a href="/tags/compilers">compilers</a> | <a href="/tags/academia">academia</a> | <a href="/tags/relief">relief</a>)</h3><div class="storycontent"><div><p>Oh, good evening my hackfriends!  I am just chuffed to share a thing with yall: tomorrow we release Guile 2.2.0.  Yaaaay!</p><p>I know in these days of version number inflation that this seems like a very incremental, point-release kind of a thing, but it's a big deal to me.  This is a project I have been working on since soon after <a href="https://wingolog.org/archives/2011/02/16/guile-is-out">the release of Guile 2.0</a> some 6 years ago.  It wasn't always clear that this project would work, but now it's here, going into production.</p><p>In that time I have worked on JavaScriptCore and V8 and SpiderMonkey and so I got a feel for what a state-of-the-art programming language implementation looks like.  Also in that time I ate and breathed optimizing compilers, and really hit the wall until finally paging in what Fluet and Weeks were saying so many years ago about <a href="https://sourceforge.net/p/mlton/mailman/message/31023809/">continuation-passing style and scope</a>, and eventually came through with a solution that was still CPS: <a href="https://vimeo.com/182585166">CPS soup</a>.  At this point Guile's &quot;middle-end&quot; is, I think, totally respectable.  The backend targets a <a href="https://www.gnu.org/software/guile/manual/html_node/A-Virtual-Machine-for-Guile.html#A-Virtual-Machine-for-Guile">quite good virtual machine</a>.</p><p>The virtual machine is still a bytecode interpreter for now; native code is a next step.  Oddly my journey here has been precisely opposite, in a way, to <a href="http://scheme2006.cs.uchicago.edu/11-ghuloum.pdf">An incremental approach to compiler construction</a>; incremental, yes, but starting from the other end.  But I am very happy with where things are.  Guile remains very portable, bootstrappable from C, and the compiler is in a good shape to take us the rest of the way to register allocation and native code generation, and <a href="https://ecraven.github.io/r7rs-benchmarks/benchmark.html">performance is pretty ok</a>, even better than some natively-compiled Schemes.</p><p>For a &quot;scripting&quot; language (what does that mean?), I also think that Guile is breaking nice ground by using <a href="https://wingolog.org/archives/2014/01/19/elf-in-guile">ELF as its object file format</a>.  Very cute.  As this seems to be a &quot;Andy mentions things he's proud of&quot; segment, I was also pleased with how <a href="https://wingolog.org/archives/2014/03/17/stack-overflow">we were able to completely remove the stack size restriction</a>.</p><p><b>high fives all around</b></p><p>As is often the case with these things, I got the idea for removing the stack limit after talking with Sam Tobin-Hochstadt from Racket and the PLT group.  I admire Racket and its makers very much and look forward to <strike>stealing from</strike>working with them in the future.</p><p>Of course the ideas for the <a href="https://wingolog.org/archives/2016/02/08/a-lambda-is-not-necessarily-a-closure">contification and closure optimization passes</a> are in debt to Matthew Fluet and Stephen Weeks for the former, and Andy Keep and Kent Dybvig for the the latter.  The intmap/intset representation of CPS soup itself is highly endebted to the late Phil Bagwell, to Rich Hickey, and to Clojure folk; persistent data structures were an amazing revelation to me.</p><p>Guile's virtual machine itself was initially heavily inspired by JavaScriptCore's VM.  Thanks to WebKit folks for writing so much about the early days of Squirrelfish!  As far as the actual optimizations in the compiler itself, I was inspired a lot by V8's Crankshaft in a weird way -- it was my first touch with fixed-point flow analysis.  As most of yall know, I didn't study CS, for better and for worse; for worse, because I didn't know a lot of this stuff, and for better, as I had the joy of learning it as I needed it.  Since starting with flow analysis, Carl Offner's <a href="http://www.cs.umb.edu/~offner/files/flow_graph.pdf">Notes on graph algorithms used in optimizing compilers</a> was invaluable.  I still open it up from time to time.</p><p>While I'm high-fiving, large ups to two amazing support teams: firstly to my colleagues at <a href="https://igalia.com">Igalia</a> for supporting me on this.  Almost the whole time I've been at Igalia, I've been working on this, for about a day or two a week.  Sometimes at work we get to take advantage of a Guile thing, but Igalia's Guile investment mainly pays out in the sense of keeping me happy, keeping me up to date with language implementation techniques, and attracting talent.  At work we have a lot of language implementation people, in JS engines obviously but also in other niches like the networking group, and it helps to be able to transfer hackers from Scheme to these domains.</p><p>I put in my own time too, of course; but my time isn't really my own either.  My wife Kate has been really supportive and understanding of my not-infrequent impulses to just nerd out and hack a thing.  She probably won't read this (though maybe?), but it's important to acknowledge that many of us hackers are only able to do our work because of the support that we get from our families.</p><p><b>a digression on the nature of seeking and knowledge</b></p><p>I am jealous of my colleagues in academia sometimes; of course it must be this way, that we are jealous of each other.  Greener grass and all that.  But when you go through a doctoral program, you know that you <a href="http://matt.might.net/articles/phd-school-in-pictures/">push the boundaries of human knowledge</a>.  You know because you are acutely aware of the state of recorded knowledge in your field, and you know that your work expands that record.  If you stay in academia, you use your honed skills to continue chipping away at the unknown.  The papers that this process reifies have a huge impact on the flow of knowledge in the world.  As just one example, I've read all of Dybvig's papers, with delight and pleasure and avarice and jealousy, and learned loads from them.  (Incidentally, I am given to understand that all of these are proper academic reactions :)</p><p>But in my work on Guile I don't actually know that I've expanded knowledge in any way.  I don't actually know that anything I did is new and suspect that nothing is.  Maybe CPS soup?  There have been some similar publications in the last couple years but you never know.  Maybe some of the multicore Concurrent ML stuff I haven't written about yet.  Really not sure.  I am starting to see papers these days that are similar to what I do and I have the feeling that they have a bit more impact than my work because of their medium, and I wonder if I could be putting my work in a more useful form, or orienting it in a more newness-oriented way.</p><p>I also don't know how important new knowledge is.  Simply being able to practice language implementation at a state-of-the-art level is a valuable skill in itself, and releasing a quality, stable free-software language implementation is valuable to the world.  So it's not like I'm negative on where I'm at, but I do feel wonderful talking with folks at academic conferences and wonder how to pull some more of that into my life.</p><p>In the meantime, I feel like (my part of) Guile 2.2 is my master work in a way -- a savepoint in my hack career.  It's fine work; see <a href="https://www.gnu.org/software/guile/manual/html_node/A-Virtual-Machine-for-Guile.html#A-Virtual-Machine-for-Guile">A Virtual Machine for Guile</a> and <a href="https://www.gnu.org/software/guile/manual/html_node/Continuation_002dPassing-Style.html#Continuation_002dPassing-Style">Continuation-Passing Style</a> for some high level documentation, or many of these bloggies for the nitties and the gritties.  OKitties!</p><p><b>getting the goods</b></p><p>It's been a joy over the last two or three years to see the growth of <a href="https://www.gnu.org/software/guix/">Guix</a>, a packaging system written in Guile and inspired by <a href="https://www.gnu.org/software/stow/">GNU stow</a> and <a href="http://nixos.org/nix/">Nix</a>.  The laptop I'm writing this on runs GuixSD, and Guix is up to some 5000 packages at this point.</p><p>I've always wondered what the right solution for packaging Guile and Guile modules was.  At one point I thought that <a href="https://wingolog.org/archives/2011/07/04/guile-2-0-2-building-the-guildhall">we would have a Guile-specific packaging system</a>, but one with stow-like characteristics.  We had problems with C extensions though: how do you build one?  Where do you get the compilers?  Where do you get the libraries?</p><p>Guix solves this in a comprehensive way.  From the four or five bootstrap binaries, Guix can download and build the world from source, for any of its supported architectures.  The result is a farm of weirdly-named files in <tt>/gnu/store</tt>, but the transitive closure of a store item works on <i>any</i> distribution of that architecture.</p><p>This state of affairs was clear from the Guix <a href="https://www.gnu.org/software/guix/manual/html_node/Binary-Installation.html">binary installation instructions</a> that just have you extract a tarball over your current distro, regardless of what's there.  The process of building this weird tarball was always a bit ad-hoc though, geared to Guix's installation needs.</p><p>It turns out that we can use the same strategy to distribute reproducible binaries for any package that Guix includes.  So if you download <a href="https://ftp.gnu.org/gnu/guile/guile-2.2.0-pack-x86_64-linux-gnu.tar.lz">this tarball</a>, and extract it as root in <tt>/</tt>, then it will extract some paths in <tt>/gnu/store</tt> and also add a <tt>/opt/guile-2.2.0</tt>.  Run Guile as <tt>/opt/guile-2.2.0/bin/guile</tt> and you have Guile 2.2, before any of your friends!  That pack was made using <tt>guix pack -C lzip -S /opt/guile-2.2.0=/ guile-next glibc-utf8-locales</tt>, at Guix git revision <tt>80a725726d3b3a62c69c9f80d35a898dcea8ad90</tt>.</p><p>(If you run that Guile, it will complain about not being able to install the locale.  Guix, like Scheme, is generally a statically scoped system; but locales are dynamically scoped.  That is to say, you have to set <tt>GUIX_LOCPATH=/opt/guile-2.2.0/lib/locale</tt> in the environment, for locales to work.  See the <a href="https://www.gnu.org/software/guix/manual/html_node/Application-Setup.html#Application-Setup">GUIX_LOCPATH docs</a> for the gnarlies.)</p><p>Alternately of course you can install Guix and just <tt>guix package -i guile-next</tt>.  Guix itself will migrate to 2.2 over the next week or so.</p><p>Welp, that's all for this evening.  I'll be relieved to push the release tag and announcements tomorrow.  In the meantime, happy hacking, and yes: this blog is served by Guile 2.2! :)</p></div></div><div class="feedback"><a href="/archives/2017/03/15/guile-2-2-omg#comments">(23)</a></div></div></div><div id="footer">powered by <a href="//wingolog.org/software/tekuti/">tekuti</a></div></div></body></html>