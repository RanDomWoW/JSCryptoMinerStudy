<!DOCTYPE HTML>
<html>

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Arxiv Sanity Preserver</title>

<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML">
</script>

<!-- CSS -->
<link rel="stylesheet" type="text/css" href="/static/style.css">

<!-- Favicon -->
<link rel="shortcut icon" type="image/png" href="/static/favicon.png" />

<!-- JS -->
<script src="/static/jquery-1.8.3.min.js"></script>
<script src="/static/d3.min.js"></script>
<script src="/static/as-common.js"></script>

<!-- Google Analytics JS -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-3698471-25', 'auto');
  ga('send', 'pageview');

</script>

<script>

// passed in from flask as json
var tweets = [];
var papers = [{"abstract": "In the past decade, Convolutional Neural Networks (CNNs) have demonstrated\nstate-of-the-art performance in various Artificial Intelligence tasks. To\naccelerate the experimentation and development of CNNs, several software\nframeworks have been released, primarily targeting power-hungry CPUs and GPUs.\nIn this context, reconfigurable hardware in the form of FPGAs constitutes a\npotential alternative platform that can be integrated in the existing deep\nlearning ecosystem to provide a tunable balance between performance, power\nconsumption and programmability. In this paper, a survey of the existing\nCNN-to-FPGA toolflows is presented, comprising a comparative study of their key\ncharacteristics which include the supported applications, architectural\nchoices, design space exploration methods and achieved performance. Moreover,\nmajor challenges and objectives introduced by the latest trends in CNN\nalgorithmic research are identified and presented. Finally, a uniform\nevaluation methodology is proposed, aiming at the comprehensive, complete and\nin-depth evaluation of CNN-to-FPGA toolflows.", "authors": ["Stylianos I. Venieris", "Alexandros Kouris", "Christos-Savvas Bouganis"], "category": "cs.CV", "comment": "Accepted for publication at the ACM Computing Surveys (CSUR) journal,\n  2018", "img": "/static/thumbs/1803.05900v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05900v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05900v1", "published_time": "3/15/2018", "rawpid": "1803.05900", "tags": ["cs.CV", "cs.AR", "cs.LG"], "title": "Toolflows for Mapping Convolutional Neural Networks on FPGAs: A Survey\n  and Future Directions"}, {"abstract": "Deep neural networks (DNNs) have a wide range of applications, and software\nemploying them must be thoroughly tested, especially in safety critical\ndomains. However, traditional software testing methodology, including test\ncoverage criteria and test case generation algorithms, cannot be applied\ndirectly to DNNs. This paper bridges this gap. First, inspired by the\ntraditional MC/DC coverage criterion, we propose a set of four test criteria\nthat are tailored to the distinct features of DNNs. Our novel criteria are\nincomparable and complement each other. Second, for each criterion, we give an\nalgorithm for generating test cases based on linear programming (LP). The\nalgorithms produce a new test case (i.e., an input to the DNN) by perturbing a\ngiven one. They encode the test requirement and a fragment of the DNN by fixing\nthe activation pattern obtained from the given input example, and then minimize\nthe difference between the new and the current inputs. Finally, we validate our\nmethod on a set of networks trained on the MNIST dataset. The utility of our\nmethod is shown experimentally with four objectives: (1) bug finding; (2) DNN\nsafety statistics; (3) testing efficiency and (4) DNN internal structure\nanalysis.", "authors": ["Youcheng Sun", "Xiaowei Huang", "Daniel Kroening"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1803.04792v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04792v2", "num_discussion": 0, "originally_published_time": "3/10/2018", "pid": "1803.04792v2", "published_time": "3/15/2018", "rawpid": "1803.04792", "tags": ["cs.LG", "cs.CV", "cs.SE"], "title": "Testing Deep Neural Networks"}, {"abstract": "Biological and artificial neural systems are composed of many local\nprocessors, and their capabilities depend upon the transfer function that\nrelates each local processor\u0027s outputs to its inputs. This paper uses a recent\nadvance in the foundations of information theory to study the properties of\nlocal processors that use contextual input to amplify or attenuate transmission\nof information about their driving inputs. This advance enables the information\ntransmitted by processors with two distinct inputs to be decomposed into those\ncomponents unique to each input, that shared between the two inputs, and that\nwhich depends on both though it is in neither, i.e. synergy. The decompositions\nthat we report here show that contextual modulation has information processing\nproperties that contrast with those of all four simple arithmetic operators,\nthat it can take various forms, and that the form used in our previous studies\nof artificial neural nets composed of local processors with both driving and\ncontextual inputs is particularly well-suited to provide the distinctive\ncapabilities of contextual modulation under a wide range of conditions. We\nargue that the decompositions reported here could be compared with those\nobtained from empirical neurobiological and psychophysical data under\nconditions thought to reflect contextual modulation. That would then shed new\nlight on the underlying processes involved. Finally, we suggest that such\ndecompositions could aid the design of context-sensitive machine learning\nalgorithms.", "authors": ["Jim W. Kay", "William A. Phillips"], "category": "cs.IT", "comment": "23 pages, 6 figures", "img": "/static/thumbs/1803.05897v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05897v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05897v1", "published_time": "3/15/2018", "rawpid": "1803.05897", "tags": ["cs.IT", "math.IT", "q-bio.NC", "q-bio.QM", "stat.ML"], "title": "Contrasting information theoretic decompositions of modulatory and\n  arithmetic interactions in neural information processing systems"}, {"abstract": "Recent advances in facial landmark detection achieve success by learning\ndiscriminative features from rich deformation of face shapes and poses. Besides\nthe variance of faces themselves, the intrinsic variance of image styles, e.g.,\ngrayscale vs. color images, light vs. dark, intense vs. dull, and so on, has\nconstantly been overlooked. This issue becomes inevitable as increasing web\nimages are collected from various sources for training neural networks. In this\nwork, we propose a style-aggregated approach to deal with the large intrinsic\nvariance of image styles for facial landmark detection. Our method transforms\noriginal face images to style-aggregated images by a generative adversarial\nmodule. The proposed scheme uses the style-aggregated image to maintain face\nimages that are more robust to environmental changes. Then the original face\nimages accompanying with style-aggregated ones play a duet to train a landmark\ndetector which is complementary to each other. In this way, for each face, our\nmethod takes two images as input, i.e., one in its original style and the other\nin the aggregated style. In experiments, we observe that the large variance of\nimage styles would degenerate the performance of facial landmark detectors.\nMoreover, we show the robustness of our method to the large variance of image\nstyles by comparing to a variant of our approach, in which the generative\nadversarial module is removed, and no style-aggregated images are used. Our\napproach is demonstrated to perform well when compared with state-of-the-art\nalgorithms on benchmark datasets AFLW and 300-W. Code is publicly available on\nGitHub: https://github.com/D-X-Y/SAN", "authors": ["Xuanyi Dong", "Yan Yan", "Wanli Ouyang", "Yi Yang"], "category": "cs.CV", "comment": "Accepted to CVPR 2018", "img": "/static/thumbs/1803.04108v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04108v3", "num_discussion": 0, "originally_published_time": "3/12/2018", "pid": "1803.04108v3", "published_time": "3/15/2018", "rawpid": "1803.04108", "tags": ["cs.CV"], "title": "Style Aggregated Network for Facial Landmark Detection"}, {"abstract": "In this work we describe a novel deep reinforcement learning neural network\narchitecture that allows multiple actions to be selected at every time-step.\nMulti-action policies allows complex behaviors to be learnt that are otherwise\nhard to achieve when using single action selection techniques. This work\ndescribes an algorithm that uses both imitation learning (IL) and temporal\ndifference (TD) reinforcement learning (RL) to provide a 4x improvement in\ntraining time and 2.5x improvement in performance over single action selection\nTD RL. We demonstrate the capabilities of this network using a complex in-house\n3D game. Mimicking the behavior of the expert teacher significantly improves\nworld state exploration and allows the agents vision system to be trained more\nrapidly than TD RL alone. This initial training technique kick-starts TD\nlearning and the agent quickly learns to surpass the capabilities of the\nexpert.", "authors": ["Jack Harmer", "Linus Gissl\u00e9n", "Henrik Holst", "Joakim Bergdahl", "Tom Olsson", "Kristoffer Sj\u00f6\u00f6", "Magnus Nordin"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1803.05402v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05402v2", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05402v2", "published_time": "3/15/2018", "rawpid": "1803.05402", "tags": ["cs.AI", "cs.LG", "stat.ML"], "title": "Imitation Learning with Concurrent Actions in 3D Games"}, {"abstract": "In this paper, we present GossipGraD - a gossip communication protocol based\nStochastic Gradient Descent (SGD) algorithm for scaling Deep Learning (DL)\nalgorithms on large-scale systems. The salient features of GossipGraD are: 1)\nreduction in overall communication complexity from {\\Theta}(log(p)) for p\ncompute nodes in well-studied SGD to O(1), 2) model diffusion such that compute\nnodes exchange their updates (gradients) indirectly after every log(p) steps,\n3) rotation of communication partners for facilitating direct diffusion of\ngradients, 4) asynchronous distributed shuffle of samples during the\nfeedforward phase in SGD to prevent over-fitting, 5) asynchronous communication\nof gradients for further reducing the communication cost of SGD and GossipGraD.\nWe implement GossipGraD for GPU and CPU clusters and use NVIDIA GPUs (Pascal\nP100) connected with InfiniBand, and Intel Knights Landing (KNL) connected with\nAries network. We evaluate GossipGraD using well-studied dataset ImageNet-1K\n(~250GB), and widely studied neural network topologies such as GoogLeNet and\nResNet50 (current winner of ImageNet Large Scale Visualization Research\nChallenge (ILSVRC)). Our performance evaluation using both KNL and Pascal GPUs\nindicates that GossipGraD can achieve perfect efficiency for these datasets and\ntheir associated neural network topologies. Specifically, for ResNet50,\nGossipGraD is able to achieve ~100% compute efficiency using 128 NVIDIA Pascal\nP100 GPUs - while matching the top-1 classification accuracy published in\nliterature.", "authors": ["Jeff Daily", "Abhinav Vishnu", "Charles Siegel", "Thomas Warfel", "Vinay Amatya"], "category": "cs.DC", "comment": "13 pages, 17 figures", "img": "/static/thumbs/1803.05880v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05880v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05880v1", "published_time": "3/15/2018", "rawpid": "1803.05880", "tags": ["cs.DC", "cs.LG"], "title": "GossipGraD: Scalable Deep Learning using Gossip Communication based\n  Asynchronous Gradient Descent"}, {"abstract": "Facial expressions are combinations of basic components called Action Units\n(AU). Recognizing AUs is key for developing general facial expression analysis.\nIn recent years, most efforts in automatic AU recognition have been dedicated\nto learning combinations of local features and to exploiting correlations\nbetween Action Units. In this paper, we propose a deep neural architecture that\ntackles both problems by combining learned local and global features in its\ninitial stages and replicating a message passing algorithm between classes\nsimilar to a graphical model inference approach in later stages. We show that\nby training the model end-to-end with increased supervision we improve\nstate-of-the-art by 5.3\\% and 8.2\\% performance on BP4D and DISFA datasets,\nrespectively.", "authors": ["Ciprian A. Corneanu", "Meysam Madadi", "Sergio Escalera"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05873v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05873v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05873v1", "published_time": "3/15/2018", "rawpid": "1803.05873", "tags": ["cs.CV"], "title": "Deep Structure Inference Network for Facial Action Unit Recognition"}, {"abstract": "In this paper we introduce an ensemble method for convolutional neural\nnetwork (CNN), called \"virtual branching,\" which can be implemented with nearly\nno additional parameters and computation on top of standard CNNs. We propose\nour method in the context of person re-identification (re-ID). Our CNN model\nconsists of shared bottom layers, followed by \"virtual\" branches, where neurons\nfrom a block of regular convolutional and fully-connected layers are\npartitioned into multiple sets. Each virtual branch is trained with different\ndata to specialize in different aspects, e.g., a specific body region or pose\norientation. In this way, robust ensemble representations are obtained against\nhuman body misalignment, deformations, or variations in viewing angles, at\nnearly no any additional cost. The proposed method achieves competitive\nperformance on multiple person re-ID benchmark datasets, including Market-1501,\nCUHK03, and DukeMTMC-reID.", "authors": ["Albert Gong", "Qiang Qiu", "Guillermo Sapiro"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05872v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05872v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05872v1", "published_time": "3/15/2018", "rawpid": "1803.05872", "tags": ["cs.CV"], "title": "Virtual CNN Branching: Efficient Feature Ensemble for Person\n  Re-Identification"}, {"abstract": "Recent advances in blockchain technologies have provided exciting\nopportunities for decentralized applications. Specifically, blockchain-based\nsmart contracts enable credible transactions without authorized third parties.\nThe attractive properties of smart contracts facilitate distributed data\nvending, allowing for proprietary data to be securely exchanged on a\nblockchain. Distributed data vending can transform domains such as healthcare\nby encouraging data distribution from owners and enabling large-scale data\naggregation. However, one key challenge in distributed data vending is the\ntrade-off dilemma between the effectiveness of data retrieval, and the leakage\nrisk from indexing the data. In this paper, we propose a framework for\ndistributed data vending through a combination of data embedding and similarity\nlearning. We illustrate our framework through a practical scenario of\ndistributing and aggregating electronic medical records on a blockchain.\nExtensive empirical results demonstrate the effectiveness of our framework.", "authors": ["Jiayu Zhou", "Fengyi Tang", "He Zhu", "Ning Nan", "Ziheng Zhu"], "category": "cs.CR", "comment": "", "img": "/static/thumbs/1803.05871v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05871v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05871v1", "published_time": "3/15/2018", "rawpid": "1803.05871", "tags": ["cs.CR", "cs.LG"], "title": "Distributed Data Vending on Blockchain"}, {"abstract": "Scientific fields such as insider-threat detection and highway-safety\nplanning often lack sufficient amounts of time-series data to estimate\nstatistical models for the purpose of scientific discovery. Moreover, the\navailable limited data are quite noisy. This presents a major challenge when\nestimating time-series models that are robust to overfitting and have\nwell-calibrated uncertainty estimates. Most of the current literature in these\nfields involve visualizing the time-series for noticeable structure and hard\ncoding them into pre-specified parametric functions. This approach is\nassociated with two limitations. First, given that such trends may not be\neasily noticeable in small data, it is difficult to explicitly incorporate\nexpressive structure into the models during formulation. Second, it is\ndifficult to know $\\textit{a priori}$ the most appropriate functional form to\nuse. To address these limitations, a nonparametric Bayesian approach was\nproposed to implicitly capture hidden structure from time series having limited\ndata. The proposed model, a Gaussian process with a spectral mixture kernel,\nprecludes the need to pre-specify a functional form and hard code trends, is\nrobust to overfitting and has well-calibrated uncertainty estimates.", "authors": ["Daniel Emaasit", "Matthew Johnson"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1803.05867v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05867v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05867v1", "published_time": "3/15/2018", "rawpid": "1803.05867", "tags": ["stat.ML", "cs.LG"], "title": "Capturing Structure Implicitly from Time-Series having Limited Data"}, {"abstract": "For lossy image compression systems, we develop an algorithm called iterative\nrefinement, to improve the decoder\u0027s reconstruction compared with standard\ndecoding techniques. Specifically, we propose a recurrent neural network\napproach for nonlinear, iterative decoding. Our neural decoder, which can work\nwith any encoder, employs self-connected memory units that make use of both\ncausal and non-causal spatial context information to progressively reduce\nreconstruction error over a fixed number of steps. We experiment with\nvariations of our proposed estimator and obtain as much as a 0.8921 decibel\n(dB) gain over the standard JPEG algorithm and a 0.5848 dB gain over a\nstate-of-the-art neural compression model.", "authors": ["Alexander G. Ororbia", "Ankur Mali", "Jian Wu", "Scott O\u0027Connell", "David Miller", "C. Lee Giles"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05863v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05863v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05863v1", "published_time": "3/15/2018", "rawpid": "1803.05863", "tags": ["cs.CV"], "title": "Learned Iterative Decoding for Lossy Image Compression Systems"}, {"abstract": "The iris is considered as the biometric trait with the highest unique\nprobability. The iris location is an important task for biometrics systems,\naffecting directly the results obtained in specific applications such as iris\nrecognition, spoofing and contact lenses detection, among others. This work\ndefines the iris location problem as the delimitation of a smallest squared\nwindow that encompass the iris region. In order to build a benchmark for iris\nlocation we annotate (iris squared bounding boxes) four databases from\ndifferent biometric applications and make them publicly available to the\ncommunity. Besides these 4 annotated databases, we include other 2 from the\nliterature, and we perform experiments on these six databases, five obtained\nwith near infra-red sensors and one with visible light sensor. We compare the\nclassical and outstanding Daugman iris location approach with two window based\ndetectors: 1) a sliding window detector based on features from Histogram of\nGradients (HoG) and a linear Support Vector Machines classifier; 2) a Deep\nLearning based detector fine-tuned from YOLO object detector. Experimental\nresults showed that the Deep Learning based detector outperforms the other ones\nin terms of accuracy and runtime (GPUs version) and should be chosen whenever\npossible.", "authors": ["Evair Severo", "Rayson Laroca", "Cides S. Bezerra", "Luiz A. Zanlorensi", "Daniel Weingaertner", "Gladston Moreira", "David Menotti"], "category": "cs.CV", "comment": "Submitted to International Joint Conference on Neural Networks\n  (IJCNN) 2018", "img": "/static/thumbs/1803.01250v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.01250v2", "num_discussion": 0, "originally_published_time": "3/3/2018", "pid": "1803.01250v2", "published_time": "3/15/2018", "rawpid": "1803.01250", "tags": ["cs.CV"], "title": "A Benchmark for Iris Location and a Deep Learning Detector Evaluation"}, {"abstract": "Self-replication is a key aspect of biological life that has been largely\noverlooked in Artificial Intelligence systems. Here we describe how to build\nand train self-replicating neural networks. The network replicates itself by\nlearning to output its own weights. The network is designed using a loss\nfunction that can be optimized with either gradient-based or non-gradient-based\nmethods. We also describe a method we call regeneration to train the network\nwithout explicit optimization, by injecting the network with predictions of its\nown parameters. The best solution for a self-replicating network was found by\nalternating between regeneration and optimization steps. Finally, we describe a\ndesign for a self-replicating neural network that can solve an auxiliary task\nsuch as MNIST image classification. We observe that there is a trade-off\nbetween the network\u0027s ability to classify images and its ability to replicate,\nbut training is biased towards increasing its specialization at image\nclassification at the expense of replication. This is analogous to the\ntrade-off between reproduction and other tasks observed in nature. We suggest\nthat a self-replication mechanism for artificial intelligence is useful because\nit introduces the possibility of continual improvement through natural\nselection.", "authors": ["Oscar Chang", "Hod Lipson"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1803.05859v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05859v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05859v1", "published_time": "3/15/2018", "rawpid": "1803.05859", "tags": ["cs.AI", "cs.NE"], "title": "Neural Network Quine"}, {"abstract": "In this work, we present a novel and effective framework to facilitate object\ndetection with the instance-level segmentation information that is only\nsupervised by bounding box annotation. Starting from the joint object detection\nand instance segmentation network, we propose to recursively estimate the\npseudo ground-truth object masks from the instance-level object segmentation\nnetwork training, and then enhance the detection network with top-down\nsegmentation feedbacks. The pseudo ground truth mask and network parameters are\noptimized alternatively to mutually benefit for each other. To obtain the\npromising pseudo masks in each iteration, we embed a graphical inference that\nincorporates the low-level image appearance consistency and the bounding box\nannotations to refine the segmentation masks predicted by the segmentation\nnetwork. Our approach progressively improves the object detection performance\nby incorporating the detailed pixel-wise information learned from the\nweakly-supervised segmentation network. Extensive evaluation on the detection\ntask in PASCAL VOC 2007 and 2012 [12] verifies that the proposed approach is\neffective.", "authors": ["Xiangyun Zhao", "Shuang Liang", "Yichen Wei"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05858v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05858v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05858v1", "published_time": "3/15/2018", "rawpid": "1803.05858", "tags": ["cs.CV"], "title": "Pseudo Mask Augmented Object Detection"}, {"abstract": "Convolutional neural networks (CNNs) have become the most successful approach\nin many vision-related domains. However, they are limited to domains where data\nis abundant. Recent works have looked at multi-task learning (MTL) to mitigate\ndata scarcity by leveraging domain-specific information from related tasks. In\nthis paper, we present a novel soft-parameter sharing mechanism for CNNs in a\nMTL setting, which we refer to as Deep Collaboration. We propose taking into\naccount the notion that task relevance depends on depth by using lateral\ntransformation blocs with skip connections. This allows extracting\ntask-specific features at various depth without sacrificing features relevant\nto all tasks. We show that CNNs connected with our Deep Collaboration obtain\nbetter accuracy on facial landmark detection with related tasks. We finally\nverify that our approach effectively allows knowledge sharing by showing\ndepth-specific influence of tasks that we know are related.", "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "category": "cs.CV", "comment": "Under review at the 15th European Conference on Computer Vision\n  (ECCV) (2018)", "img": "/static/thumbs/1711.00111v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.00111v2", "num_discussion": 0, "originally_published_time": "10/28/2017", "pid": "1711.00111v2", "published_time": "3/15/2018", "rawpid": "1711.00111", "tags": ["cs.CV"], "title": "Multi-Task Learning by Deep Collaboration and Application in Facial\n  Landmark Detection"}, {"abstract": "Classical deep convolutional networks increase receptive field size by either\ngradual resolution reduction or application of hand-crafted dilated\nconvolutions to prevent increase in the number of parameters. In this paper we\npropose a novel displaced aggregation unit (DAU) that does not require\nhand-crafting. In contrast to classical filters with units (pixels) placed on a\nfixed regular grid, the displacement of the DAUs are learned, which enables\nfilters to spatially-adapt their receptive field to a given problem. We\nextensively demonstrate the strength of DAUs on a classification and semantic\nsegmentation tasks. Compared to ConvNets with regular filter, ConvNets with\nDAUs achieve comparable performance at faster convergence and up to 3-times\nreduction in parameters. Furthermore, DAUs allow us to study deep networks from\nnovel perspectives. We study spatial distributions of DAU filters and analyze\nthe number of parameters allocated for spatial coverage in a filter.", "authors": ["Domen Tabernik", "Matej Kristan", "Ale\u0161 Leonardis"], "category": "cs.CV", "comment": "Accepted to Computer Vision and Pattern Recognition 2018", "img": "/static/thumbs/1711.11473v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.11473v2", "num_discussion": 0, "originally_published_time": "11/30/2017", "pid": "1711.11473v2", "published_time": "3/15/2018", "rawpid": "1711.11473", "tags": ["cs.CV"], "title": "Spatially-Adaptive Filter Units for Deep Neural Networks"}, {"abstract": "We introduce a new lexical resource that enriches the Framester knowledge\ngraph, which links Framnet, WordNet, VerbNet and other resources, with semantic\nfeatures from text corpora. These features are extracted from distributionally\ninduced sense inventories and subsequently linked to the manually-constructed\nframe representations to boost the performance of frame disambiguation in\ncontext. Since Framester is a frame-based knowledge graph, which enables\nfull-fledged OWL querying and reasoning, our resource paves the way for the\ndevelopment of novel, deeper semantic-aware applications that could benefit\nfrom the combination of knowledge from text and complex symbolic\nrepresentations of events and participants. Together with the resource we also\nprovide the software we developed for the evaluation in the task of Word Frame\nDisambiguation (WFD).", "authors": ["Stefano Faralli", "Alexander Panchenko", "Chris Biemann", "Simone Paolo Ponzetto"], "category": "cs.CL", "comment": "In Proceedings of the 11th Conference on Language Resources and\n  Evaluation (LREC 2018). Miyazaki, ...", "img": "/static/thumbs/1803.05829v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05829v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05829v1", "published_time": "3/15/2018", "rawpid": "1803.05829", "tags": ["cs.CL"], "title": "Enriching Frame Representations with Distributionally Induced Senses"}, {"abstract": "Feature learning on point clouds has shown great promise, with the\nintroduction of effective and generalizable deep learning frameworks such as\npointnet++. Thus far, however, point features have been abstracted in an\nindependent and isolated manner, ignoring the relative layout of neighboring\npoints as well as their features. In the present article, we propose to\novercome this limitation by using spectral graph convolution on a local graph,\ncombined with a novel graph pooling strategy. In our approach, graph\nconvolution is carried out on a nearest neighbor graph constructed from a\npoint\u0027s neighborhood, such that features are jointly learned. We replace the\nstandard max pooling step with a recursive clustering and pooling strategy,\ndevised to aggregate information from within clusters of nodes that are close\nto one another in their spectral coordinates, leading to richer overall feature\ndescriptors. Through extensive experiments on diverse datasets, we show a\nconsistent demonstrable advantage for the tasks of both point set\nclassification and segmentation.", "authors": ["Chu Wang", "Babak Samari", "Kaleem Siddiqi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05827v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05827v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05827v1", "published_time": "3/15/2018", "rawpid": "1803.05827", "tags": ["cs.CV", "cs.LG"], "title": "Local Spectral Graph Convolution for Point Set Feature Learning"}, {"abstract": "The paper gives an overview of the Russian Semantic Similarity Evaluation\n(RUSSE) shared task held in conjunction with the Dialogue 2015 conference.\nThere exist a lot of comparative studies on semantic similarity, yet no\nanalysis of such measures was ever performed for the Russian language.\nExploring this problem for the Russian language is even more interesting,\nbecause this language has features, such as rich morphology and free word\norder, which make it significantly different from English, German, and other\nwell-studied languages. We attempt to bridge this gap by proposing a shared\ntask on the semantic similarity of Russian nouns. Our key contribution is an\nevaluation methodology based on four novel benchmark datasets for the Russian\nlanguage. Our analysis of the 105 submissions from 19 teams reveals that\nsuccessful approaches for English, such as distributional and skip-gram models,\nare directly applicable to Russian as well. On the one hand, the best results\nin the contest were obtained by sophisticated supervised models that combine\nevidence from different sources. On the other hand, completely unsupervised\napproaches, such as a skip-gram model estimated on a large-scale corpus, were\nable score among the top 5 systems.", "authors": ["Alexander Panchenko", "Natalia Loukachevitch", "Dmitry Ustalov", "Denis Paperno", "Christian Meyer", "Natalia Konstantinova"], "category": "cs.CL", "comment": "In Proceedings of the 21st International Conference on Computational\n  Linguistics and Intellectual ...", "img": "/static/thumbs/1803.05820v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05820v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05820v1", "published_time": "3/15/2018", "rawpid": "1803.05820", "tags": ["cs.CL"], "title": "RUSSE: The First Workshop on Russian Semantic Similarity"}, {"abstract": "Examining and interpreting of a large number of wireless endoscopic images\nfrom the gastrointestinal tract is a tiresome task for physicians. A practical\nsolution is to automatically construct a two dimensional representation of the\ngastrointestinal tract for easy inspection. However, little has been done on\nwireless endoscopic image stitching, let alone systematic investigation. The\nproposed new wireless endoscopic image stitching method consists of two main\nsteps to improve the accuracy and efficiency of image registration. First, the\nkeypoints are extracted by Principle Component Analysis and Scale Invariant\nFeature Transform (PCA-SIFT) algorithm and refined with Maximum Likelihood\nEstimation SAmple Consensus (MLESAC) outlier removal to find the most reliable\nkeypoints. Second, the optimal transformation parameters obtained from first\nstep are fed to the Normalised Mutual Information (NMI) algorithm as an initial\nsolution. With modified Marquardt-Levenberg search strategy in a multiscale\nframework, the NMI can find the optimal transformation parameters in the\nshortest time. The proposed methodology has been tested on two different\ndatasets - one with real wireless endoscopic images and another with images\nobtained from Micro-Ball (a new wireless cubic endoscopy system with six image\nsensors). The results have demonstrated the accuracy and robustness of the\nproposed methodology both visually and quantitatively.", "authors": ["Rahman Attar", "Xiang Xie", "Zhihua Wang", "Shigang Yue"], "category": "eess.SP", "comment": "Journal draft", "img": "/static/thumbs/1803.05817v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05817v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05817v1", "published_time": "3/15/2018", "rawpid": "1803.05817", "tags": ["eess.SP", "cs.CV", "eess.IV"], "title": "2D Reconstruction of Small Intestine\u0027s Interior Wall"}, {"abstract": "Compressed Sensing MRI (CS-MRI) has provided theoretical foundations upon\nwhich the time-consuming MRI acquisition process can be accelerated. However,\nit primarily relies on iterative numerical solvers which still hinders their\nadaptation in time-critical applications. In addition, recent advances in deep\nneural networks have shown their potential in computer vision and image\nprocessing, but their adaptation to MRI reconstruction is still in an early\nstage. In this paper, we propose a novel deep learning-based generative\nadversarial model, RefineGAN, for fast and accurate CS-MRI reconstruction. The\nproposed model is a variant of fully-residual convolutional autoencoder and\ngenerative adversarial networks (GANs), specifically designed for CS-MRI\nformulation; it employs deeper generator and discriminator networks with cyclic\ndata consistency loss for faithful interpolation in the given under-sampled\nk-space data. In addition, our solution leverages a chained network to further\nenhance the reconstruction quality. RefineGAN is fast and accurate -- the\nreconstruction process is extremely rapid, as low as tens of milliseconds for\nreconstruction of a 256x256 image, because it is one-way deployment on a\nfeed-forward network, and the image quality is superior even for extremely low\nsampling rate (as low as 10%) due to the data-driven nature of the method. We\ndemonstrate that RefineGAN outperforms the state-of-the-art CS-MRI methods by a\nlarge margin in terms of both running time and image quality via evaluation\nusing several open-source MRI databases.", "authors": ["Tran Minh Quan", "Thanh Nguyen-Duc", "Won-Ki Jeong"], "category": "cs.CV", "comment": "submitted to IEEE Transactions on Medical Imaging", "img": "/static/thumbs/1709.00753v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.00753v2", "num_discussion": 0, "originally_published_time": "9/3/2017", "pid": "1709.00753v2", "published_time": "3/15/2018", "rawpid": "1709.00753", "tags": ["cs.CV"], "title": "Compressed Sensing MRI Reconstruction using a Generative Adversarial\n  Network with a Cyclic Loss"}, {"abstract": "We present data-dependent learning bounds for the general scenario of\nnon-stationary non-mixing stochastic processes. Our learning guarantees are\nexpressed in terms of a data-dependent measure of sequential complexity and a\ndiscrepancy measure that can be estimated from data under some mild\nassumptions. We also also provide novel analysis of stable time series\nforecasting algorithm using this new notion of discrepancy that we introduce.\nWe use our learning bounds to devise new algorithms for non-stationary time\nseries forecasting for which we report some preliminary experimental results.", "authors": ["Vitaly Kuznetsov", "Mehryar Mohri"], "category": "cs.LG", "comment": "An extended abstract has appeared in (Kuznetsov and Mohri, 2015)", "img": "/static/thumbs/1803.05814v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05814v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05814v1", "published_time": "3/15/2018", "rawpid": "1803.05814", "tags": ["cs.LG"], "title": "Theory and Algorithms for Forecasting Time Series"}, {"abstract": "Recently advancements in deep learning allowed the development of end-to-end\ntrained goal-oriented dialog systems. Although these systems already achieve\ngood performance, some simplifications limit their usage in real-life\nscenarios.\n  In this work, we address two of these limitations: ignoring positional\ninformation and a fixed number of possible response candidates. We propose to\nuse positional encodings in the input to model the word order of the user\nutterances. Furthermore, by using a feedforward neural network, we are able to\ngenerate the output word by word and are no longer restricted to a fixed number\nof possible response candidates. Using the positional encoding, we were able to\nachieve better accuracies in the Dialog bAbI Tasks and using the feedforward\nneural network for generating the response, we were able to save computation\ntime and space consumption.", "authors": ["Stefan Constantin", "Jan Niehues", "Alex Waibel"], "category": "cs.CL", "comment": "11 pages, 4 figures, forthcoming in IWSDS 2018; added quantitative\n  analysis of sensitivity to modi...", "img": "/static/thumbs/1803.02279v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.02279v2", "num_discussion": 0, "originally_published_time": "3/6/2018", "pid": "1803.02279v2", "published_time": "3/15/2018", "rawpid": "1803.02279", "tags": ["cs.CL"], "title": "An End-to-End Goal-Oriented Dialog System with a Generative Natural\n  Language Response Generation"}, {"abstract": "In order to enhance the real-time performance of convolutional neural\nnetworks(CNNs), more and more researchers are focusing on improving the\nefficiency of CNN. Based on the analysis of some CNN architectures, such as\nResNet, DenseNet, ShuffleNet and so on, we combined their advantages and\nproposed a very efficient model called Highly Efficient Networks(HENet). The\nnew architecture uses an unusual way to combine group convolution and channel\nshuffle which was mentioned in ShuffleNet. Inspired by ResNet and DenseNet, we\nalso proposed a new way to use element-wise addition and concatenation\nconnection with each block. In order to make greater use of feature maps,\npooling operations are removed from HENet. The experiments show that our\nmodel\u0027s efficiency is more than 1 times higher than ShuffleNet on many open\nsource datasets, such as CIFAR-10/100 and SVHN.", "authors": ["Qiuyu Zhu", "Ruixin Zhang"], "category": "cs.CV", "comment": "11 pages,3 figures", "img": "/static/thumbs/1803.02742v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.02742v2", "num_discussion": 0, "originally_published_time": "3/7/2018", "pid": "1803.02742v2", "published_time": "3/15/2018", "rawpid": "1803.02742", "tags": ["cs.CV"], "title": "HENet:A Highly Efficient Convolutional Neural Networks Optimized for\n  Accuracy, Speed and Storage"}, {"abstract": "Object ranking is an important problem in the realm of preference learning.\nOn the basis of training data in the form of a set of rankings of objects,\nwhich are typically represented as feature vectors, the goal is to learn a\nranking function that predicts a linear order of any new set of objects.\nCurrent approaches commonly focus on ranking by scoring, i.e., on learning an\nunderlying latent utility function that seeks to capture the inherent utility\nof each object. These approaches, however, are not able to take possible\neffects of context-dependence into account, where context-dependence means that\nthe utility or usefulness of an object may also depend on what other objects\nare available as alternatives. In this paper, we formalize the problem of\ncontext-dependent ranking and present two general approaches based on two\nnatural representations of context-dependent ranking functions. Both approaches\nare instantiated by means of appropriate neural network architectures. We\ndemonstrate empirically that our methods outperform traditional approaches on\nbenchmark tasks, for which context-dependence is playing a relevant role.", "authors": ["Karlson Pfannschmidt", "Pritha Gupta", "Eyke H\u00fcllermeier"], "category": "stat.ML", "comment": "Under review for SIGKDD 2018", "img": "/static/thumbs/1803.05796v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05796v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05796v1", "published_time": "3/15/2018", "rawpid": "1803.05796", "tags": ["stat.ML", "cs.IR", "cs.LG", "cs.NE"], "title": "Deep architectures for learning context-dependent ranking functions"}, {"abstract": "The paper describes the results of the first shared task on word sense\ninduction (WSI) for the Russian language. While similar shared tasks were\nconducted in the past for some Romance and Germanic languages, we explore the\nperformance of sense induction and disambiguation methods for a Slavic language\nthat shares many features with other Slavic languages, such as rich morphology\nand free word order. The participants were asked to group contexts with a given\nword in accordance with its senses that were not provided beforehand. For\ninstance, given a word \"bank\" and a set of contexts with this word, e.g. \"bank\nis a financial institution that accepts deposits\" and \"river bank is a slope\nbeside a body of water\", a participant was asked to cluster such contexts in\nthe unknown in advance number of clusters corresponding to, in this case, the\n\"company\" and the \"area\" senses of the word \"bank\". For the purpose of this\nevaluation campaign, we developed three new evaluation datasets based on sense\ninventories that have different sense granularity. The contexts in these\ndatasets were sampled from texts of Wikipedia, the academic corpus of Russian,\nand an explanatory dictionary of Russian. Overall 18 teams participated in the\ncompetition submitting 383 models. Multiple teams managed to substantially\noutperform competitive state-of-the-art baselines from the previous years based\non sense embeddings.", "authors": ["Alexander Panchenko", "Anastasiya Lopukhina", "Dmitry Ustalov", "Konstantin Lopukhin", "Nikolay Arefyev", "Alexey Leontyev", "Natalia Loukachevitch"], "category": "cs.CL", "comment": "To appear in the proceedings of the 24rd International Conference on\n  Computational Linguistics and...", "img": "/static/thumbs/1803.05795v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05795v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05795v1", "published_time": "3/15/2018", "rawpid": "1803.05795", "tags": ["cs.CL"], "title": "RUSSE\u00272018: A Shared Task on Word Sense Induction for the Russian\n  Language"}, {"abstract": "We generalize the Langevin Dynamics through the mirror descent framework for\nfirst-order sampling. The na\\\"ive approach of incorporating Brownian motion\ninto the mirror descent dynamics, which we refer to as Symmetric Mirrored\nLangevin Dynamics (S-MLD), is shown to connected to the theory of Weighted\nHessian Manifolds. The S-MLD, unfortunately, contains the hard instance of\nCox--Ingersoll--Ross processes, whose discrete-time approximation exhibits slow\nconvergence both theoretically and empirically. We then propose a new dynamics,\nwhich we refer to as the Asymmetric Mirrored Langevin Dynamics (A-MLD), that\navoids the hurdles of S-MLD. In particular, we prove that discretized A-MLD\nimplies the existence of a first-order sampling algorithm that sharpens the\nstate-of-the-art $\\tilde{O}(\\epsilon^{-6}d^5)$ rate to\n$\\tilde{O}(\\epsilon^{-2}d)$, when the target distribution is strongly\nlog-concave with compact support. For sampling on a simplex, A-MLD can\ntransform certain non-log-concave sampling problems into log-concave ones. As a\nconcrete example, we derive the first non-asymptotic\n$\\tilde{O}(\\epsilon^{-4}d^2)$ rate for first-order sampling of Dirichlet\nposteriors.", "authors": ["Ya-Ping Hsieh", "Volkan Cevher"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1802.10174v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.10174v2", "num_discussion": 0, "originally_published_time": "2/27/2018", "pid": "1802.10174v2", "published_time": "3/15/2018", "rawpid": "1802.10174", "tags": ["cs.LG", "math.OC"], "title": "Mirrored Langevin Dynamics"}, {"abstract": "We present an effective dynamic clustering algorithm for the task of temporal\nhuman action segmentation, which has comprehensive applications such as\nrobotics, motion analysis, and patient monitoring. Our proposed algorithm is\nunsupervised, fast, generic to process various types of features, and\napplicable in both the online and offline settings. We perform extensive\nexperiments of processing data streams, and show that our algorithm achieves\nthe state-of-the-art results for both online and offline settings.", "authors": ["Yan Zhang", "He Sun", "Siyu Tang", "Heiko Neumann"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05790v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05790v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05790v1", "published_time": "3/15/2018", "rawpid": "1803.05790", "tags": ["cs.CV"], "title": "Temporal Human Action Segmentation via Dynamic Clustering"}, {"abstract": "In this paper, we apply the attention mechanism to autonomous driving for\nsteering angle prediction. We propose the first model, applying the recently\nintroduced sparse attention mechanism to visual domain, as well as the\naggregated extension for this model. We show the improvement of the proposed\nmethod, comparing to no attention as well as to different types of attention.", "authors": ["Sen He", "Dmitry Kangin", "Yang Mi", "Nicolas Pugeault"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05785v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05785v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05785v1", "published_time": "3/15/2018", "rawpid": "1803.05785", "tags": ["cs.CV"], "title": "Aggregated Sparse Attention for Steering Angle Prediction"}, {"abstract": "Introduced by Breiman (2001), Random Forests are widely used as\nclassification and regression algorithms. While being initially designed as\nbatch algorithms, several variants have been proposed to handle online\nlearning. One particular instance of such forests is the Mondrian Forest, whose\ntrees are built using the so-called Mondrian process, therefore allowing to\neasily update their construction in a streaming fashion. In this paper, we\nstudy Mondrian Forests in a batch setting and prove their consistency assuming\na proper tuning of the lifetime sequence. A thorough theoretical study of\nMondrian partitions allows us to derive an upper bound for the risk of Mondrian\nForests, which turns out to be the minimax optimal rate for both Lipschitz and\ntwice differentiable regression functions. These results are actually the first\nto state that some particular random forests achieve minimax rates \\textit{in\narbitrary dimension}, paving the way to a refined theoretical analysis and thus\na deeper understanding of these black box algorithms.", "authors": ["Jaouad Mourtada", "St\u00e9phane Ga\u00efffas", "Erwan Scornet"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1803.05784v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05784v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05784v1", "published_time": "3/15/2018", "rawpid": "1803.05784", "tags": ["stat.ML"], "title": "Minimax optimal rates for Mondrian trees and forests"}, {"abstract": "This paper presents SO-Net, a permutation invariant architecture for deep\nlearning with orderless point clouds. The SO-Net models the spatial\ndistribution of point cloud by building a Self-Organizing Map (SOM). Based on\nthe SOM, SO-Net performs hierarchical feature extraction on individual points\nand SOM nodes, and ultimately represents the input point cloud by a single\nfeature vector. The receptive field of the network can be systematically\nadjusted by conducting point-to-node k nearest neighbor search. In recognition\ntasks such as point cloud reconstruction, classification, object part\nsegmentation and shape retrieval, our proposed network demonstrates performance\nthat is similar with or better than state-of-the-art approaches. In addition,\nthe training speed is significantly faster than existing point cloud\nrecognition networks because of the parallelizability and simplicity of the\nproposed architecture. Our code is available at the project website.\nhttps://github.com/lijx10/SO-Net", "authors": ["Jiaxin Li", "Ben M. Chen", "Gim Hee Lee"], "category": "cs.CV", "comment": "17 pages, CVPR 2018", "img": "/static/thumbs/1803.04249v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04249v3", "num_discussion": 0, "originally_published_time": "3/12/2018", "pid": "1803.04249v3", "published_time": "3/15/2018", "rawpid": "1803.04249", "tags": ["cs.CV"], "title": "SO-Net: Self-Organizing Network for Point Cloud Analysis"}, {"abstract": "We propose Gaussian processes for signals over graphs (GPG) using the apriori\nknowledge that the target vectors lie over a graph. We incorporate this\ninformation using a graph- Laplacian based regularization which enforces the\ntarget vectors to have a specific profile in terms of graph Fourier transform\ncoeffcients, for example lowpass or bandpass graph signals. We discuss how the\nregularization affects the mean and the variance in the prediction output. In\nparticular, we prove that the predictive variance of the GPG is strictly\nsmaller than the conventional Gaussian process (GP) for any non-trivial graph.\nWe validate our concepts by application to various real-world graph signals.\nOur experiments show that the performance of the GPG is superior to GP for\nsmall training data sizes and under noisy training.", "authors": ["Arun Venkitaraman", "Saikat Chatterjee", "Peter Ha\u00e4ndel"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1803.05776v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05776v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05776v1", "published_time": "3/15/2018", "rawpid": "1803.05776", "tags": ["stat.ML", "cs.LG", "eess.SP"], "title": "Gaussian Processes Over Graphs"}, {"abstract": "We consider the problem of predicting plausible missing facts in relational\ndata, given a set of imperfect logical rules. In particular, our aim is to\nprovide bounds on the (expected) number of incorrect inferences that are made\nin this way. Since for classical inference it is in general impossible to bound\nthis number in a non-trivial way, we consider two inference relations that\nweaken, but remain close in spirit to classical inference.", "authors": ["Ondrej Kuzelka", "Yuyi Wang", "Jesse Davis", "Steven Schockaert"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1803.05768v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05768v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05768v1", "published_time": "3/15/2018", "rawpid": "1803.05768", "tags": ["cs.AI", "cs.LG"], "title": "PAC-Reasoning in Relational Domains"}, {"abstract": "Tick is a statistical learning library for Python~3, with a particular\nemphasis on time-dependent models, such as point processes, and tools for\ngeneralized linear models and survival analysis. The core of the library is an\noptimization module providing model computational classes, solvers and proximal\noperators for regularization. tick relies on a C++ implementation and\nstate-of-the-art optimization algorithms to provide very fast computations in a\nsingle node multi-core setting. Source code and documentation can be downloaded\nfrom https://github.com/X-DataInitiative/tick", "authors": ["Emmanuel Bacry", "Martin Bompaire", "St\u00e9phane Ga\u00efffas", "Soren Poulsen"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1707.03003v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.03003v2", "num_discussion": 0, "originally_published_time": "7/10/2017", "pid": "1707.03003v2", "published_time": "3/15/2018", "rawpid": "1707.03003", "tags": ["stat.ML"], "title": "Tick: a Python library for statistical learning, with a particular\n  emphasis on time-dependent modelling"}, {"abstract": "Saliency prediction is a well studied problem in computer vision. Early\nsaliency models were based on low-level hand-crafted feature derived from\ninsights gained in neuroscience and psychophysics. In the wake of deep learning\nbreakthrough, a new cohort of models were proposed based on neural network\narchitectures, allowing significantly higher gaze prediction than previous\nshallow models, on all metrics.\n  However, most models treat the saliency prediction as a \\textit{regression}\nproblem, and accurate regression of high-dimensional data is known to be a hard\nproblem. Furthermore, it is unclear that intermediate levels of saliency (ie,\nneither very high, nor very low) are meaningful: Something is either salient,\nor it is not.\n  Drawing from those two observations, we reformulate the saliency prediction\nproblem as a salient region \\textit{segmentation} problem. We demonstrate that\nthe reformulation allows for faster convergence than the classical regression\nproblem, while performance is comparable to state-of-the-art.\n  We also visualise the general features learned by the model, which are showed\nto be consistent with insights from psychophysics.", "authors": ["Sen He", "Nicolas Pugeault"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05759v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05759v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05759v1", "published_time": "3/15/2018", "rawpid": "1803.05759", "tags": ["cs.CV"], "title": "Salient Region Segmentation"}, {"abstract": "Deep convolutional neural networks have demonstrated high performances for\nfixation prediction in recent years. How they achieve this, however, is less\nexplored and they remain to be black box models. Here, we attempt to shed light\non the internal structure of deep saliency models and study what features they\nextract for fixation prediction. Specifically, we use a simple yet powerful\narchitecture, consisting of only one CNN and a single resolution input,\ncombined with a new loss function for pixel-wise fixation prediction during\nfree viewing of natural scenes. We show that our simple method is on par or\nbetter than state-of-the-art complicated saliency models. Furthermore, we\npropose a method, related to saliency model evaluation metrics, to visualize\ndeep models for fixation prediction. Our method reveals the inner\nrepresentations of deep models for fixation prediction and provides evidence\nthat saliency, as experienced by humans, is likely to involve high-level\nsemantic knowledge in addition to low-level perceptual cues. Our results can be\nuseful to measure the gap between current saliency models and the human\ninter-observer model and to build new models to close this gap.", "authors": ["Sen He", "Nicolas Pugeault", "Yang Mi", "Ali Borji"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05753v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05753v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05753v1", "published_time": "3/15/2018", "rawpid": "1803.05753", "tags": ["cs.CV"], "title": "What Catches the Eye? Visualizing and Understanding Deep Saliency Models"}, {"abstract": "Rearranging objects on a tabletop surface by means of nonprehensile\nmanipulation is a task which requires skillful interaction with the physical\nworld. Usually, this is achieved by precisely modeling physical properties of\nthe objects, robot, and the environment for explicit planning. In contrast, as\nexplicitly modeling the physical environment is not always feasible and\ninvolves various uncertainties, we learn a nonprehensile rearrangement strategy\nwith deep reinforcement learning based on only visual feedback. For this, we\nmodel the task with rewards and train a deep Q-network. Our potential\nfield-based heuristic exploration strategy reduces the amount of collisions\nwhich lead to suboptimal outcomes and we actively balance the training set to\navoid bias towards poor examples. Our training process leads to quicker\nlearning and better performance on the task as compared to uniform exploration\nand standard experience replay. We demonstrate empirical evidence from\nsimulation that our method leads to a success rate of 85%, show that our system\ncan cope with sudden changes of the environment, and compare our performance\nwith human level performance.", "authors": ["Weihao Yuan", "Johannes A. Stork", "Danica Kragic", "Michael Y. Wang", "Kaiyu Hang"], "category": "cs.RO", "comment": "2018 International Conference on Robotics and Automation", "img": "/static/thumbs/1803.05752v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05752v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05752v1", "published_time": "3/15/2018", "rawpid": "1803.05752", "tags": ["cs.RO", "cs.AI", "cs.LG"], "title": "Rearrangement with Nonprehensile Manipulation Using Deep Reinforcement\n  Learning"}, {"abstract": "Accurate Traffic Sign Detection (TSD) can help drivers make better decision\naccording to the traffic regulations. TSD, regarded as a typical small object\ndetection problem in some way, is fundamental in the field of self-driving and\nadvanced driver assistance systems. However, small object detection is still an\nopen question. In this paper, we proposed a human brain inspired network to\nhandle this problem. Attention mechanism is an essential function of our brain,\nwe used a novel recurrent attentive neural network to improve the detection\naccuracy in a fine-grained manner. Further, as we human can combine domain\nspecific knowledge and intuitive knowledge to solve tricky tasks, we proposed\nan assumption that the location of the traffic signs obeys the reverse gaussian\ndistribution, which means the location is around the central bias of every\npicture. Experimental result shows that our methods achieved better performance\nthan several popular methods used in object detection.", "authors": ["Kai Yi", "Zhiqiang Jian", "Shitao Chen", "Yu Chen", "Nanning Zheng"], "category": "cs.CV", "comment": "7 pages, 6 figures", "img": "/static/thumbs/1803.05263v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05263v2", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.05263v2", "published_time": "3/15/2018", "rawpid": "1803.05263", "tags": ["cs.CV", "cs.AI"], "title": "Knowledge-based Recurrent Attentive Neural Network for Traffic Sign\n  Detection"}, {"abstract": "Many computer vision pipelines involve dynamic programming primitives such as\nfinding a shortest path or the minimum energy solution in a tree-shaped\nprobabilistic graphical model. In such cases, extracting not merely the best,\nbut the set of M-best solutions is useful to generate a rich collection of\ncandidate proposals that can be used in downstream processing. In this work, we\nshow how M-best solutions of tree-shaped graphical models can be obtained by\ndynamic programming on a special graph with M layers. The proposed multi-layer\nconcept is optimal for searching M-best solutions, and so flexible that it can\nalso approximate M-best diverse solutions. We illustrate the usefulness with\napplications to object detection, panorama stitching and centerline extraction.\n  Note: We have observed that an assumption in section 4 of our paper is not\nalways fulfilled, see the attached corrigendum for details.", "authors": ["Carsten Haubold", "Virginie Uhlmann", "Michael Unser", "Fred A. Hamprecht"], "category": "cs.CV", "comment": "Includes supplementary and corrigendum", "img": "/static/thumbs/1803.05748v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05748v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05748v1", "published_time": "3/15/2018", "rawpid": "1803.05748", "tags": ["cs.CV"], "title": "Diverse M-Best Solutions by Dynamic Programming"}, {"abstract": "Recently, there has been a considerable attention given to the motion\ndetection problem due to the explosive growth of its applications in video\nanalysis and surveillance systems. While the previous approaches can produce\ngood results, an accurate detection of motion remains a challenging task due to\nthe difficulties raised by illumination variations, occlusion, camouflage,\nburst physical motion, dynamic texture, and environmental changes such as those\non climate changes, sunlight changes during a day, etc. In this paper, we\npropose a novel per-pixel motion descriptor for both motion detection and\ndynamic texture segmentation which outperforms the current methods in the\nliterature particularly in severe scenarios. The proposed descriptor is based\non two complementary three-dimensional-discrete wavelet transform (3D-DWT) and\nthree-dimensional wavelet leader. In this approach, a feature vector is\nextracted for each pixel by applying a novel three dimensional wavelet-based\nmotion descriptor. Then, the extracted features are clustered by a clustering\nmethod such as well-known k-means algorithm or Gaussian Mixture Model (GMM).\nThe experimental results demonstrate the effectiveness of our proposed method\ncompared to the other motion detection approaches from the literature. The\napplication of the proposed method and additional experimental results for the\ndifferent datasets are available at\n(http://dspl.ce.sharif.edu/motiondetector.html).", "authors": ["Sahar Yousefi", "M. T. Manzuri Shalmani", "Jeremy Lin", "Marius Staring"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.03382v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.03382v6", "num_discussion": 0, "originally_published_time": "12/11/2016", "pid": "1612.03382v6", "published_time": "3/15/2018", "rawpid": "1612.03382", "tags": ["cs.CV"], "title": "A Novel Motion Detection Method Resistant to Severe Illumination Changes"}, {"abstract": "While the research on convolutional neural networks (CNNs) is progressing\nquickly, the real-world deployment of these models is often limited by\ncomputing resources and memory constraints. In this paper, we address this\nissue by proposing a novel filter pruning method to compress and accelerate\nCNNs. Our work is based on the linear relationship identified in different\nfeature map subspaces via visualization of feature maps. Such linear\nrelationship implies that the information in CNNs is redundant. Our method\neliminates the redundancy in convolutional filters by applying subspace\nclustering to feature maps. In this way, most of the representative information\nin the network can be retained in each cluster. Therefore, our method provides\nan effective solution to filter pruning for which most existing methods\ndirectly remove filters based on simple heuristics. The proposed method is\nindependent of the network structure, thus it can be adopted by any\noff-the-shelf deep learning libraries. Experiments on different networks and\ntasks show that our method outperforms existing techniques before fine-tuning,\nand achieves the state-of-the-art results after fine-tuning.", "authors": ["Dong Wang", "Lei Zhou", "Xueni Zhang", "Xiao Bai", "Jun Zhou"], "category": "cs.CV", "comment": "17 pages, 4 figures", "img": "/static/thumbs/1803.05729v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05729v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05729v1", "published_time": "3/15/2018", "rawpid": "1803.05729", "tags": ["cs.CV"], "title": "Exploring Linear Relationship in Feature Map Subspace for ConvNets\n  Compression"}, {"abstract": "This paper introduces a new nonlinear dictionary learning method for\nhistograms in the probability simplex. The method leverages optimal transport\ntheory, in the sense that our aim is to reconstruct histograms using so-called\ndisplacement interpolations (a.k.a. Wasserstein barycenters) between dictionary\natoms; such atoms are themselves synthetic histograms in the probability\nsimplex. Our method simultaneously estimates such atoms, and, for each\ndatapoint, the vector of weights that can optimally reconstruct it as an\noptimal transport barycenter of such atoms. Our method is computationally\ntractable thanks to the addition of an entropic regularization to the usual\noptimal transportation problem, leading to an approximation scheme that is\nefficient, parallel and simple to differentiate. Both atoms and weights are\nlearned using a gradient-based descent method. Gradients are obtained by\nautomatic differentiation of the generalized Sinkhorn iterations that yield\nbarycenters with entropic smoothing. Because of its formulation relying on\nWasserstein barycenters instead of the usual matrix product between dictionary\nand codes, our method allows for nonlinear relationships between atoms and the\nreconstruction of input data. We illustrate its application in several\ndifferent image processing settings.", "authors": ["Morgan A. Schmitz", "Matthieu Heitz", "Nicolas Bonneel", "Fred Maurice Ngol\u00e8 Mboula", "David Coeurjolly", "Marco Cuturi", "Gabriel Peyr\u00e9", "Jean-Luc Starck"], "category": "stat.ML", "comment": "Published in SIAM SIIMS. 46 pages, 24 figures", "img": "/static/thumbs/1708.01955v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1708.01955v3", "num_discussion": 0, "originally_published_time": "8/7/2017", "pid": "1708.01955v3", "published_time": "3/15/2018", "rawpid": "1708.01955", "tags": ["stat.ML", "cs.GR", "math.OC"], "title": "Wasserstein Dictionary Learning: Optimal Transport-based unsupervised\n  non-linear dictionary learning"}, {"abstract": "Machine learning algorithms use error function minimization to fit a large\nset of parameters in a preexisting model. However, error minimization\neventually leads to a memorization of the training dataset, losing the ability\nto generalize to other datasets. To achieve generalization something else is\nneeded, for example a regularization method or stopping the training when error\nin a validation dataset is minimal. Here we propose a different approach to\nlearning and generalization that is parameter-free, fully discrete and that\ndoes not use function minimization. We use the training data to find an\nalgebraic representation with minimal size and maximal freedom, explicitly\nexpressed as a product of irreducible components. This algebraic representation\nis shown to directly generalize, giving high accuracy in test data, more so the\nsmaller the representation. We prove that the number of generalizing\nrepresentations can be very large and the algebra only needs to find one. We\nalso derive and test a relationship between compression and error rate. We give\nresults for a simple problem solved step by step, hand-written character\nrecognition, and the Queens Completion problem as an example of unsupervised\nlearning. As an alternative to statistical learning, algebraic learning may\noffer advantages in combining bottom-up and top-down information, formal\nconcept derivation from data and large-scale parallelization.", "authors": ["Fernando Martin-Maroto", "Gonzalo G. de Polavieja"], "category": "cs.LG", "comment": "In v2 Figures 10 and 12 are images (v1 used latex commands), so all\n  queens on board are now visibl...", "img": "/static/thumbs/1803.05252v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05252v2", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05252v2", "published_time": "3/15/2018", "rawpid": "1803.05252", "tags": ["cs.LG", "cs.DM", "math.AC", "math.RA"], "title": "Algebraic Machine Learning"}, {"abstract": "Discourse-annotated corpora are an important resource for the community, but\nthey are often annotated according to different frameworks. This makes\ncomparison of the annotations difficult, thereby also preventing researchers\nfrom searching the corpora in a unified way, or using all annotated data\njointly to train computational systems. Several theoretical proposals have\nrecently been made for mapping the relational labels of different frameworks to\neach other, but these proposals have so far not been validated against existing\nannotations. The two largest discourse relation annotated resources, the Penn\nDiscourse Treebank and the Rhetorical Structure Theory Discourse Treebank, have\nhowever been annotated on the same text, allowing for a direct comparison of\nthe annotation layers. We propose a method for automatically aligning the\ndiscourse segments, and then evaluate existing mapping proposals by comparing\nthe empirically observed against the proposed mappings. Our analysis highlights\nthe influence of segmentation on subsequent discourse relation labeling, and\nshows that while agreement between frameworks is reasonable for explicit\nrelations, agreement on implicit relations is low. We identify several sources\nof systematic discrepancies between the two annotation schemes and discuss\nconsequences of these discrepancies for future annotation and for the training\nof automatic discourse relation labellers.", "authors": ["Vera Demberg", "Fatemeh Torabi Asr", "Merel Scholman"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1704.08893v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.08893v2", "num_discussion": 0, "originally_published_time": "4/28/2017", "pid": "1704.08893v2", "published_time": "3/15/2018", "rawpid": "1704.08893", "tags": ["cs.CL"], "title": "How compatible are our discourse annotations? Insights from mapping\n  RST-DT and PDTB annotations"}, {"abstract": "Crowdsourcing is a strategy to categorize data through the contribution of\nmany individuals. A wide range of theoretical and algorithmic contributions are\nbased on the model of Dawid and Skene [1]. Recently it was shown in [2,3] that,\nin certain regimes, belief propagation is asymptotically optimal for data\ngenerated from the Dawid-Skene model. This paper is motivated by this recent\nprogress. We analyze the dense limit of the Dawid-Skene model. It is shown that\nit belongs to a larger class of low-rank matrix estimation problems for which\nit is possible to express the asymptotic, Bayes-optimal, performance in a\nsimple closed form. In the dense limit the mapping to a low-rank matrix\nestimation problem provides an approximate message passing algorithm that\nsolves the problem algorithmically. We identify the regions where the algorithm\nefficiently computes the Bayes-optimal estimates. Our analysis refines the\nresults of [2,3] about optimality of message passing algorithms by\ncharacterizing regions of parameters where these algorithms do not match the\nBayes-optimal performance. We further study numerically the performance of\napproximate message passing, derived in the dense limit, on sparse instances\nand carry out experiments on a real world dataset.", "authors": ["Christian Schmidt", "Lenka Zdeborov\u00e1"], "category": "stat.ML", "comment": "16 pages, 7 figures", "img": "/static/thumbs/1803.04924v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04924v2", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.04924v2", "published_time": "3/15/2018", "rawpid": "1803.04924", "tags": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "physics.data-an"], "title": "Dense Limit of the Dawid-Skene Model for Crowdsourcing and Regions of\n  Sub-optimality of Message Passing Algorithms"}, {"abstract": "Compared with raw images, the more common JPEG images are less useful for\nmachine vision algorithms and professional photographers because JPEG-sRGB does\nnot preserve a linear relation between pixel values and the light measured from\nthe scene. A camera is said to be radiometrically calibrated if there is a\ncomputational model which can predict how the raw linear sensor image is mapped\nto the corresponding rendered image (e.g. JPEGs) and vice versa. This paper\nbegins with the observation that the rank order of pixel values are mostly\npreserved post colour correction. We show that this observation is the key to\nsolving for the whole camera pipeline (colour correction, tone and gamut\nmapping). Our rank-based calibration method is simpler than the prior art and\nso is parametrised by fewer variables which, concomitantly, can be solved for\nusing less calibration data. Another advantage is that we can derive the camera\npipeline from a single pair of raw-JPEG images. Experiments demonstrate that\nour method delivers state-of-the-art results (especially for the most\ninteresting case of JPEG to raw).", "authors": ["Han Gong", "Graham D. Finlayson", "Maryam M. Darrodi"], "category": "cs.CV", "comment": "accepted by BMVC 2017. Correction: Note that the reported model\n  parameter number in the original B...", "img": "/static/thumbs/1707.08943v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.08943v3", "num_discussion": 0, "originally_published_time": "7/27/2017", "pid": "1707.08943v3", "published_time": "3/15/2018", "rawpid": "1707.08943", "tags": ["cs.CV"], "title": "Concise Radiometric Calibration Using The Power of Ranking"}, {"abstract": "Computational approaches to the analysis of collective behavior in social\ninsects increasingly rely on motion paths as an intermediate data layer from\nwhich one can infer individual behaviors or social interactions. Honey bees are\na popular model for learning and memory. Previous experience has been shown to\naffect and modulate future social interactions. So far, no lifetime history\nobservations have been reported for all bees of a colony. In a previous work we\nintroduced a tracking system customized to track up to $4000$ bees over several\nweeks. In this contribution we present an in-depth description of the\nunderlying multi-step algorithm which both produces the motion paths, and also\nimproves the marker decoding accuracy significantly. We automatically tracked\n${\\sim}2000$ marked honey bees over 10 weeks with inexpensive recording\nhardware using markers without any error correction bits. We found that the\nproposed two-step tracking reduced incorrect ID decodings from initially\n${\\sim}13\\%$ to around $2\\%$ post-tracking. Alongside this paper, we publish\nthe first trajectory dataset for all bees in a colony, extracted from ${\\sim}\n4$ million images. We invite researchers to join the collective scientific\neffort to investigate this intriguing animal system. All components of our\nsystem are open-source.", "authors": ["Franziska Boenisch", "Benjamin Rosemann", "Benjamin Wild", "Fernando Wario", "David Dormagen", "Tim Landgraf"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1802.03192v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.03192v2", "num_discussion": 0, "originally_published_time": "2/9/2018", "pid": "1802.03192v2", "published_time": "3/15/2018", "rawpid": "1802.03192", "tags": ["cs.CV"], "title": "Tracking all members of a honey bee colony over their lifetime"}, {"abstract": "This paper proposes a new framework to regularize the \\textit{ill-posed} and\n\\textit{non-linear} blind image deconvolution problem by using deep generative\npriors. We employ two separate deep generative models --- one trained to\nproduce sharp images while the other trained to generate blur kernels from\nlower-dimensional parameters. The regularized problem is efficiently solved by\nsimple alternating gradient descent algorithm operating in the latent\nlower-dimensional space of each generative model. We empirically show that by\ndoing so, excellent image deblurring results are achieved even under\nextravagantly large blurs, and heavy noise. Our proposed method is in stark\ncontrast to the conventional end-to-end approaches, where a deep neural network\nis trained on blurred input, and the corresponding sharp output images while\ncompletely ignoring the knowledge of the underlying forward map (convolution\noperator) in image blurring.", "authors": ["Muhammad Asim", "Fahad Shamshad", "Ali Ahmed"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1802.04073v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.04073v2", "num_discussion": 0, "originally_published_time": "2/12/2018", "pid": "1802.04073v2", "published_time": "3/15/2018", "rawpid": "1802.04073", "tags": ["cs.CV"], "title": "Blind Image Deconvolution using Deep Generative Priors"}, {"abstract": "We propose a convolutional network with hierarchical classifiers for\nper-pixel semantic segmentation, which is able to be trained on multiple,\nheterogeneous datasets and exploit their semantic hierarchy. Our network is the\nfirst to be simultaneously trained on three different datasets from the\nintelligent vehicles domain, i.e. Cityscapes, GTSDB and Mapillary Vistas, and\nis able to handle different semantic level-of-detail, class imbalances, and\ndifferent annotation types, i.e. dense per-pixel and sparse bounding-box\nlabels. We assess our hierarchical approach, by comparing against flat,\nnon-hierarchical classifiers and we show improvements in mean pixel accuracy of\n13.0% for Cityscapes classes and 2.4% for Vistas classes and 32.3% for GTSDB\nclasses. Our implementation achieves inference rates of 17 fps at a resolution\nof 520 x 706 for 108 classes running on a GPU.", "authors": ["P. Meletis", "G. Dubbelman"], "category": "cs.CV", "comment": "under review in IEEE IV 2018", "img": "/static/thumbs/1803.05675v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05675v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05675v1", "published_time": "3/15/2018", "rawpid": "1803.05675", "tags": ["cs.CV", "cs.LG"], "title": "Training of Convolutional Networks on Multiple Heterogeneous Datasets\n  for Street Scene Semantic Segmentation"}, {"abstract": "Speech enhancement model is used to map a noisy speech to a clean speech. In\nthe training stage, an objective function is often adopted to optimize the\nmodel parameters. However, in most studies, there is an inconsistency between\nthe model optimization criterion and the evaluation criterion on the enhanced\nspeech. For example, in measuring speech intelligibility, most of the\nevaluation metric is based on a short-time objective intelligibility (STOI)\nmeasure, while the frame based minimum mean square error (MMSE) between\nestimated and clean speech is widely used in optimizing the model. Due to the\ninconsistency, there is no guarantee that the trained model can provide optimal\nperformance in applications. In this study, we propose an end-to-end\nutterance-based speech enhancement framework using fully convolutional neural\nnetworks (FCN) to reduce the gap between the model optimization and evaluation\ncriterion. Because of the utterance-based optimization, temporal correlation\ninformation of long speech segments, or even at the entire utterance level, can\nbe considered when perception-based objective functions are used for the direct\noptimization. As an example, we implement the proposed FCN enhancement\nframework to optimize the STOI measure. Experimental results show that the STOI\nof test speech is better than conventional MMSE-optimized speech due to the\nconsistency between the training and evaluation target. Moreover, by\nintegrating the STOI in model optimization, the intelligibility of human\nsubjects and automatic speech recognition (ASR) system on the enhanced speech\nis also substantially improved compared to those generated by the MMSE\ncriterion.", "authors": ["Szu-Wei Fu", "Tao-Wei Wang", "Yu Tsao", "Xugang Lu", "Hisashi Kawai"], "category": "stat.ML", "comment": "Accepted in IEEE Transactions on Audio, Speech and Language\n  Processing (TASLP)", "img": "/static/thumbs/1709.03658v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.03658v2", "num_discussion": 0, "originally_published_time": "9/12/2017", "pid": "1709.03658v2", "published_time": "3/15/2018", "rawpid": "1709.03658", "tags": ["stat.ML", "cs.LG", "cs.SD"], "title": "End-to-End Waveform Utterance Enhancement for Direct Evaluation Metrics\n  Optimization by Fully Convolutional Neural Networks"}, {"abstract": "Real-time semantic segmentation is of significant importance for mobile and\nrobotics related applications. We propose a computationally efficient\nsegmentation network which we term as ShuffleSeg. The proposed architecture is\nbased on grouped convolution and channel shuffling in its encoder for improving\nthe performance. An ablation study of different decoding methods is compared\nincluding Skip architecture, UNet, and Dilation Frontend. Interesting insights\non the speed and accuracy tradeoff is discussed. It is shown that skip\narchitecture in the decoding method provides the best compromise for the goal\nof real-time performance, while it provides adequate accuracy by utilizing\nhigher resolution feature maps for a more accurate segmentation. ShuffleSeg is\nevaluated on CityScapes and compared against the state of the art real-time\nsegmentation networks. It achieves 2x GFLOPs reduction, while it provides on\npar mean intersection over union of 58.3% on CityScapes test set. ShuffleSeg\nruns at 15.7 frames per second on NVIDIA Jetson TX2, which makes it of great\npotential for real-time applications.", "authors": ["Mostafa Gamal", "Mennatullah Siam", "Moemen Abdel-Razek"], "category": "cs.CV", "comment": "6 pages, under review by ICIP 2018", "img": "/static/thumbs/1803.03816v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.03816v2", "num_discussion": 0, "originally_published_time": "3/10/2018", "pid": "1803.03816v2", "published_time": "3/15/2018", "rawpid": "1803.03816", "tags": ["cs.CV"], "title": "ShuffleSeg: Real-time Semantic Segmentation Network"}, {"abstract": "This paper reports on modern approaches in Information Extraction (IE) and\nits two main sub-tasks of Named Entity Recognition (NER) and Relation\nExtraction (RE). Basic concepts and the most recent approaches in this area are\nreviewed, which mainly include Machine Learning (ML) based approaches and the\nmore recent trend to Deep Learning (DL) based methods.", "authors": ["Parisa Naderi Golshan", "HosseinAli Rahmani Dashti", "Shahrzad Azizi", "Leila Safari"], "category": "cs.IR", "comment": "", "img": "/static/thumbs/1803.05667v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05667v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05667v1", "published_time": "3/15/2018", "rawpid": "1803.05667", "tags": ["cs.IR", "cs.CL"], "title": "A Study of Recent Contributions on Information Extraction"}, {"abstract": "Here we consider some well-known facts in syntax from a physics perspective,\nwhich allows us to establish some remarkable equivalences. Specifically, we\nobserve that the operation MERGE put forward by N. Chomsky in 1995 can be\ninterpreted as a physical information coarse-graining. Thus, MERGE in\nlinguistics entails information renormalization in physics, according to\ndifferent time scales. We make this point mathematically formal in terms of\nlanguage models, i.e., probability distributions over word sequences, widely\nused in natural language processing as well as other ambits. In this setting,\nMERGE corresponds to a 3-index probability tensor implementing a\ncoarse-graining, akin to a probabilistic context-free grammar. The probability\nvectors of meaningful sentences are naturally given by stochastic tensor\nnetworks (TN) that are mostly loop-free, such as Tree Tensor Networks and\nMatrix Product States. These structures have short-ranged correlations in the\nsyntactic distance by construction and, because of the peculiarities of human\nlanguage, they are extremely efficient to manipulate computationally. We also\npropose how to obtain such language models from probability distributions of\ncertain TN quantum states, which we show to be efficiently preparable by a\nquantum computer. Moreover, using tools from entanglement theory, we use these\nquantum states to prove classical lower bounds on the perplexity of the\nprobability distribution for a set of words in a sentence. Implications of\nthese results are discussed in the ambits of theoretical and computational\nlinguistics, artificial intelligence, programming languages, RNA and protein\nsequencing, quantum many-body systems, and beyond. Our work shows how many of\nthe key linguistic ideas from the last century, including developments in\ncomputational linguistics, fit perfectly with known physical concepts linked to\nrenormalization.", "authors": ["Angel J. Gallego", "Roman Orus"], "category": "cs.CL", "comment": "22 pages, 21 figures, 1 table. Revised version with new title,\n  appendix with some formal linguisti...", "img": "/static/thumbs/1708.01525v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1708.01525v3", "num_discussion": 0, "originally_published_time": "8/4/2017", "pid": "1708.01525v3", "published_time": "3/15/2018", "rawpid": "1708.01525", "tags": ["cs.CL", "cond-mat.str-el", "physics.hist-ph", "quant-ph"], "title": "Language Design and Renormalization"}, {"abstract": "Relation classification is an important semantic processing task in the field\nof natural language processing. In this paper, we propose the task of relation\nclassification for Chinese literature text. A new dataset of Chinese literature\ntext is constructed to facilitate the study in this task. We present a novel\nmodel, named Structure Regularized Bidirectional Recurrent Convolutional Neural\nNetwork (SR-BRCNN), to identify the relation between entities. The proposed\nmodel learns relation representations along the shortest dependency path (SDP)\nextracted from the structure regularized dependency tree, which has the\nbenefits of reducing the complexity of the whole model. Experimental results\nshow that the proposed method significantly improves the F1 score by 10.3, and\noutperforms the state-of-the-art approaches on Chinese literature text.", "authors": ["Ji Wen", "Xu Sun", "Xuancheng Ren", "Qi Su"], "category": "cs.CL", "comment": "Accepted at NAACL HLT 2018", "img": "/static/thumbs/1803.05662v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05662v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05662v1", "published_time": "3/15/2018", "rawpid": "1803.05662", "tags": ["cs.CL"], "title": "Structure Regularized Neural Network for Entity Relation Classification\n  for Chinese Literature Text"}, {"abstract": "A visual-relational knowledge graph (KG) is a multi-relational graph whose\nentities are associated with images. We introduce ImageGraph, a KG with 1,330\nrelation types, 14,870 entities, and 829,931 images. Visual-relational KGs lead\nto novel probabilistic query types where images are treated as first-class\ncitizens. Both the prediction of relations between unseen images and\nmulti-relational image retrieval can be formulated as query types in a\nvisual-relational KG. We approach the problem of answering such queries with a\nnovel combination of deep convolutional networks and models for learning\nknowledge graph embeddings. The resulting models can answer queries such as\n\"How are these two unseen images related to each other?\" We also explore a\nzero-shot learning scenario where an image of an entirely new entity is linked\nwith multiple relations to entities of an existing KG. The multi-relational\ngrounding of unseen entity images into a knowledge graph serves as the\ndescription of such an entity. We conduct experiments to demonstrate that the\nproposed deep architectures in combination with KG embedding objectives can\nanswer the visual-relational queries efficiently and accurately.", "authors": ["Daniel O\u00f1oro-Rubio", "Mathias Niepert", "Alberto Garc\u00eda-Dur\u00e1n", "Roberto Gonz\u00e1lez", "Roberto J. L\u00f3pez-Sastre"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1709.02314v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.02314v4", "num_discussion": 0, "originally_published_time": "9/7/2017", "pid": "1709.02314v4", "published_time": "3/15/2018", "rawpid": "1709.02314", "tags": ["cs.LG", "cs.AI"], "title": "Representation Learning for Visual-Relational Knowledge Graphs"}, {"abstract": "Subspace clustering is a useful technique for many computer vision\napplications in which the intrinsic dimension of high-dimensional data is often\nsmaller than the ambient dimension. Spectral clustering, as one of the main\napproaches to subspace clustering, often takes on a sparse representation or a\nlow-rank representation to learn a block diagonal self-representation matrix\nfor subspace generation. However, existing methods require solving a large\nscale convex optimization problem with a large set of data, with computational\ncomplexity reaches O(N^3) for N data points. Therefore, the efficiency and\nscalability of traditional spectral clustering methods can not be guaranteed\nfor large scale datasets. In this paper, we propose a subspace clustering model\nbased on the Kronecker product. Due to the property that the Kronecker product\nof a block diagonal matrix with any other matrix is still a block diagonal\nmatrix, we can efficiently learn the representation matrix which is formed by\nthe Kronecker product of k smaller matrices. By doing so, our model\nsignificantly reduces the computational complexity to O(kN^{3/k}). Furthermore,\nour model is general in nature, and can be adapted to different regularization\nbased subspace clustering methods. Experimental results on two public datasets\nshow that our model significantly improves the efficiency compared with several\nstate-of-the-art methods. Moreover, we have conducted experiments on synthetic\ndata to verify the scalability of our model for large scale datasets.", "authors": ["Lei Zhou", "Xiao Bai", "Xianglong Liu", "Jun Zhou", "Hancock Edwin"], "category": "cs.LG", "comment": "16 pages, 2 figures", "img": "/static/thumbs/1803.05657v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05657v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05657v1", "published_time": "3/15/2018", "rawpid": "1803.05657", "tags": ["cs.LG", "cs.CV", "stat.ML"], "title": "Fast Subspace Clustering Based on the Kronecker Product"}, {"abstract": "This paper describes the system which got the state-of-the-art results at\nSemEval-2018 Task 11: Machine Comprehension using Commonsense Knowledge. In\nthis paper, we present a neural network called Hybrid Multi-Aspects (HMA)\nmodel, which mimic the human\u0027s intuitions on dealing with the multiple-choice\nreading comprehension. In this model, we aim to produce the predictions in\nmultiple aspects by calculating attention among the text, question and choices,\nand combine these results for final predictions. Experimental results show that\nour HMA model could give substantial improvements over the baseline system and\ngot the first place on the final test set leaderboard with the accuracy of\n84.13%.", "authors": ["Zhipeng Chen", "Yiming Cui", "Wentao Ma", "Shijin Wang", "Ting Liu", "Guoping Hu"], "category": "cs.CL", "comment": "6 pages", "img": "/static/thumbs/1803.05655v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05655v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05655v1", "published_time": "3/15/2018", "rawpid": "1803.05655", "tags": ["cs.CL"], "title": "HFL-RC System at SemEval-2018 Task 11: Hybrid Multi-Aspects Model for\n  Commonsense Reading Comprehension"}, {"abstract": "Machine Reading Comprehension (MRC) has become enormously popular recently\nand has attracted a lot of attention. However, existing reading comprehension\ndatasets are mostly in English. To add diversity in reading comprehension\ndatasets, in this paper we propose a new Chinese reading comprehension dataset\nfor accelerating related research in the community. The proposed dataset\ncontains two different types: cloze-style reading comprehension and user query\nreading comprehension, associated with large-scale training data as well as\nhuman-annotated validation and hidden test set. Along with this dataset, we\nalso hosted the first Evaluation on Chinese Machine Reading Comprehension\n(CMRC-2017) and successfully attracted tens of participants, which suggest the\npotential impact of this dataset.", "authors": ["Yiming Cui", "Ting Liu", "Zhipeng Chen", "Wentao Ma", "Shijin Wang", "Guoping Hu"], "category": "cs.CL", "comment": "5 pages, published at LREC 2018", "img": "/static/thumbs/1709.08299v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.08299v2", "num_discussion": 0, "originally_published_time": "9/25/2017", "pid": "1709.08299v2", "published_time": "3/15/2018", "rawpid": "1709.08299", "tags": ["cs.CL"], "title": "Dataset for the First Evaluation on Chinese Machine Reading\n  Comprehension"}, {"abstract": "Word vectors require significant amounts of memory and storage, posing issues\nto resource limited devices like mobile phones and GPUs. We show that high\nquality quantized word vectors using 1-2 bits per parameter can be learned by\nintroducing a quantization function into Word2Vec. We furthermore show that\ntraining with the quantization function acts as a regularizer. We train word\nvectors on English Wikipedia (2017) and evaluate them on standard word\nsimilarity and analogy tasks and on question answering (SQuAD). Our quantized\nword vectors not only take 8-16x less space than full precision (32 bit) word\nvectors but also outperform them on word similarity tasks and question\nanswering.", "authors": ["Maximilian Lam"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1803.05651v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05651v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05651v1", "published_time": "3/15/2018", "rawpid": "1803.05651", "tags": ["cs.CL"], "title": "Word2Bits - Quantized Word Vectors"}, {"abstract": "Reading comprehension has embraced a booming in recent NLP research. Several\ninstitutes have released the Cloze-style reading comprehension data, and these\nhave greatly accelerated the research of machine comprehension. In this work,\nwe firstly present Chinese reading comprehension datasets, which consist of\nPeople Daily news dataset and Children\u0027s Fairy Tale (CFT) dataset. Also, we\npropose a consensus attention-based neural network architecture to tackle the\nCloze-style reading comprehension problem, which aims to induce a consensus\nattention over every words in the query. Experimental results show that the\nproposed neural network significantly outperforms the state-of-the-art\nbaselines in several public datasets. Furthermore, we setup a baseline for\nChinese reading comprehension task, and hopefully this would speed up the\nprocess for future research.", "authors": ["Yiming Cui", "Ting Liu", "Zhipeng Chen", "Shijin Wang", "Guoping Hu"], "category": "cs.CL", "comment": "9+1 pages, published at COLING 2016", "img": "/static/thumbs/1607.02250v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1607.02250v3", "num_discussion": 0, "originally_published_time": "7/8/2016", "pid": "1607.02250v3", "published_time": "3/15/2018", "rawpid": "1607.02250", "tags": ["cs.CL", "cs.NE"], "title": "Consensus Attention-based Neural Networks for Chinese Reading\n  Comprehension"}, {"abstract": "Variational inference relies on flexible approximate posterior distributions.\nNormalizing flows provide a general recipe to construct flexible variational\nposteriors. We introduce Sylvester normalizing flows, which can be seen as a\ngeneralization of planar flows. Sylvester normalizing flows remove the\nwell-known single-unit bottleneck from planar flows, making a single\ntransformation much more flexible. We compare the performance of Sylvester\nnormalizing flows against planar flows and inverse autoregressive flows and\ndemonstrate that they compare favorably on several datasets.", "authors": ["Rianne van den Berg", "Leonard Hasenclever", "Jakub M. Tomczak", "Max Welling"], "category": "stat.ML", "comment": "12 pages, 4 figures", "img": "/static/thumbs/1803.05649v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05649v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05649v1", "published_time": "3/15/2018", "rawpid": "1803.05649", "tags": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "title": "Sylvester Normalizing Flows for Variational Inference"}, {"abstract": "Learning to estimate 3D geometry in a single image by watching unlabeled\nvideos via deep convolutional network is attracting significant attention. In\nthis paper, we introduce a \"3D as-smooth-as-possible (3D-ASAP)\" priori inside\nthe pipeline, which enables joint estimation of edges and 3D scene, yielding\nresults with significant improvement in accuracy for fine detailed structures.\nSpecifically, we define the 3D-ASAP priori by requiring that any two points\nrecovered in 3D from an image should lie on an existing planar surface if no\nother cues provided. We design an unsupervised framework that Learns Edges and\nGeometry (depth, normal) all at Once (LEGO). The predicted edges are embedded\ninto depth and surface normal smoothness terms, where pixels without edges\nin-between are constrained to satisfy the priori. In our framework, the\npredicted depths, normals and edges are forced to be consistent all the time.\nWe conduct experiments on KITTI to evaluate our estimated geometry and\nCityScapes to perform edge evaluation. We show that in all of the tasks,\ni.e.depth, normal and edge, our algorithm vastly outperforms other\nstate-of-the-art (SOTA) algorithms, demonstrating the benefits of our approach.", "authors": ["Zhenheng Yang", "Peng Wang", "Yang Wang", "Wei Xu", "Ram Nevatia"], "category": "cs.CV", "comment": "Accepted to CVPR 2018 as spotlight", "img": "/static/thumbs/1803.05648v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05648v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05648v1", "published_time": "3/15/2018", "rawpid": "1803.05648", "tags": ["cs.CV"], "title": "LEGO: Learning Edge with Geometry all at Once by Watching Videos"}, {"abstract": "Distributed sparse learning with a cluster of multiple machines has attracted\nmuch attention in machine learning, especially for large-scale applications\nwith high-dimensional data. One popular way to implement sparse learning is to\nuse $L_1$ regularization. In this paper, we propose a novel method, called\nproximal \\mbox{SCOPE}~(\\mbox{pSCOPE}), for distributed sparse learning with\n$L_1$ regularization. pSCOPE is based on a \\underline{c}ooperative\n\\underline{a}utonomous \\underline{l}ocal \\underline{l}earning~(\\mbox{CALL})\nframework. In the \\mbox{CALL} framework of \\mbox{pSCOPE}, we find that the data\npartition affects the convergence of the learning procedure, and subsequently\nwe define a metric to measure the goodness of a data partition. Based on the\ndefined metric, we theoretically prove that pSCOPE is convergent with a linear\nconvergence rate if the data partition is good enough. We also prove that\nbetter data partition implies faster convergence rate. Furthermore, pSCOPE is\nalso communication efficient. Experimental results on real data sets show that\npSCOPE can outperform other state-of-the-art distributed methods for sparse\nlearning.", "authors": ["Shen-Yi Zhao", "Gong-Duo Zhang", "Ming-Wei Li", "Wu-Jun Li"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1803.05621v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05621v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05621v1", "published_time": "3/15/2018", "rawpid": "1803.05621", "tags": ["stat.ML", "cs.LG"], "title": "Proximal SCOPE for Distributed Sparse Learning: Better Data Partition\n  Implies Faster Convergence Rate"}, {"abstract": "Combinatorial features are essential for the success of many commercial\nmodels. Manually crafting these features usually comes with high cost due to\nthe variety, volume and velocity of raw data in web-scale systems.\nFactorization based models, which measure interactions in terms of vector\nproduct, can learn patterns of combinatorial features automatically and\ngeneralize to unseen features as well. With the great success of deep neural\nworks (DNNs) in various fields, recently researchers have proposed several\nDNN-based factorization model to learn both low- and high-order feature\ninteractions. Despite the powerful ability of learning an arbitrary function\nfrom data, plain DNNs generate feature interactions implicitly and at the\nbit-wise level. In this paper, we propose a novel Compressed Interaction\nNetwork (CIN), which aims to generate feature interactions in an explicit\nfashion and at the vector-wise level. We show that the CIN share some\nfunctionalities with convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs). We further combine a CIN and a classical DNN into one unified\nmodel, and named this new model eXtreme Deep Factorization Machine (xDeepFM).\nOn one hand, the xDeepFM is able to learn certain bounded-degree feature\ninteractions explicitly; on the other hand, it can learn arbitrary low- and\nhigh-order feature interactions implicitly. We conduct comprehensive\nexperiments on three real-world datasets. Our results demonstrate that xDeepFM\noutperforms state-of-the-art models. We have released the source code of\nxDeepFM at https://github.com/Leavingseason/xDeepFM.", "authors": ["Jianxun Lian", "Xiaohuan Zhou", "Fuzheng Zhang", "Zhongxia Chen", "Xing Xie", "Guangzhong Sun"], "category": "cs.LG", "comment": "9 pages", "img": "/static/thumbs/1803.05170v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05170v2", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05170v2", "published_time": "3/15/2018", "rawpid": "1803.05170", "tags": ["cs.LG", "cs.IR"], "title": "xDeepFM: Combining Explicit and Implicit Feature Interactions for\n  Recommender Systems"}, {"abstract": "Image processing and pixel-wise dense prediction have been advanced by\nharnessing the capabilities of deep learning. One central issue of deep\nlearning is the limited capacity to handle joint upsampling. We present a deep\nlearning building block for joint upsampling, namely guided filtering layer.\nThis layer aims at efficiently generating the high-resolution output given the\ncorresponding low-resolution one and a high-resolution guidance map. The\nproposed layer is composed of a guided filter, which is reformulated as a fully\ndifferentiable block. To this end, we show that a guided filter can be\nexpressed as a group of spatial varying linear transformation matrices. This\nlayer could be integrated with the convolutional neural networks (CNNs) and\njointly optimized through end-to-end training. To further take advantage of\nend-to-end training, we plug in a trainable transformation function that\ngenerates task-specific guidance maps. By integrating the CNNs and the proposed\nlayer, we form deep guided filtering networks. The proposed networks are\nevaluated on five advanced image processing tasks. Experiments on MIT-Adobe\nFiveK Dataset demonstrate that the proposed approach runs 10-100 times faster\nand achieves the state-of-the-art performance. We also show that the proposed\nguided filtering layer helps to improve the performance of multiple pixel-wise\ndense prediction tasks. The code is available at\nhttps://github.com/wuhuikai/DeepGuidedFilter.", "authors": ["Huikai Wu", "Shuai Zheng", "Junge Zhang", "Kaiqi Huang"], "category": "cs.CV", "comment": "Accepted by CVPR 2018. Project Website:\n  http://wuhuikai.me/DeepGuidedFilterProject", "img": "/static/thumbs/1803.05619v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05619v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05619v1", "published_time": "3/15/2018", "rawpid": "1803.05619", "tags": ["cs.CV"], "title": "Fast End-to-End Trainable Guided Filter"}, {"abstract": "This paper introduces a novel rotation-based framework for arbitrary-oriented\ntext detection in natural scene images. We present the Rotation Region Proposal\nNetworks (RRPN), which are designed to generate inclined proposals with text\norientation angle information. The angle information is then adapted for\nbounding box regression to make the proposals more accurately fit into the text\nregion in terms of the orientation. The Rotation Region-of-Interest (RRoI)\npooling layer is proposed to project arbitrary-oriented proposals to a feature\nmap for a text region classifier. The whole framework is built upon a\nregion-proposal-based architecture, which ensures the computational efficiency\nof the arbitrary-oriented text detection compared with previous text detection\nsystems. We conduct experiments using the rotation-based framework on three\nreal-world scene text detection datasets and demonstrate its superiority in\nterms of effectiveness and efficiency over previous approaches.", "authors": ["Jianqi Ma", "Weiyuan Shao", "Hao Ye", "Li Wang", "Hong Wang", "Yingbin Zheng", "Xiangyang Xue"], "category": "cs.CV", "comment": "Code is available at: https://github.com/mjq11302010044/RRPN", "img": "/static/thumbs/1703.01086v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.01086v3", "num_discussion": 0, "originally_published_time": "3/3/2017", "pid": "1703.01086v3", "published_time": "3/15/2018", "rawpid": "1703.01086", "tags": ["cs.CV"], "title": "Arbitrary-Oriented Scene Text Detection via Rotation Proposals"}, {"abstract": "Consider a Gaussian memoryless multiple source with $m$ components with joint\nprobability distribution known only to lie in a given class of distributions. A\nsubset of $k \\leq m$ components are sampled and compressed with the objective\nof reconstructing all the $m$ components within a specified level of distortion\nunder a mean-squared error criterion. In Bayesian and nonBayesian settings, the\nnotion of universal sampling rate distortion function for Gaussian sources is\nintroduced to capture the optimal tradeoffs among sampling, compression rate\nand distortion level. Single-letter characterizations are provided for the\nuniversal sampling rate distortion function. Our achievability proofs highlight\nthe following structural property: it is optimal to compress and reconstruct\nfirst the sampled components of the GMMS alone, and then form estimates for the\nunsampled components based on the former.", "authors": ["Vinay Praneeth Boda"], "category": "cs.IT", "comment": "", "img": "/static/thumbs/1803.05605v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05605v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05605v1", "published_time": "3/15/2018", "rawpid": "1803.05605", "tags": ["cs.IT", "cs.LG", "math.IT"], "title": "Reconstructing Gaussian sources by spatial sampling"}, {"abstract": "We present a formulation of deep learning that aims at producing a large\nmargin classifier. The notion of margin, minimum distance to a decision\nboundary, has served as the foundation of several theoretically profound and\nempirically successful results for both classification and regression tasks.\nHowever, most large margin algorithms are applicable only to shallow models\nwith a preset feature representation; and conventional margin methods for\nneural networks only enforce margin at the output layer. Such methods are\ntherefore not well suited for deep networks.\n  In this work, we propose a novel loss function to impose a margin on any\nchosen set of layers of a deep network (including input and hidden layers). Our\nformulation allows choosing any norm on the metric measuring the margin. We\ndemonstrate that the decision boundary obtained by our loss has nice properties\ncompared to standard classification loss functions. Specifically, we show\nimproved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on\nmultiple tasks: generalization from small training sets, corrupted labels, and\nrobustness against adversarial perturbations. The resulting loss is general and\ncomplementary to existing data augmentation (such as random/adversarial input\ntransform) and regularization techniques (such as weight decay, dropout, and\nbatch norm).", "authors": ["Gamaleldin F. Elsayed", "Dilip Krishnan", "Hossein Mobahi", "Kevin Regan", "Samy Bengio"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1803.05598v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05598v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05598v1", "published_time": "3/15/2018", "rawpid": "1803.05598", "tags": ["stat.ML", "cs.LG"], "title": "Large Margin Deep Networks for Classification"}, {"abstract": "Momentum based stochastic gradient methods such as heavy ball (HB) and\nNesterov\u0027s accelerated gradient descent (NAG) method are widely used in\npractice for training deep networks and other supervised learning models, as\nthey often provide significant improvements over stochastic gradient descent\n(SGD). Rigorously speaking, \"fast gradient\" methods have provable improvements\nover gradient descent only for the deterministic case, where the gradients are\nexact. In the stochastic case, the popular explanations for their wide\napplicability is that when these fast gradient methods are applied in the\nstochastic case, they partially mimic their exact gradient counterparts,\nresulting in some practical gain. This work provides a counterpoint to this\nbelief by proving that there exist simple problem instances where these methods\ncannot outperform SGD despite the best setting of its parameters. These\nnegative problem instances are, in an informal sense, generic; they do not look\nlike carefully constructed pathological instances. These results suggest (along\nwith empirical evidence) that HB or NAG\u0027s practical performance gains are a\nby-product of mini-batching.\n  Furthermore, this work provides a viable (and provable) alternative, which,\non the same set of problem instances, significantly improves over HB, NAG, and\nSGD\u0027s performance. This algorithm, referred to as Accelerated Stochastic\nGradient Descent (ASGD), is a simple to implement stochastic algorithm, based\non a relatively less popular variant of Nesterov\u0027s Acceleration. Extensive\nempirical results in this paper show that ASGD has performance gains over HB,\nNAG, and SGD.", "authors": ["Rahul Kidambi", "Praneeth Netrapalli", "Prateek Jain", "Sham M. Kakade"], "category": "cs.LG", "comment": "28 pages, 10 figures. Appears as an oral presentation at\n  International Conference on Learning Repr...", "img": "/static/thumbs/1803.05591v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05591v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05591v1", "published_time": "3/15/2018", "rawpid": "1803.05591", "tags": ["cs.LG"], "title": "On the insufficiency of existing momentum schemes for Stochastic\n  Optimization"}, {"abstract": "Probabilistic graphical models are a key tool in machine learning\napplications. Computing the partition function, i.e., normalizing constant, is\na fundamental task of statistical inference but it is generally computationally\nintractable, leading to extensive study of approximation methods. Iterative\nvariational methods are a popular and successful family of approaches. However,\neven state of the art variational methods can return poor results or fail to\nconverge on difficult instances. In this paper, we instead consider computing\nthe partition function via sequential summation over variables. We develop\nrobust approximate algorithms by combining ideas from mini-bucket elimination\nwith tensor network and renormalization group methods from statistical physics.\nThe resulting \"convergence-free\" methods show good empirical performance on\nboth synthetic and real-world benchmark models, even for difficult instances.", "authors": ["Sungsoo Ahn", "Michael Chertkov", "Adrian Weller", "Jinwoo Shin"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1803.05104v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05104v2", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05104v2", "published_time": "3/15/2018", "rawpid": "1803.05104", "tags": ["stat.ML"], "title": "Bucket Renormalization for Approximate Inference"}, {"abstract": "In recent years, car makers and tech companies have been racing towards self\ndriving cars. It seems that the main parameter in this race is who will have\nthe first car on the road. The goal of this paper is to add to the equation two\nadditional crucial parameters. The first is standardization of safety assurance\n--- what are the minimal requirements that every self-driving car must satisfy,\nand how can we verify these requirements. The second parameter is scalability\n--- engineering solutions that lead to unleashed costs will not scale to\nmillions of cars, which will push interest in this field into a niche academic\ncorner, and drive the entire field into a \"winter of autonomous driving\". In\nthe first part of the paper we propose a white-box, interpretable, mathematical\nmodel for safety assurance, which we call Responsibility-Sensitive Safety\n(RSS). In the second part we describe a design of a system that adheres to our\nsafety assurance requirements and is scalable to millions of cars.", "authors": ["Shai Shalev-Shwartz", "Shaked Shammah", "Amnon Shashua"], "category": "cs.RO", "comment": "", "img": "/static/thumbs/1708.06374v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1708.06374v5", "num_discussion": 0, "originally_published_time": "8/21/2017", "pid": "1708.06374v5", "published_time": "3/15/2018", "rawpid": "1708.06374", "tags": ["cs.RO", "cs.AI", "stat.ML"], "title": "On a Formal Model of Safe and Scalable Self-driving Cars"}, {"abstract": "Recent efforts on combining deep models with probabilistic graphical models\nare promising in providing flexible models that are also easy to interpret. We\npropose a variational message-passing algorithm for variational inference in\nsuch models. We make three contributions. First, we propose structured\ninference networks that incorporate the structure of the graphical model in the\ninference network of variational auto-encoders (VAE). Second, we establish\nconditions under which such inference networks enable fast amortized inference\nsimilar to VAE. Finally, we derive a variational message passing algorithm to\nperform efficient natural-gradient inference while retaining the efficiency of\nthe amortized inference. By simultaneously enabling structured, amortized, and\nnatural-gradient inference for deep structured models, our method simplifies\nand generalizes existing methods.", "authors": ["Wu Lin", "Nicolas Hubacher", "Mohammad Emtiyaz Khan"], "category": "stat.ML", "comment": "Published at ICLR 2018", "img": "/static/thumbs/1803.05589v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05589v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05589v1", "published_time": "3/15/2018", "rawpid": "1803.05589", "tags": ["stat.ML"], "title": "Variational Message Passing with Structured Inference Networks"}, {"abstract": "Facial action unit (AU) detection and face alignment are two highly\ncorrelated tasks since facial landmarks can provide precise AU locations to\nfacilitate the extraction of meaningful local features for AU detection. Most\nexisting AU detection works often treat face alignment as a preprocessing and\nhandle the two tasks independently. In this paper, we propose a novel\nend-to-end deep learning framework for joint AU detection and face alignment,\nwhich has not been explored before. In particular, multi-scale shared features\nare learned firstly, and high-level features of face alignment are fed into AU\ndetection. Moreover, to extract precise local features, we propose an adaptive\nattention learning module to refine the attention map of each AU adaptively.\nFinally, the assembled local features are integrated with face alignment\nfeatures and global features for AU detection. Experiments on BP4D and DISFA\nbenchmarks demonstrate that our framework significantly outperforms the\nstate-of-the-art methods for AU detection.", "authors": ["Zhiwen Shao", "Zhilei Liu", "Jianfei Cai", "Lizhuang Ma"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05588v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05588v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05588v1", "published_time": "3/15/2018", "rawpid": "1803.05588", "tags": ["cs.CV"], "title": "Deep Adaptive Attention for Joint Facial Action Unit Detection and Face\n  Alignment"}, {"abstract": "Models applied on real time response task, like click-through rate (CTR)\nprediction model, require high accuracy and rigorous response time. Therefore,\ntop-performing deep models of high depth and complexity are not well suited for\nthese applications with the limitations on the inference time. In order to\nfurther improve the neural networks\u0027 performance given the time and\ncomputational limitations, we propose an approach that exploits a cumbersome\nnet to help train the lightweight net for prediction. We dub the whole process\nrocket launching, where the cumbersome booster net is used to guide the\nlearning of the target light net throughout the whole training process. We\nanalyze different loss functions aiming at pushing the light net to behave\nsimilarly to the booster net, and adopt the loss with best performance in our\nexperiments. We use one technique called gradient block to improve the\nperformance of the light net and booster net further. Experiments on benchmark\ndatasets and real-life industrial advertisement data present that our light\nmodel can get performance only previously achievable with more complex models.", "authors": ["Guorui Zhou", "Ying Fan", "Runpeng Cui", "Weijie Bian", "Xiaoqiang Zhu", "Kun Gai"], "category": "stat.ML", "comment": "10 pages, AAAI2018", "img": "/static/thumbs/1708.04106v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1708.04106v3", "num_discussion": 0, "originally_published_time": "8/14/2017", "pid": "1708.04106v3", "published_time": "3/15/2018", "rawpid": "1708.04106", "tags": ["stat.ML", "cs.LG", "I.2.6"], "title": "Rocket Launching: A Universal and Efficient Framework for Training\n  Well-performing Light Net"}, {"abstract": "Human parsing is attracting increasing research attention. In this work, we\naim to push the frontier of human parsing by introducing the problem of\nmulti-human parsing in the wild. Existing works on human parsing mainly tackle\nsingle-person scenarios, which deviates from real-world applications where\nmultiple persons are present simultaneously with interaction and occlusion. To\naddress the multi-human parsing problem, we introduce a new multi-human parsing\n(MHP) dataset and a novel multi-human parsing model named MH-Parser. The MHP\ndataset contains multiple persons captured in real-world scenes with\npixel-level fine-grained semantic annotations in an instance-aware setting. The\nMH-Parser generates global parsing maps and person instance masks\nsimultaneously in a bottom-up fashion with the help of a new Graph-GAN model.\nWe envision that the MHP dataset will serve as a valuable data resource to\ndevelop new multi-human parsing models, and the MH-Parser offers a strong\nbaseline to drive future research for multi-human parsing in the wild.", "authors": ["Jianshu Li", "Jian Zhao", "Yunchao Wei", "Congyan Lang", "Yidong Li", "Terence Sim", "Shuicheng Yan", "Jiashi Feng"], "category": "cs.CV", "comment": "The first two authors are with equal contribution", "img": "/static/thumbs/1705.07206v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.07206v2", "num_discussion": 0, "originally_published_time": "5/19/2017", "pid": "1705.07206v2", "published_time": "3/15/2018", "rawpid": "1705.07206", "tags": ["cs.CV"], "title": "Multiple-Human Parsing in the Wild"}, {"abstract": "Digital face manipulation has become a popular and fascinating way to touch\nimages due to the prevalence of smart phones and social networks. With a wide\nvariety of user preferences, facial expressions and accessories, a general and\nflexible model is necessary to accommodate different types of facial editing.\nIn this paper, we propose such a model based on an end-to-end convolutional\nneural network that supports fast inference, edit-effect control, and quick\npartial-model update. In addition, this model learns from unpaired image sets\nwith different attributes. Experimental results show that our framework can\nhandle a wide range of face expressions, accessories, and makeup effects,\nproducing high-resolution and high-quality results in high speed.", "authors": ["Ying-Cong Chen", "Huaijia Lin", "Ruiyu Li", "Xin Tao", "Michelle Shu", "Yangang Ye", "Xiaoyong Shen", "Jiaya Jia"], "category": "cs.CV", "comment": "Accepted by CVPR 2018", "img": "/static/thumbs/1803.05576v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05576v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05576v1", "published_time": "3/15/2018", "rawpid": "1803.05576", "tags": ["cs.CV"], "title": "Facelet-Bank for Fast Portrait Manipulation"}, {"abstract": "We present Optimal Transport GAN (OT-GAN), a variant of generative\nadversarial nets minimizing a new metric measuring the distance between the\ngenerator distribution and the data distribution. This metric, which we call\nmini-batch energy distance, combines optimal transport in primal form with an\nenergy distance defined in an adversarially learned feature space, resulting in\na highly discriminative distance function with unbiased mini-batch gradients.\nExperimentally we show OT-GAN to be highly stable when trained with large\nmini-batches, and we present state-of-the-art results on several popular\nbenchmark problems for image generation.", "authors": ["Tim Salimans", "Han Zhang", "Alec Radford", "Dimitris Metaxas"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1803.05573v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05573v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05573v1", "published_time": "3/15/2018", "rawpid": "1803.05573", "tags": ["cs.LG", "stat.ML"], "title": "Improving GANs Using Optimal Transport"}, {"abstract": "We address the correspondence search problem among multiple graphs with\ncomplex properties while considering the matching consistency. We describe each\npair of graphs by combining multiple attributes, then jointly match them in a\nunified framework. The main contribution of this paper is twofold. First, we\nformulate the global correspondence search problem of multi-attributed graphs\nby utilizing a set of multi-layer structures. The proposed formulation\ndescribes each pair of graphs as a multi-layer structure, and jointly considers\nwhole matching pairs. Second, we propose a robust multiple graph matching\nmethod based on the multi-layer random walks framework. The proposed framework\nsynchronizes movements of random walkers, and leads them to consistent matching\ncandidates. In our extensive experiments, the proposed method exhibits robust\nand accurate performance over the state-of-the-art multiple graph matching\nalgorithms.", "authors": ["Han-Mu Park", "Kuk-Jin Yoon"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1712.02575v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1712.02575v2", "num_discussion": 0, "originally_published_time": "12/7/2017", "pid": "1712.02575v2", "published_time": "3/15/2018", "rawpid": "1712.02575", "tags": ["cs.CV"], "title": "Consistent Multiple Graph Matching with Multi-layer Random Walks\n  Synchronization"}, {"abstract": "Employing deep learning-based approaches for fine-grained facial expression\nanalysis, such as those involving the estimation of Action Unit (AU)\nintensities, is difficult due to the lack of a large-scale dataset of real\nfaces with sufficiently diverse AU labels for training. In this paper, we\nconsider how AU-level facial image synthesis can be used to substantially\naugment such a dataset. We propose an AU synthesis framework that combines the\nwell-known 3D Morphable Model (3DMM), which intrinsically disentangles\nexpression parameters from other face attributes, with models that\nadversarially generate 3DMM expression parameters conditioned on given target\nAU labels, in contrast to the more conventional approach of generating facial\nimages directly. In this way, we are able to synthesize new combinations of\nexpression parameters and facial images from desired AU labels. Extensive\nquantitative and qualitative results on the benchmark DISFA dataset demonstrate\nthe effectiveness of our method on 3DMM facial expression parameter synthesis\nand data augmentation for deep learning-based AU intensity estimation.", "authors": ["Zhilei Liu", "Guoxian Song", "Jianfei Cai", "Tat-Jen Cham", "Juyong Zhang"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1802.07421v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.07421v2", "num_discussion": 0, "originally_published_time": "2/21/2018", "pid": "1802.07421v2", "published_time": "3/15/2018", "rawpid": "1802.07421", "tags": ["cs.CV"], "title": "Conditional Adversarial Synthesis of 3D Facial Action Units"}, {"abstract": "We propose a simple extension to the ReLU-family of activation functions that\nallows them to shift the mean activation across a layer towards zero. Combined\nwith proper weight initialization, this alleviates the need for normalization\nlayers. We explore the training of deep vanilla recurrent neural networks\n(RNNs) with up to 144 layers, and show that bipolar activation functions help\nlearning in this setting. On the Penn Treebank and Text8 language modeling\ntasks we obtain competitive results, improving on the best reported results for\nnon-gated networks. In experiments with convolutional neural networks without\nbatch normalization, we find that bipolar activations produce a faster drop in\ntraining error, and results in a lower test error on the CIFAR-10\nclassification task.", "authors": ["Lars Eidnes", "Arild N\u00f8kland"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1709.04054v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.04054v3", "num_discussion": 0, "originally_published_time": "9/12/2017", "pid": "1709.04054v3", "published_time": "3/15/2018", "rawpid": "1709.04054", "tags": ["stat.ML", "cs.LG", "cs.NE"], "title": "Shifting Mean Activation Towards Zero with Bipolar Activation Functions"}, {"abstract": "The past few years have witnessed great success in applying deep learning to\nenhance the quality of compressed image/video. The existing approaches mainly\nfocus on enhancing the quality of a single frame, ignoring the similarity\nbetween consecutive frames. In this paper, we investigate that heavy quality\nfluctuation exists across compressed video frames, and thus low quality frames\ncan be enhanced using the neighboring high quality frames, seen as Multi-Frame\nQuality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach\nfor compressed video, as a first attempt in this direction. In our approach, we\nfirstly develop a Support Vector Machine (SVM) based detector to locate Peak\nQuality Frames (PQFs) in compressed video. Then, a novel Multi-Frame\nConvolutional Neural Network (MF-CNN) is designed to enhance the quality of\ncompressed video, in which the non-PQF and its nearest two PQFs are as the\ninput. The MF-CNN compensates motion between the non-PQF and PQFs through the\nMotion Compensation subnet (MC-subnet). Subsequently, the Quality Enhancement\nsubnet (QE-subnet) reduces compression artifacts of the non-PQF with the help\nof its nearest PQFs. Finally, the experiments validate the effectiveness and\ngenerality of our MFQE approach in advancing the state-of-the-art quality\nenhancement of compressed video. The code of our MFQE approach is available at\nhttps://github.com/ryangBUAA/MFQE.git.", "authors": ["Ren Yang", "Mai Xu", "Zulin Wang", "Tianyi Li"], "category": "cs.CV", "comment": "to appear in CVPR 2018", "img": "/static/thumbs/1803.04680v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04680v3", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.04680v3", "published_time": "3/15/2018", "rawpid": "1803.04680", "tags": ["cs.CV", "cs.MM"], "title": "Multi-Frame Quality Enhancement for Compressed Video"}, {"abstract": "Machine translation has made rapid advances in recent years. Millions of\npeople are using it today in online translation systems and mobile applications\nin order to communicate across language barriers. The question naturally arises\nwhether such systems can approach or achieve parity with human translations. In\nthis paper, we first address the problem of how to define and accurately\nmeasure human parity in translation. We then describe Microsoft\u0027s machine\ntranslation system and measure the quality of its translations on the widely\nused WMT 2017 news translation task from Chinese to English. We find that our\nlatest neural machine translation system has reached a new state-of-the-art,\nand that the translation quality is at human parity when compared to\nprofessional human translations. We also find that it significantly exceeds the\nquality of crowd-sourced non-professional translations.", "authors": ["Hany Hassan", "Anthony Aue", "Chang Chen", "Vishal Chowdhary", "Jonathan Clark", "Christian Federmann", "Xuedong Huang", "Marcin Junczys-Dowmunt", "William Lewis", "Mu Li", "Shujie Liu", "Tie-Yan Liu", "Renqian Luo", "Arul Menezes", "Tao Qin", "Frank Seide", "Xu Tan", "Fei Tian", "Lijun Wu", "Shuangzhi Wu", "Yingce Xia", "Dongdong Zhang", "Zhirui Zhang", "Ming Zhou"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1803.05567v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05567v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05567v1", "published_time": "3/15/2018", "rawpid": "1803.05567", "tags": ["cs.CL"], "title": "Achieving Human Parity on Automatic Chinese to English News Translation"}, {"abstract": "The acoustic-to-word model based on the connectionist temporal classification\n(CTC) criterion was shown as a natural end-to-end (E2E) model directly\ntargeting words as output units. However, the word-based CTC model suffers from\nthe out-of-vocabulary (OOV) issue as it can only model limited number of words\nin the output layer and maps all the remaining words into an OOV output node.\nHence, such a word-based CTC model can only recognize the frequent words\nmodeled by the network output nodes. Our first attempt to improve the\nacoustic-to-word model is a hybrid CTC model which consults a letter-based CTC\nwhen the word-based CTC model emits OOV tokens during testing time. Then, we\npropose a much better solution by training a mixed-unit CTC model which\ndecomposes all the OOV words into sequences of frequent words and multi-letter\nunits. Evaluated on a 3400 hours Microsoft Cortana voice assistant task, the\nfinal acoustic-to-word solution improves the baseline word-based CTC by\nrelative 12.09% word error rate (WER) reduction when combined with our proposed\nattention CTC. Such an E2E model without using any language model (LM) or\ncomplex decoder outperforms the traditional context-dependent phoneme CTC which\nhas strong LM and decoder by relative 6.79%.", "authors": ["Jinyu Li", "Guoli Ye", "Amit Das", "Rui Zhao", "Yifan Gong"], "category": "cs.CL", "comment": "Accepted at ICASSP 2018", "img": "/static/thumbs/1803.05566v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05566v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05566v1", "published_time": "3/15/2018", "rawpid": "1803.05566", "tags": ["cs.CL"], "title": "Advancing Acoustic-to-Word CTC Model"}, {"abstract": "Conditional Generative Adversarial Networks (cGANs) are generative models\nthat can produce data samples ($x$) conditioned on both latent variables ($z$)\nand known auxiliary information ($c$). We propose the Bidirectional cGAN\n(BiCoGAN), which effectively disentangles $z$ and $c$ in the generation process\nand provides an encoder that learns inverse mappings from $x$ to both $z$ and\n$c$, trained jointly with the generator and the discriminator. We present\ncrucial techniques for training BiCoGANs, which involve an extrinsic factor\nloss along with an associated dynamically-tuned importance weight. As compared\nto other encoder-based cGANs, BiCoGANs encode $c$ more accurately, and utilize\n$z$ and $c$ more effectively and in a more disentangled way to generate\nsamples.", "authors": ["Ayush Jaiswal", "Wael AbdAlmageed", "Yue Wu", "Premkumar Natarajan"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1711.07461v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.07461v2", "num_discussion": 0, "originally_published_time": "11/20/2017", "pid": "1711.07461v2", "published_time": "3/15/2018", "rawpid": "1711.07461", "tags": ["cs.LG", "stat.ML"], "title": "Bidirectional Conditional Generative Adversarial Networks"}, {"abstract": "In this study, we propose advancing all-neural speech recognition by directly\nincorporating attention modeling within the Connectionist Temporal\nClassification (CTC) framework. In particular, we derive new context vectors\nusing time convolution features to model attention as part of the CTC network.\nTo further improve attention modeling, we utilize content information extracted\nfrom a network representing an implicit language model. Finally, we introduce\nvector based attention weights that are applied on context vectors across both\ntime and their individual components. We evaluate our system on a 3400 hours\nMicrosoft Cortana voice assistant task and demonstrate that our proposed model\nconsistently outperforms the baseline model achieving about 20% relative\nreduction in word error rates.", "authors": ["Amit Das", "Jinyu Li", "Rui Zhao", "Yifan Gong"], "category": "cs.CL", "comment": "Accepted at ICASSP 2018", "img": "/static/thumbs/1803.05563v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05563v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05563v1", "published_time": "3/15/2018", "rawpid": "1803.05563", "tags": ["cs.CL"], "title": "Advancing Connectionist Temporal Classification With Attention Modeling"}, {"abstract": "Water balance models (WBMs) are often employed to understand regional\nhydrologic cycles over various time scales. Most WBMs, however, are\nphysically-based, and few employ state-of-the-art statistical methods to\nreconcile independent input measurement uncertainty and bias. Further, few WBMs\nexist for large lakes, and most large lake WBMs perform additive accounting,\nwith minimal consideration towards input data uncertainty. Here, we introduce a\nframework for improving a previously developed large lake statistical water\nbalance model (L2SWBM). Focusing on the water balances of Lakes Superior and\nMichigan-Huron, we demonstrate our new analytical framework, identifying\nL2SWBMs from 26 alternatives that adequately close the water balance of the\nlakes with satisfactory computation times compared with the prototype model. We\nexpect our new framework will be used to develop water balance models for other\nlakes around the world.", "authors": ["Joeseph P. Smith", "Andrew D. Gronewold"], "category": "stat.AP", "comment": "Revision for Journal of Applied Statistics; 7 figures, 7 tables, 20\n  pages of text", "img": "/static/thumbs/1710.10161v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.10161v3", "num_discussion": 0, "originally_published_time": "10/26/2017", "pid": "1710.10161v3", "published_time": "3/15/2018", "rawpid": "1710.10161", "tags": ["stat.AP", "physics.data-an", "stat.ML"], "title": "Development and analysis of a Bayesian water balance model for large\n  lake systems"}, {"abstract": "Variational inference is an umbrella term for algorithms which cast Bayesian\ninference as optimization. Classically, variational inference uses the\nKullback-Leibler divergence to define the optimization. Though this divergence\nhas been widely used, the resultant posterior approximation can suffer from\nundesirable statistical properties. To address this, we reexamine variational\ninference from its roots as an optimization problem. We use operators, or\nfunctions of functions, to design variational objectives. As one example, we\ndesign a variational objective with a Langevin-Stein operator. We develop a\nblack box algorithm, operator variational inference (OPVI), for optimizing any\noperator objective. Importantly, operators enable us to make explicit the\nstatistical and computational tradeoffs for variational inference. We can\ncharacterize different properties of variational objectives, such as objectives\nthat admit data subsampling---allowing inference to scale to massive data---as\nwell as objectives that admit variational programs---a rich class of posterior\napproximations that does not require a tractable density. We illustrate the\nbenefits of OPVI on a mixture model and a generative model of images.", "authors": ["Rajesh Ranganath", "Jaan Altosaar", "Dustin Tran", "David M. Blei"], "category": "stat.ML", "comment": "Appears in Neural Information Processing Systems, 2016", "img": "/static/thumbs/1610.09033v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.09033v3", "num_discussion": 0, "originally_published_time": "10/27/2016", "pid": "1610.09033v3", "published_time": "3/15/2018", "rawpid": "1610.09033", "tags": ["stat.ML", "cs.LG", "stat.CO", "stat.ME"], "title": "Operator Variational Inference"}, {"abstract": "We introduce Spatial-Temporal Memory Networks (STMN) for object detection in\nvideos. At its core, we propose a novel Spatial-Temporal Memory module (STMM)\nas the recurrent computation unit to model long-term temporal appearance and\nmotion dynamics. The STMM\u0027s design enables the full integration of pretrained\nbackbone CNN weights, which we find to be critical for accurate detection.\nFurthermore, in order to tackle object motion in videos, we propose a novel\nMatchTrans module to align the spatial-temporal memory from frame to frame. Our\nmethod produces state-of-the-art results on the benchmark ImageNet VID dataset,\nand our ablative studies clearly demonstrate the contribution of our different\ndesign choices. We will publicly-share our code and models.", "authors": ["Fanyi Xiao", "Yong Jae Lee"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1712.06317v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1712.06317v2", "num_discussion": 0, "originally_published_time": "12/18/2017", "pid": "1712.06317v2", "published_time": "3/15/2018", "rawpid": "1712.06317", "tags": ["cs.CV"], "title": "Object Detection with an Aligned Spatial-Temporal Memory"}, {"abstract": "Learning a Bayesian network (BN) from data can be useful for decision-making\nor discovering causal relationships. However, traditional methods often fail in\nmodern applications, which exhibit a larger number of observed variables than\ndata points. The resulting uncertainty about the underlying network as well as\nthe desire to incorporate prior information recommend a Bayesian approach to\nlearning the BN, but the highly combinatorial structure of BNs poses a striking\nchallenge for inference. The current state-of-the-art methods such as order\nMCMC are faster than previous methods but prevent the use of many natural\nstructural priors and still have running time exponential in the maximum\nindegree of the true directed acyclic graph (DAG) of the BN. We here propose an\nalternative posterior approximation based on the observation that, if we\nincorporate empirical conditional independence tests, we can focus on a\nhigh-probability DAG associated with each order of the vertices. We show that\nour method allows the desired flexibility in prior specification, removes\ntiming dependence on the maximum indegree and yields provably good posterior\napproximations; in addition, we show that it achieves superior accuracy,\nscalability, and sampler mixing on several datasets.", "authors": ["Raj Agrawal", "Tamara Broderick", "Caroline Uhler"], "category": "stat.CO", "comment": "", "img": "/static/thumbs/1803.05554v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05554v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05554v1", "published_time": "3/15/2018", "rawpid": "1803.05554", "tags": ["stat.CO", "cs.LG", "stat.ME", "stat.ML"], "title": "Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models"}, {"abstract": "State-of-the-art systems for semantic image segmentation use feed-forward\npipelines with fixed computational costs. Building an image segmentation system\nthat works across a range of computational budgets is challenging and\ntime-intensive as new architectures must be designed and trained for every\ncomputational setting. To address this problem we develop a recurrent neural\nnetwork that successively improves prediction quality with each iteration.\nImportantly, the RNN may be deployed across a range of computational budgets by\nmerely running the model for a variable number of iterations. We find that this\narchitecture is uniquely suited for efficiently segmenting videos. By\nexploiting the segmentation of past frames, the RNN can perform video\nsegmentation at similar quality but reduced computational cost compared to\nstate-of-the-art image segmentation methods. When applied to static images in\nthe PASCAL VOC 2012 and Cityscapes segmentation datasets, the RNN traces out a\nspeed-accuracy curve that saturates near the performance of state-of-the-art\nsegmentation methods.", "authors": ["Lane McIntosh", "Niru Maheswaranathan", "David Sussillo", "Jonathon Shlens"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1711.10151v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.10151v2", "num_discussion": 0, "originally_published_time": "11/28/2017", "pid": "1711.10151v2", "published_time": "3/15/2018", "rawpid": "1711.10151", "tags": ["cs.CV"], "title": "Recurrent Segmentation for Variable Computational Budgets"}, {"abstract": "We propose a Spatiotemporal Sampling Network (STSN) that uses deformable\nconvolutions across time for object detection in videos. Our STSN performs\nobject detection in a video frame by learning to spatially sample features from\nthe adjacent frames. This naturally renders the approach robust to occlusion or\nmotion blur in individual frames. Our framework does not require additional\nsupervision, as it optimizes sampling locations directly with respect to object\ndetection performance. Our STSN outperforms the state-of-the-art on the\nImageNet VID dataset and compared to prior video object detection methods it\nuses a simpler design, and does not require optical flow data for training. We\nalso show that after training STSN on videos, we can adapt it for object\ndetection in images, by adding and training a single deformable convolutional\nlayer on still-image data. This leads to improvements in accuracy compared to\ntraditional object detection in images.", "authors": ["Gedas Bertasius", "Lorenzo Torresani", "Jianbo Shi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05549v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05549v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05549v1", "published_time": "3/15/2018", "rawpid": "1803.05549", "tags": ["cs.CV"], "title": "Object Detection in Video with Spatiotemporal Sampling Networks"}, {"abstract": "In the Story Cloze Test, a system is presented with a 4-sentence prompt to a\nstory, and must determine which one of two potential endings is the \u0027right\u0027\nending to the story. Previous work has shown that ignoring the training set and\ntraining a model on the validation set can achieve high accuracy on this task\ndue to stylistic differences between the story endings in the training set and\nvalidation and test sets. Following this approach, we present a simpler\nfully-neural approach to the Story Cloze Test using skip-thought embeddings of\nthe stories in a feed-forward network that achieves close to state-of-the-art\nperformance on this task without any feature engineering. We also find that\nconsidering just the last sentence of the prompt instead of the whole prompt\nyields higher accuracy with our approach.", "authors": ["Siddarth Srinivasan", "Richa Arora", "Mark Riedl"], "category": "cs.CL", "comment": "6 pages", "img": "/static/thumbs/1803.05547v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05547v1", "num_discussion": 0, "originally_published_time": "3/15/2018", "pid": "1803.05547v1", "published_time": "3/15/2018", "rawpid": "1803.05547", "tags": ["cs.CL"], "title": "A Simple and Effective Approach to the Story Cloze Test"}, {"abstract": "Mixed Reality (MR) is a powerful interactive technology that yields new types\nof user experience. We present a semantic based interactive MR framework that\nexceeds the current geometry level approaches, a step change in generating\nhigh-level context-aware interactions. Our key insight is to build semantic\nunderstanding in MR that not only can greatly enhance user experience through\nobject-specific behaviours, but also pave the way for solving complex\ninteraction design challenges. The framework generates semantic properties of\nthe real world environment through dense scene reconstruction and deep image\nunderstanding. We demonstrate our approach with a material-aware prototype\nsystem for generating context-aware physical interactions between the real and\nthe virtual objects. Quantitative and qualitative evaluations are carried out\nand the results show that the framework delivers accurate and fast semantic\ninformation in interactive MR environment, providing effective semantic level\ninteractions.", "authors": ["Long Chen", "Wen Tang", "Nigel John", "Tao Ruan Wan", "Jian Jun Zhang"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05541v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05541v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05541v1", "published_time": "3/14/2018", "rawpid": "1803.05541", "tags": ["cs.CV"], "title": "Context-Aware Mixed Reality: A Framework for Ubiquitous Interaction"}, {"abstract": "Maintaining natural image statistics is a crucial factor in restoration and\ngeneration of realistic looking images. When training CNNs, photorealism is\nusually attempted by adversarial training (GAN), that pushes the output images\nto lie on the manifold of natural images. GANs are very powerful, but not\nperfect. They are hard to train and the results still often suffer from\nartifacts. In this paper we propose a complementary approach, whose goal is to\ntrain a feed-forward CNN to maintain natural internal statistics. We look\nexplicitly at the distribution of features in an image and train the network to\ngenerate images with natural feature distributions. Our approach reduces by\norders of magnitude the number of images required for training and achieves\nstate-of-the-art results on both single-image super-resolution, and\nhigh-resolution surface normal estimation.", "authors": ["Roey Mechrez", "Itamar Talmi", "Firas Shama", "Lihi Zelnik-Manor"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.04626v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04626v2", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.04626v2", "published_time": "3/14/2018", "rawpid": "1803.04626", "tags": ["cs.CV"], "title": "Learning to Maintain Natural Image Statistics"}, {"abstract": "This paper investigates the evaluation of dense 3D face reconstruction from a\nsingle 2D image in the wild. To this end, we organise a competition that\nprovides a new benchmark dataset that contains 2000 2D facial images of 135\nsubjects as well as their 3D ground truth face scans. In contrast to previous\ncompetitions or challenges, the aim of this new benchmark dataset is to\nevaluate the accuracy of a 3D dense face reconstruction algorithm using real,\naccurate and high-resolution 3D ground truth face scans. In addition to the\ndataset, we provide a standard protocol as well as a Python script for the\nevaluation. Last, we report the results obtained by three state-of-the-art 3D\nface reconstruction systems on the new benchmark dataset. The competition is\norganised along with the 2018 13th IEEE Conference on Automatic Face \u0026 Gesture\nRecognition.", "authors": ["Zhen-Hua Feng", "Patrik Huber", "Josef Kittler", "Peter Hancock", "Xiao-Jun Wu", "Qijun Zhao", "Paul Koppen", "Matthias R\u00e4tsch"], "category": "cs.CV", "comment": "7 pages", "img": "/static/thumbs/1803.05536v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05536v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05536v1", "published_time": "3/14/2018", "rawpid": "1803.05536", "tags": ["cs.CV"], "title": "Evaluation of Dense 3D Reconstruction from 2D Face Images in the Wild"}, {"abstract": "Convolutional Neural Networks (CNNs) need large amounts of data with ground\ntruth annotation, which is a challenging problem that has limited the\ndevelopment and fast deployment of CNNs for many computer vision tasks. We\npropose a novel framework for depth estimation from monocular images with\ncorresponding confidence in a self-supervised manner. A fully differential\npatch-based cost function is proposed by using the Zero-Mean Normalized Cross\nCorrelation (ZNCC) that takes multi-scale patches as a matching strategy. This\napproach greatly increases the accuracy and robustness of the depth learning.\nIn addition, the proposed patch-based cost function can provide a 0 to 1\nconfidence, which is then used to supervise the training of a parallel network\nfor confidence map learning and estimation. Evaluation on KITTI dataset shows\nthat our method outperforms the state-of-the-art results.", "authors": ["Long Chen", "Wen Tang", "Nigel John"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05530v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05530v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05530v1", "published_time": "3/14/2018", "rawpid": "1803.05530", "tags": ["cs.CV"], "title": "Self-Supervised Monocular Image Depth Learning and Confidence Estimation"}, {"abstract": "Image captioning is a multimodal task involving computer vision and natural\nlanguage processing, where the goal is to learn a mapping from the image to its\nnatural language description. In general, the mapping function is learned from\na training set of image-caption pairs. However, for some language, large scale\nimage-caption paired corpus might not be available. We present an approach to\nthis unpaired image captioning problem by language pivoting. Our method can\neffectively capture the characteristics of an image captioner from the pivot\nlanguage (Chinese) and align it to the target language (English) using another\npivot-target (Chinese-English) parallel corpus. We evaluate our method on two\nimage-to-English benchmark datasets: MSCOCO and Flickr30K. Quantitative\ncomparisons against several baseline approaches demonstrate the effectiveness\nof our method.", "authors": ["Jiuxiang Gu", "Shafiq Joty", "Jianfei Cai", "Gang Wang"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05526v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05526v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05526v1", "published_time": "3/14/2018", "rawpid": "1803.05526", "tags": ["cs.CV"], "title": "Unpaired Image Captioning by Language Pivoting"}, {"abstract": "In this work, we propose a novel robot learning framework called Neural Task\nProgramming (NTP), which bridges the idea of few-shot learning from\ndemonstration and neural program induction. NTP takes as input a task\nspecification (e.g., video demonstration of a task) and recursively decomposes\nit into finer sub-task specifications. These specifications are fed to a\nhierarchical neural program, where bottom-level programs are callable\nsubroutines that interact with the environment. We validate our method in three\nrobot manipulation tasks. NTP achieves strong generalization across sequential\ntasks that exhibit hierarchal and compositional structures. The experimental\nresults show that NTP learns to generalize well to- wards unseen tasks with\nincreasing lengths, variable topologies, and changing objectives.", "authors": ["Danfei Xu", "Suraj Nair", "Yuke Zhu", "Julian Gao", "Animesh Garg", "Li Fei-Fei", "Silvio Savarese"], "category": "cs.AI", "comment": "ICRA 2018", "img": "/static/thumbs/1710.01813v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.01813v2", "num_discussion": 0, "originally_published_time": "10/4/2017", "pid": "1710.01813v2", "published_time": "3/14/2018", "rawpid": "1710.01813", "tags": ["cs.AI", "cs.LG", "cs.RO"], "title": "Neural Task Programming: Learning to Generalize Across Hierarchical\n  Tasks"}, {"abstract": "The availability of a large amount of electronic health records (EHR)\nprovides huge opportunities to improve health care service by mining these\ndata. One important application is clinical endpoint prediction, which aims to\npredict whether a disease, a symptom or an abnormal lab test will happen in the\nfuture according to patients\u0027 history records. This paper develops deep\nlearning techniques for clinical endpoint prediction, which are effective in\nmany practical applications. However, the problem is very challenging since\npatients\u0027 history records contain multiple heterogeneous temporal events such\nas lab tests, diagnosis, and drug administrations. The visiting patterns of\ndifferent types of events vary significantly, and there exist complex nonlinear\nrelationships between different events. In this paper, we propose a novel model\nfor learning the joint representation of heterogeneous temporal events. The\nmodel adds a new gate to control the visiting rates of different events which\neffectively models the irregular patterns of different events and their\nnonlinear correlations. Experiment results with real-world clinical data on the\ntasks of predicting death and abnormal lab tests prove the effectiveness of our\nproposed approach over competitive baselines.", "authors": ["Luchen Liu", "Jianhao Shen", "Ming Zhang", "Zichang Wang", "Jian Tang"], "category": "cs.AI", "comment": "8 pages, this paper has been accepted by AAAI 2018", "img": "/static/thumbs/1803.04837v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04837v2", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.04837v2", "published_time": "3/14/2018", "rawpid": "1803.04837", "tags": ["cs.AI", "cs.LG", "stat.ML"], "title": "Learning the Joint Representation of Heterogeneous Temporal Events for\n  Clinical Endpoint Prediction"}, {"abstract": "Reinforcement learning algorithms can train agents that solve problems in\ncomplex, interesting environments. Normally, the complexity of the trained\nagent is closely related to the complexity of the environment. This suggests\nthat a highly capable agent requires a complex environment for training. In\nthis paper, we point out that a competitive multi-agent environment trained\nwith self-play can produce behaviors that are far more complex than the\nenvironment itself. We also point out that such environments come with a\nnatural curriculum, because for any skill level, an environment full of agents\nof this level will have the right level of difficulty. This work introduces\nseveral competitive multi-agent environments where agents compete in a 3D world\nwith simulated physics. The trained agents learn a wide variety of complex and\ninteresting skills, even though the environment themselves are relatively\nsimple. The skills include behaviors such as running, blocking, ducking,\ntackling, fooling opponents, kicking, and defending using both arms and legs. A\nhighlight of the learned behaviors can be found here: https://goo.gl/eR7fbX", "authors": ["Trapit Bansal", "Jakub Pachocki", "Szymon Sidor", "Ilya Sutskever", "Igor Mordatch"], "category": "cs.AI", "comment": "Published as a conference paper at ICLR 2018", "img": "/static/thumbs/1710.03748v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.03748v3", "num_discussion": 0, "originally_published_time": "10/10/2017", "pid": "1710.03748v3", "published_time": "3/14/2018", "rawpid": "1710.03748", "tags": ["cs.AI"], "title": "Emergent Complexity via Multi-Agent Competition"}, {"abstract": "In this paper, we aim to improve the state-of-the-art video generative\nadversarial networks (GANs) with a view towards multi-functional applications.\nOur improved video GAN model does not separate foreground from background nor\ndynamic from static patterns, but learns to generate the entire video clip\nconjointly. Our model can thus be trained to generate - and learn from - a\nbroad set of videos with no restriction. This is achieved by designing a robust\none-stream video generation architecture with an extension of the\nstate-of-the-art Wasserstein GAN framework that allows for better convergence.\nThe experimental results show that our improved video GAN model outperforms\nstate-of-theart video generative models on multiple challenging datasets.\nFurthermore, we demonstrate the superiority of our model by successfully\nextending it to three challenging problems: video colorization, video\ninpainting, and future prediction. To the best of our knowledge, this is the\nfirst work using GANs to colorize and inpaint video clips.", "authors": ["Bernhard Kratzwald", "Zhiwu Huang", "Danda Pani Paudel", "Acharya Dinesh", "Luc Van Gool"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1711.11453v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.11453v2", "num_discussion": 0, "originally_published_time": "11/30/2017", "pid": "1711.11453v2", "published_time": "3/14/2018", "rawpid": "1711.11453", "tags": ["cs.CV"], "title": "Improving Video Generation for Multi-functional Applications"}, {"abstract": "In this study we approach the problem of distinguishing general profanity\nfrom hate speech in social media, something which has not been widely\nconsidered. Using a new dataset annotated specifically for this task, we employ\nsupervised classification along with a set of features that includes n-grams,\nskip-grams and clustering-based word representations. We apply approaches based\non single classifiers as well as more advanced ensemble classifiers and stacked\ngeneralization, achieving the best result of 80% accuracy for this 3-class\nclassification task. Analysis of the results reveals that discriminating hate\nspeech and profanity is not a simple task, which may require features that\ncapture a deeper understanding of the text not always possible with surface\nn-grams. The variability of gold labels in the annotated data, due to\ndifferences in the subjective adjudications of the annotators, is also an\nissue. Other directions for future work are discussed.", "authors": ["Shervin Malmasi", "Marcos Zampieri"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1803.05495v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05495v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05495v1", "published_time": "3/14/2018", "rawpid": "1803.05495", "tags": ["cs.CL"], "title": "Challenges in Discriminating Profanity from Hate Speech"}, {"abstract": "In this paper, we propose a simple and effective way to improve one-look\nregression models for object counting from images. We use class activation map\nvisualizations to illustrate the drawbacks of learning a pure one-look\nregression model for a counting task. Based on these insights, we enhance\none-look regression counting models by regulating activation maps from the\nfinal convolution layer of the network with coarse ground-truth activation maps\ngenerated from simple dot annotations. We call this strategy heatmap regulation\n(HR). We show that this simple enhancement effectively suppresses false\ndetections generated by the corresponding one-look baseline model and also\nimproves the performance in terms of false negatives. Evaluations are performed\non four different counting datasets --- two for car counting (CARPK, PUCPR+),\none for crowd counting (WorldExpo) and another for biological cell counting\n(VGG-Cells). Adding HR to a simple VGG front-end improves performance on all\nthese benchmarks compared to a simple one-look baseline model and results in\nstate-of-the-art performance for car counting.", "authors": ["Shubhra Aich", "Ian Stavness"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05494v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05494v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05494v1", "published_time": "3/14/2018", "rawpid": "1803.05494", "tags": ["cs.CV"], "title": "Improving Object Counting with Heatmap Regulation"}, {"abstract": "Existing Markov Chain Monte Carlo (MCMC) methods are either based on\ngeneral-purpose and domain-agnostic schemes which can lead to slow convergence,\nor hand-crafting of problem-specific proposals by an expert. We propose\nA-NICE-MC, a novel method to train flexible parametric Markov chain kernels to\nproduce samples with desired properties. First, we propose an efficient\nlikelihood-free adversarial training method to train a Markov chain and mimic a\ngiven data distribution. Then, we leverage flexible volume preserving flows to\nobtain parametric kernels for MCMC. Using a bootstrap approach, we show how to\ntrain efficient Markov chains to sample from a prescribed posterior\ndistribution by iteratively improving the quality of both the model and the\nsamples. A-NICE-MC provides the first framework to automatically design\nefficient domain-specific MCMC proposals. Empirical results demonstrate that\nA-NICE-MC combines the strong guarantees of MCMC with the expressiveness of\ndeep neural networks, and is able to significantly outperform competing methods\nsuch as Hamiltonian Monte Carlo.", "authors": ["Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "category": "stat.ML", "comment": "NIPS 2017", "img": "/static/thumbs/1706.07561v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.07561v3", "num_discussion": 0, "originally_published_time": "6/23/2017", "pid": "1706.07561v3", "published_time": "3/14/2018", "rawpid": "1706.07561", "tags": ["stat.ML", "cs.AI", "cs.LG"], "title": "A-NICE-MC: Adversarial Training for MCMC"}, {"abstract": "We present a research study aimed at testing of applicability of machine\nlearning techniques for prediction of permeability of digitized rock samples.\nWe prepare a training set containing 3D images of sandstone samples imaged with\nX-ray microtomography and corresponding permeability values simulated with Pore\nNetwork approach. We also use Minkowski functionals and Deep Learning-based\ndescriptors of 3D images and 2D slices as input features for predictive model\ntraining and prediction. We compare predictive power of various feature sets\nand methods. The later include Gradient Boosting and various architectures of\nDeep Neural Networks (DNN). The results demonstrate applicability of machine\nlearning for image-based permeability prediction and open a new area of Digital\nRock research.", "authors": ["Oleg Sudakov", "Evgeny Burnaev", "Dmitry Koroteev"], "category": "physics.geo-ph", "comment": "21 pages, 7 figures", "img": "/static/thumbs/1803.00758v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.00758v2", "num_discussion": 0, "originally_published_time": "3/2/2018", "pid": "1803.00758v2", "published_time": "3/14/2018", "rawpid": "1803.00758", "tags": ["physics.geo-ph", "cs.CV", "physics.comp-ph"], "title": "Driving Digital Rock towards Machine Learning: predicting permeability\n  with Gradient Boosting and Deep Neural Networks"}, {"abstract": "Recent developments in the remote sensing systems and image processing made\nit possible to propose a new method of the object classification and detection\nof the specific changes in the series of satellite Earth images (so called\ntargeted change detection). In this paper we propose a formal problem statement\nthat allows to use effectively the deep learning approach to analyze\ntime-dependent series of remote sensing images. We also introduce a new\nframework for the development of deep learning models for targeted change\ndetection and demonstrate some cases of business applications it can be used\nfor.", "authors": ["Vladimir Ignatiev", "Alexey Trekin", "Viktor Lobachev", "Georgy Potapov", "Evgeny Burnaev"], "category": "cs.CV", "comment": "10 pages, 1 figures, 1 table", "img": "/static/thumbs/1803.05482v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05482v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05482v1", "published_time": "3/14/2018", "rawpid": "1803.05482", "tags": ["cs.CV", "cs.CE", "eess.IV"], "title": "Targeted change detection in remote sensing images"}, {"abstract": "This paper presents a new method, which we call SUSTain, that extends\nreal-valued matrix and tensor factorizations to data where values are integers.\nSuch data are common when the values correspond to event counts or ordinal\nmeasures. The conventional approach is to treat integer data as real, and then\napply real-valued factorizations. However, doing so fails to preserve important\ncharacteristics of the original data, thereby making it hard to interpret the\nresults. Instead, our approach extracts factor values from integer datasets as\nscores that are constrained to take values from a small integer set. These\nscores are easy to interpret: a score of zero indicates no feature contribution\nand higher scores indicate distinct levels of feature importance.\n  At its core, SUSTain relies on: a) a problem partitioning into\ninteger-constrained subproblems, so that they can be optimally solved in an\nefficient manner; and b) organizing the order of the subproblems\u0027 solution, to\npromote reuse of shared intermediate results. We propose two variants,\nSUSTain_M and SUSTain_T, to handle both matrix and tensor inputs, respectively.\nWe evaluate SUSTain against several state-of-the-art baselines on both\nsynthetic and real Electronic Health Record (EHR) datasets. Comparing to those\nbaselines, SUSTain shows either significantly better fit or orders of magnitude\nspeedups that achieve a comparable fit (up to 425X faster). We apply SUSTain to\nEHR datasets to extract patient phenotypes (i.e., clinically meaningful patient\nclusters). Furthermore, 87% of them were validated as clinically meaningful\nphenotypes related to heart failure by a cardiologist.", "authors": ["Ioakeim Perros", "Evangelos E. Papalexakis", "Haesun Park", "Richard Vuduc", "Xiaowei Yan", "Christopher Defilippi", "Walter F. Stewart", "Jimeng Sun"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1803.05473v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05473v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05473v1", "published_time": "3/14/2018", "rawpid": "1803.05473", "tags": ["cs.LG"], "title": "SUSTain: Scalable Unsupervised Scoring for Tensors and its Application\n  to Phenotyping"}, {"abstract": "Aim: Early detection and correct diagnosis of lung cancer are the most\nimportant steps in improving patient outcome. This study aims to assess which\ndeep learning models perform best in lung cancer diagnosis. Methods: Non-small\ncell lung carcinoma and small cell lung carcinoma biopsy specimens were\nconsecutively obtained and stained. The specimen slides were diagnosed by two\nexperienced pathologists (over 20 years). Several deep learning models were\ntrained to discriminate cancer and non-cancer biopsies. Result: Deep learning\nmodels give reasonable AUC from 0.8810 to 0.9119. Conclusion: The deep learning\nanalysis could help to speed up the detection process for the whole-slide image\n(WSI) and keep the comparable detection rate with human observer.", "authors": ["Zhang Li", "Zheyu Hu", "Jiaolong Xu", "Tao Tan", "Hui Chen", "Zhi Duan", "Ping Liu", "Jun Tang", "Guoping Cai", "Quchang Ouyang", "Yuling Tang", "Geert Litjens", "Qiang Li"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05471v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05471v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05471v1", "published_time": "3/14/2018", "rawpid": "1803.05471", "tags": ["cs.CV"], "title": "Computer-aided diagnosis of lung carcinoma using deep learning - a pilot\n  study"}, {"abstract": "We present a new question set, text corpus, and baselines assembled to\nencourage AI research in advanced question answering. Together, these\nconstitute the AI2 Reasoning Challenge (ARC), which requires far more powerful\nknowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC\nquestion set is partitioned into a Challenge Set and an Easy Set, where the\nChallenge Set contains only questions answered incorrectly by both a\nretrieval-based algorithm and a word co-occurence algorithm. The dataset\ncontains only natural, grade-school science questions (authored for human\ntests), and is the largest public-domain set of this kind (7,787 questions). We\ntest several baselines on the Challenge Set, including leading neural models\nfrom the SQuAD and SNLI tasks, and find that none are able to significantly\noutperform a random baseline, reflecting the difficult nature of this task. We\nare also releasing the ARC Corpus, a corpus of 14M science sentences relevant\nto the task, and implementations of the three neural baseline models tested.\nCan your model perform better? We pose ARC as a challenge to the community.", "authors": ["Peter Clark", "Isaac Cowhey", "Oren Etzioni", "Tushar Khot", "Ashish Sabharwal", "Carissa Schoenick", "Oyvind Tafjord"], "category": "cs.AI", "comment": "10 pages, 7 tables, 2 figures", "img": "/static/thumbs/1803.05457v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05457v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05457v1", "published_time": "3/14/2018", "rawpid": "1803.05457", "tags": ["cs.AI", "cs.CL", "cs.IR"], "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\n  Challenge"}, {"abstract": "We introduce SentEval, a toolkit for evaluating the quality of universal\nsentence representations. SentEval encompasses a variety of tasks, including\nbinary and multi-class classification, natural language inference and sentence\nsimilarity. The set of tasks was selected based on what appears to be the\ncommunity consensus regarding the appropriate evaluations for universal\nsentence representations. The toolkit comes with scripts to download and\npreprocess datasets, and an easy interface to evaluate sentence encoders. The\naim is to provide a fairer, less cumbersome and more centralized way for\nevaluating sentence representations.", "authors": ["Alexis Conneau", "Douwe Kiela"], "category": "cs.CL", "comment": "LREC 2018", "img": "/static/thumbs/1803.05449v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05449v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05449v1", "published_time": "3/14/2018", "rawpid": "1803.05449", "tags": ["cs.CL"], "title": "SentEval: An Evaluation Toolkit for Universal Sentence Representations"}, {"abstract": "Deep Learning methods, specifically convolutional neural networks (CNNs),\nhave seen a lot of success in the domain of image-based data, where the data\noffers a clearly structured topology in the regular lattice of pixels. This\n4-neighbourhood topological simplicity makes the application of convolutional\nmasks straightforward for time series data, such as video applications, but\nmany high-dimensional time series data are not organised in regular lattices,\nand instead values may have adjacency relationships with non-trivial\ntopologies, such as small-world networks or trees. In our application case,\nhuman kinematics, it is currently unclear how to generalise convolutional\nkernels in a principled manner. Therefore we define and implement here a\nframework for general graph-structured CNNs for time series analysis. Our\nalgorithm automatically builds convolutional layers using the specified\nadjacency matrix of the data dimensions and convolutional masks that scale with\nthe hop distance. In the limit of a lattice-topology our method produces the\nwell-known image convolutional masks. We test our method first on synthetic\ndata of arbitrarily-connected graphs and human hand motion capture data, where\nthe hand is represented by a tree capturing the mechanical dependencies of the\njoints. We are able to demonstrate, amongst other things, that inclusion of the\ngraph structure of the data dimensions improves model prediction significantly,\nwhen compared against a benchmark CNN model with only time convolution layers.", "authors": ["Thomas Teh", "Chaiyawan Auepanwiriyakul", "John Alexander Harston", "A. Aldo Faisal"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1803.05419v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05419v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05419v1", "published_time": "3/14/2018", "rawpid": "1803.05419", "tags": ["stat.ML", "cs.LG"], "title": "Generalised Structural CNNs (SCNNs) for time series data with arbitrary\n  graph-toplogies"}, {"abstract": "Deep neural networks are typically trained by optimizing a loss function with\nan SGD variant, in conjunction with a decaying learning rate, until\nconvergence. We show that simple averaging of multiple points along the\ntrajectory of SGD, with a cyclical or constant learning rate, leads to better\ngeneralization than conventional training. We also show that this Stochastic\nWeight Averaging (SWA) procedure finds much broader optima than SGD, and\napproximates the recent Fast Geometric Ensembling (FGE) approach with a single\nmodel. Using SWA we achieve notable improvement in test accuracy over\nconventional SGD training on a range of state-of-the-art residual networks,\nPyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and\nImageNet. In short, SWA is extremely easy to implement, improves\ngeneralization, and has almost no computational overhead.", "authors": ["Pavel Izmailov", "Dmitrii Podoprikhin", "Timur Garipov", "Dmitry Vetrov", "Andrew Gordon Wilson"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1803.05407v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05407v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05407v1", "published_time": "3/14/2018", "rawpid": "1803.05407", "tags": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "title": "Averaging Weights Leads to Wider Optima and Better Generalization"}, {"abstract": "We construct near-optimal coresets for kernel density estimate for points in\n$\\mathbb{R^d}$ when the kernel is positive definite. Specifically we show a\npolynomial time construction for a coreset of size $O(\\sqrt{d\\log\n(1/\\epsilon)}/\\epsilon)$, and we show a near-matching lower bound of size\n$\\Omega(\\sqrt{d}/\\epsilon)$. The upper bound is a polynomial in $1/\\epsilon$\nimprovement when $d \\in [3,1/\\epsilon^2)$ (for all kernels except the Gaussian\nkernel which had a previous upper bound of $O((1/\\epsilon) \\log^d\n(1/\\epsilon))$) and the lower bound is the first known lower bound to depend on\n$d$ for this problem. Moreover, the upper bound restriction that the kernel is\npositive definite is significant in that it applies to a wide-variety of\nkernels, specifically those most important for machine learning. This includes\nkernels for information distances and the sinc kernel which can be negative.", "authors": ["Jeff M. Phillips", "Wai Ming Tai"], "category": "cs.LG", "comment": "to appear in SOCG 2018", "img": "/static/thumbs/1802.01751v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.01751v2", "num_discussion": 0, "originally_published_time": "2/6/2018", "pid": "1802.01751v2", "published_time": "3/14/2018", "rawpid": "1802.01751", "tags": ["cs.LG", "cs.CG", "stat.ML"], "title": "Near-Optimal Coresets of Kernel Density Estimates"}, {"abstract": "Traditional image recognition involves identifying the key object in a\nportrait-type image with a single object focus (ILSVRC, AlexNet, and VGG). More\nrecent approaches consider dense image recognition - segmenting an image with\nappropriate bounding boxes and performing image recognition within these\nbounding boxes (Semantic segmentation). The Visual Genome dataset [5] is an\nattempt to bridge these various approaches to a cohesive dataset for each\nsubtask - bounding box generation, image recognition, captioning, and a new\noperation: scene graph generation. Our focus is on using such scene graphs to\nperform graph search on image databases to holistically retrieve images based\non a search criteria. We develop a method to store scene graphs and metadata in\ngraph databases (using Neo4J) and to perform fast approximate retrieval of\nimages based on a graph search query. We process more complex queries than\nsingle object search, e.g. \"girl eating cake\" retrieves images that contain the\nspecified relation as well as variations.", "authors": ["Abhijit Suprem", "Polo Chau"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05401v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05401v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05401v1", "published_time": "3/14/2018", "rawpid": "1803.05401", "tags": ["cs.CV", "cs.IR"], "title": "Approximate Query Matching for Image Retrieval"}, {"abstract": "Over the last decade, the process of automatic colorization had been studied\nthoroughly due to its vast application such as colorization of grayscale images\nand restoration of aged and/or degraded images. This problem is highly\nill-posed due to the extremely large degrees of freedom during the assignment\nof color information. Many of the recent developments in automatic colorization\ninvolved images that contained a common theme throughout training, and/or\nrequired highly processed data such as semantic maps as input data. In our\napproach, we attempted to fully generalize this procedure using a conditional\nDeep Convolutional Generative Adversarial Network (DCGAN). The network is\ntrained over datasets that are publicly available such as CIFAR-10 and\nPlaces365. The results of the generative model and tradition deep neural\nnetworks are compared.", "authors": ["Kamyar Nazeri", "Eric Ng"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05400v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05400v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05400v1", "published_time": "3/14/2018", "rawpid": "1803.05400", "tags": ["cs.CV"], "title": "Image Colorization with Generative Adversarial Networks"}, {"abstract": "Performance of distributed optimization and learning systems is bottlenecked\nby \"straggler\" nodes and slow communication links, which significantly delay\ncomputation. We propose a distributed optimization framework where the dataset\nis \"encoded\" to have an over-complete representation with built-in redundancy,\nand the straggling nodes in the system are dynamically left out of the\ncomputation at every iteration, whose loss is compensated by the embedded\nredundancy. We show that oblivious application of several popular optimization\nalgorithms on encoded data, including gradient descent, L-BFGS, proximal\ngradient under data parallelism, and coordinate descent under model\nparallelism, converge to either approximate or exact solutions of the original\nproblem when stragglers are treated as erasures. These convergence results are\ndeterministic, i.e., they establish sample path convergence for arbitrary\nsequences of delay patterns or distributions on the nodes, and are independent\nof the tail behavior of the delay distribution. We demonstrate that equiangular\ntight frames have desirable properties as encoding matrices, and propose\nefficient mechanisms for encoding large-scale data. We implement the proposed\ntechnique on Amazon EC2 clusters, and demonstrate its performance over several\nlearning problems, including matrix factorization, LASSO, ridge regression and\nlogistic regression, and compare the proposed method with uncoded,\nasynchronous, and data replication strategies.", "authors": ["Can Karakus", "Yifan Sun", "Suhas Diggavi", "Wotao Yin"], "category": "stat.ML", "comment": "39 pages, 14 figures. Submitted for publication", "img": "/static/thumbs/1803.05397v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05397v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05397v1", "published_time": "3/14/2018", "rawpid": "1803.05397", "tags": ["stat.ML", "cs.DC", "cs.LG", "math.OC"], "title": "Redundancy Techniques for Straggler Mitigation in Distributed\n  Optimization and Learning"}, {"abstract": "Neural style transfer is an emerging technique which is able to endow\ndaily-life images with attractive artistic styles. Previous work has succeeded\nin applying convolutional neural network (CNN) to style transfer for monocular\nimages or videos. However, style transfer for stereoscopic images is still a\nmissing piece. Different from processing a monocular image, the two views of a\nstylized stereoscopic pair are required to be consistent to provide the\nobserver a comfortable visual experience. In this paper, we propose a dual path\nnetwork for view-consistent style transfer on stereoscopic images. While each\nview of the stereoscopic pair is processed in an individual path, a novel\nfeature aggregation strategy is proposed to effectively share information\nbetween the two paths. Besides a traditional perceptual loss used for\ncontrolling style transfer quality in each view, a multi-layer view loss is\nproposed to enforce the network to coordinate the learning of both paths to\ngenerate view-consistent stylized results. Extensive experiments show that,\ncompared with previous methods, the proposed model can generate stylized\nstereoscopic images which achieve the best view consistency.", "authors": ["Xinyu Gong", "Haozhi Huang", "Lin Ma", "Fumin Shen", "Wei Liu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1802.09985v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.09985v2", "num_discussion": 0, "originally_published_time": "2/27/2018", "pid": "1802.09985v2", "published_time": "3/14/2018", "rawpid": "1802.09985", "tags": ["cs.CV"], "title": "Neural Stereoscopic Image Style Transfer"}, {"abstract": "Large-scale deep neural networks are both memory intensive and\ncomputation-intensive, thereby posing stringent requirements on the computing\nplatforms. Hardware accelerations of deep neural networks have been extensively\ninvestigated in both industry and academia. Specific forms of binary neural\nnetworks (BNNs) and stochastic computing based neural networks (SCNNs) are\nparticularly appealing to hardware implementations since they can be\nimplemented almost entirely with binary operations. Despite the obvious\nadvantages in hardware implementation, these approximate computing techniques\nare questioned by researchers in terms of accuracy and universal applicability.\nAlso it is important to understand the relative pros and cons of SCNNs and BNNs\nin theory and in actual hardware implementations. In order to address these\nconcerns, in this paper we prove that the \"ideal\" SCNNs and BNNs satisfy the\nuniversal approximation property with probability 1 (due to the stochastic\nbehavior). The proof is conducted by first proving the property for SCNNs from\nthe strong law of large numbers, and then using SCNNs as a \"bridge\" to prove\nfor BNNs. Based on the universal approximation property, we further prove that\nSCNNs and BNNs exhibit the same energy complexity. In other words, they have\nthe same asymptotic energy consumption with the growing of network size. We\nalso provide a detailed analysis of the pros and cons of SCNNs and BNNs for\nhardware implementations and conclude that SCNNs are more suitable for\nhardware.", "authors": ["Yanzhi Wang", "Zheng Zhan", "Jiayu Li", "Jian Tang", "Bo Yuan", "Liang Zhao", "Wujie Wen", "Siyue Wang", "Xue Lin"], "category": "cs.LG", "comment": "9 pages, 4 figures", "img": "/static/thumbs/1803.05391v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05391v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05391v1", "published_time": "3/14/2018", "rawpid": "1803.05391", "tags": ["cs.LG", "stat.ML"], "title": "On the Universal Approximation Property and Equivalence of Stochastic\n  Computing-based Neural Networks and Binary Neural Networks"}, {"abstract": "Metric embeddings are immensely useful representation of interacting entities\nsuch as videos, users, search queries, online resources, words, and more.\nEmbeddings are computed by optimizing a loss function of the form of a sum over\nprovided associations so that relation of embedding vectors reflects strength\nof association. Moreover, the resulting embeddings allow us to predict the\nstrength of unobserved associations.\n  Typically, the optimization performs stochastic gradient updates on\nminibatches of associations that are arranged independently at random. We\npropose and study here the antithesis of {\\em coordinated} arrangements, which\nwe obtain efficiently through {\\em LSH microbatching}, where similar\nassociations are grouped together. Coordinated arrangements leverage the\nsimilarity of entities evident from their association vectors.\n  We experimentally study the benefit of tunable minibatch arrangements,\ndemonstrating consistent reductions of 3-15\\% in training. Arrangement emerges\nas a powerful performance knob for SGD that is orthogonal and compatible with\nother tuning methods, and thus is a candidate for wide deployment.", "authors": ["Eliav Buchnik", "Edith Cohen", "Avinatan Hassidim", "Yossi Matias"], "category": "cs.LG", "comment": "10 pages", "img": "/static/thumbs/1803.05389v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05389v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05389v1", "published_time": "3/14/2018", "rawpid": "1803.05389", "tags": ["cs.LG"], "title": "LSH Microbatches for Stochastic Gradients: Value in Rearrangement"}, {"abstract": "Synthetic aperture radar (SAR) interferometry (InSAR) is performed using\nrepeat-pass geometry. InSAR technique is used to estimate the topographic\nreconstruction of the earth surface. The main problem of the range-Doppler\nfocusing technique is the nature of the two-dimensional SAR result, affected by\nthe layover indetermination. In order to resolve this problem, a minimum of two\nsensor acquisitions, separated by a baseline and extended in the\ncross-slant-range, are needed. However, given its multi-temporal nature, these\ntechniques are vulnerable to atmosphere and Earth environment parameters\nvariation in addition to physical platform instabilities. Furthermore, either\ntwo radars are needed or an interferometric cycle is required (that spans from\ndays to weeks), which makes real time DEM estimation impossible. In this work,\nthe authors propose a novel experimental alternative to the InSAR method that\nuses single-pass acquisitions, using a data driven approach implemented by Deep\nNeural Networks. We propose a fully Convolutional Neural Network (CNN)\nEncoder-Decoder architecture, training it on radar images in order to estimate\nDEMs from single pass image acquisitions. Our results on a set of Sentinel\nimages show that this method is able to learn to some extent the statistical\nproperties of the DEM. The results of this exploratory analysis are encouraging\nand open the way to the solution of single-pass DEM estimation problem with\ndata driven approaches.", "authors": ["Gabriele Costante", "Thomas A. Ciarfuglia", "Filippo Biondi"], "category": "eess.SP", "comment": "Accepted for publication in Proceedings of the 12th European\n  Conference on Synthetic Aperture Rada...", "img": "/static/thumbs/1803.05387v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05387v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05387v1", "published_time": "3/14/2018", "rawpid": "1803.05387", "tags": ["eess.SP", "cs.CV"], "title": "Towards Monocular Digital Elevation Model (DEM) Estimation by\n  Convolutional Neural Networks - Application on Synthetic Aperture Radar\n  Images"}, {"abstract": "Computed Tomography (CT) reconstruction is a fundamental component to a wide\nvariety of applications ranging from security, to healthcare. The classical\ntechniques require measuring projections, called sinograms, from a full\n180$^\\circ$ view of the object. This is impractical in a limited angle\nscenario, when the viewing angle is less than 180$^\\circ$, which can occur due\nto different factors including restrictions on scanning time, limited\nflexibility of scanner rotation, etc. The sinograms obtained as a result, cause\nexisting techniques to produce highly artifact-laden reconstructions. In this\npaper, we propose to address this problem through implicit sinogram completion,\non a challenging real world dataset containing scans of common checked-in\nluggage. We propose a system, consisting of 1D and 2D convolutional neural\nnetworks, that operates on a limited angle sinogram to directly produce the\nbest estimate of a reconstruction. Next, we use the x-ray transform on this\nreconstruction to obtain a \"completed\" sinogram, as if it came from a full\n180$^\\circ$ measurement. We feed this to standard analytical and iterative\nreconstruction techniques to obtain the final reconstruction. We show with\nextensive experimentation that this combined strategy outperforms many\ncompetitive baselines. We also propose a measure of confidence for the\nreconstruction that enables a practitioner to gauge the reliability of a\nprediction made by our network. We show that this measure is a strong indicator\nof quality as measured by the PSNR, while not requiring ground truth at test\ntime. Finally, using a segmentation experiment, we show that our reconstruction\npreserves the 3D structure of objects effectively.", "authors": ["Rushil Anirudh", "Hyojin Kim", "Jayaraman J. Thiagarajan", "K. Aditya Mohan", "Kyle Champley", "Timo Bremer"], "category": "cs.CV", "comment": "Spotlight presentation at CVPR 2018", "img": "/static/thumbs/1711.10388v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.10388v2", "num_discussion": 0, "originally_published_time": "11/28/2017", "pid": "1711.10388v2", "published_time": "3/14/2018", "rawpid": "1711.10388", "tags": ["cs.CV", "stat.ML"], "title": "Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram\n  Completion"}, {"abstract": "Unlike other tasks and despite recent interest, research in textual claim\nverification has been hindered by the lack of large-scale manually annotated\ndatasets. In this paper we introduce a new publicly available dataset for\nverification against textual sources, FEVER: Fact Extraction and VERification.\nIt consists of 185,441 claims generated by altering sentences extracted from\nWikipedia and subsequently verified without knowledge of the sentence they were\nderived from. The claims are classified as Supported, Refuted or NotEnoughInfo\nby annotators achieving 0.6841 in Fleiss $\\kappa$. For the first two classes,\nthe annotators also recorded the sentence(s) forming the necessary evidence for\ntheir judgment. To characterize the challenge of the dataset presented, we\ndevelop a pipeline approach using both baseline and state-of-the-art components\nand compare it to suitably designed oracles. The best accuracy we achieve on\nlabeling a claim accompanied by the correct evidence is 31.87%, while if we\nignore the evidence we achieve 50.91%. Thus we believe that FEVER is a\nchallenging testbed that will help stimulate progress on claim verification\nagainst textual sources.", "authors": ["James Thorne", "Andreas Vlachos", "Christos Christodoulopoulos", "Arpit Mittal"], "category": "cs.CL", "comment": "To appear at NAACL2018. Data will soon be released on\n  https://sheffieldnlp.github.io/fever/", "img": "/static/thumbs/1803.05355v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05355v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05355v1", "published_time": "3/14/2018", "rawpid": "1803.05355", "tags": ["cs.CL"], "title": "FEVER: a large-scale dataset for Fact Extraction and VERification"}, {"abstract": "Recent advances in artificial intelligence (AI), specifically in computer\nvision (CV) and deep learning (DL), have created opportunities for novel\nsystems in many fields. In the last few years, deep learning applications have\ndemonstrated impressive results not only in fields such as autonomous driving\nand robotics, but also in the field of medicine, where they have, in some\ncases, even exceeded human-level performance. However, despite the huge\npotential, adoption of deep learning-based methods is still slow in many areas,\nespecially in veterinary medicine, where we haven\u0027t been able to find any\nresearch papers using modern convolutional neural networks (CNNs) in medical\nimage processing. We believe that using deep learning-based medical imaging can\nenable more accurate, faster and less expensive diagnoses in veterinary\nmedicine. In order to do so, however, these methods have to be accessible to\neveryone in this field, not just to computer scientists. To show the potential\nof this technology, we present results on a real-world task in veterinary\nmedicine that is usually done manually: feline reticulocyte percentage. Using\nan open source Keras implementation of the Single-Shot MultiBox Detector (SSD)\nmodel architecture and training it on only 800 labeled images, we achieve an\naccuracy of 98.7% at predicting the correct number of aggregate reticulocytes\nin microscope images of cat blood smears. The main motivation behind this paper\nis to show not only that deep learning can approach or even exceed human-level\nperformance on a task like this, but also that anyone in the field can\nimplement it, even without a background in computer science.", "authors": ["Krunoslav Vinicki", "Pierluigi Ferrari", "Maja Belic", "Romana Turk"], "category": "cs.CV", "comment": "10 pages, 2 figures", "img": "/static/thumbs/1803.04873v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04873v2", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.04873v2", "published_time": "3/14/2018", "rawpid": "1803.04873", "tags": ["cs.CV"], "title": "Using Convolutional Neural Networks for Determining Reticulocyte\n  Percentage in Cats"}, {"abstract": "Multispectral images of color-thermal pairs have shown more effective than a\nsingle color channel for pedestrian detection, especially under challenging\nillumination conditions. However, there is still a lack of studies on how to\nfuse the two modalities effectively. In this paper, we deeply compare six\ndifferent convolutional network fusion architectures and analyse their\nadaptations, enabling a vanilla architecture to obtain detection performances\ncomparable to the state-of-the-art results. Further, we discover that\npedestrian detection confidences from color or thermal images are correlated\nwith illumination conditions. With this in mind, we propose an\nIllumination-aware Faster R-CNN (IAF RCNN). Specifically, an Illumination-aware\nNetwork is introduced to give an illumination measure of the input image. Then\nwe adaptively merge color and thermal sub-networks via a gate function defined\nover the illumination value. The experimental results on KAIST Multispectral\nPedestrian Benchmark validate the effectiveness of the proposed IAF R-CNN.", "authors": ["Chengyang Li", "Dan Song", "Ruofeng Tong", "Min Tang"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05347v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05347v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05347v1", "published_time": "3/14/2018", "rawpid": "1803.05347", "tags": ["cs.CV"], "title": "Illumination-aware Faster R-CNN for Robust Multispectral Pedestrian\n  Detection"}, {"abstract": "Machine learning employs dynamical algorithms that mimic the human capacity\nto learn, where the reinforcement learning ones are among the most similar to\nhumans in this respect. On the other hand, adaptability is an essential aspect\nto perform any task efficiently in a changing environment, and it is\nfundamental for many purposes, such as natural selection. Here, we propose an\nalgorithm based on successive measurements to adapt one quantum state to a\nreference unknown state, in the sense of achieving maximum overlap. The\nprotocol naturally provides many identical copies of the reference state, such\nthat in each measurement iteration more information about it is obtained. In\nour protocol, we consider a system composed of three parts, the \"environment\"\nsystem, which provides the reference state copies; the register, which is an\nauxiliary subsystem that interacts with the environment to acquire information\nfrom it; and the agent, which corresponds to the quantum state that is adapted\nby digital feedback with input corresponding to the outcome of the measurements\non the register. With this proposal we can achieve an average fidelity between\nthe environment and the agent of more than $90\\% $ with less than $30$\niterations of the protocol. In addition, we extend the formalism to $ d\n$-dimensional states, reaching an average fidelity of around $80\\% $ in less\nthan $400$ iterations for $d=$ 11, for a variety of genuinely quantum as well\nas semiclassical states. This work paves the way for the development of quantum\nreinforcement learning protocols using quantum data, and the future deployment\nof semi-autonomous quantum systems.", "authors": ["F. Albarr\u00e1n-Arriagada", "J. C. Retamal", "E. Solano", "L. Lamata"], "category": "quant-ph", "comment": "", "img": "/static/thumbs/1803.05340v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05340v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05340v1", "published_time": "3/14/2018", "rawpid": "1803.05340", "tags": ["quant-ph", "cond-mat.mes-hall", "cs.AI", "cs.LG", "stat.ML"], "title": "Measurement-based adaptation protocol with quantum reinforcement\n  learning"}, {"abstract": "Oral Disintegrating Tablets (ODTs) is a novel dosage form that can be\ndissolved on the tongue within 3min or less especially for geriatric and\npediatric patients. Current ODT formulation studies usually rely on the\npersonal experience of pharmaceutical experts and trial-and-error in the\nlaboratory, which is inefficient and time-consuming. The aim of current\nresearch was to establish the prediction model of ODT formulations with direct\ncompression process by Artificial Neural Network (ANN) and Deep Neural Network\n(DNN) techniques. 145 formulation data were extracted from Web of Science. All\ndata sets were divided into three parts: training set (105 data), validation\nset (20) and testing set (20). ANN and DNN were compared for the prediction of\nthe disintegrating time. The accuracy of the ANN model has reached 85.60%,\n80.00% and 75.00% on the training set, validation set and testing set\nrespectively, whereas that of the DNN model was 85.60%, 85.00% and 80.00%,\nrespectively. Compared with the ANN, DNN showed the better prediction for ODT\nformulations. It is the first time that deep neural network with the improved\ndataset selection algorithm is applied to formulation prediction on small data.\nThe proposed predictive approach could evaluate the critical parameters about\nquality control of formulation, and guide research and process development. The\nimplementation of this prediction model could effectively reduce drug product\ndevelopment timeline and material usage, and proactively facilitate the\ndevelopment of a robust drug product.", "authors": ["Run Han", "Yilong Yang", "Xiaoshan Li", "Defang Ouyang"], "category": "stat.ML", "comment": "This is a post-peer-review, pre-copyedit version of an article\n  published in Asian Journal of Pharm...", "img": "/static/thumbs/1803.05339v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05339v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05339v1", "published_time": "3/14/2018", "rawpid": "1803.05339", "tags": ["stat.ML", "cs.LG"], "title": "Predicting Oral Disintegrating Tablet Formulations by Neural Network\n  Techniques"}, {"abstract": "In industrial machine learning pipelines, data often arrive in parts.\nParticularly in the case of deep neural networks, it may be too expensive to\ntrain the model from scratch each time, so one would rather use a previously\nlearned model and the new data to improve performance. However, deep neural\nnetworks are prone to getting stuck in a suboptimal solution when trained on\nonly new data as compared to the full dataset. Our work focuses on a continuous\nlearning setup where the task is always the same and new parts of data arrive\nsequentially. We apply a Bayesian approach to update the posterior\napproximation with each new piece of data and find this method to outperform\nthe traditional approach in our experiments.", "authors": ["Max Kochurov", "Timur Garipov", "Dmitry Podoprikhin", "Dmitry Molchanov", "Arsenii Ashukha", "Dmitry Vetrov"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1802.07329v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.07329v2", "num_discussion": 0, "originally_published_time": "2/20/2018", "pid": "1802.07329v2", "published_time": "3/14/2018", "rawpid": "1802.07329", "tags": ["stat.ML", "cs.LG"], "title": "Bayesian Incremental Learning for Deep Neural Networks"}, {"abstract": "Hyperspectral signal reconstruction aims at recovering the original spectral\ninput that produced a certain trichromatic (RGB) response from a capturing\ndevice or observer. Given the heavily underconstrained, non-linear nature of\nthe problem, traditional techniques leverage different statistical properties\nof the spectral signal in order to build informative priors from real world\nobject reflectances for constructing such RGB to spectral signal mapping.\nHowever, most of them treat each sample independently, and thus do not benefit\nfrom the contextual information that the spatial dimensions can provide. We\npose hyperspectral natural image reconstruction as an image to image mapping\nlearning problem, and apply a conditional generative adversarial framework to\nhelp capture spatial semantics. This is the first time Convolutional Neural\nNetworks -and, particularly, Generative Adversarial Networks- are used to solve\nthis task. Quantitative evaluation shows a Root Mean Squared Error (RMSE) drop\nof 33.2% and a Relative RMSE drop of 54.0% on the ICVL natural hyperspectral\nimage dataset.", "authors": ["Aitor Alvarez-Gila", "Joost van de Weijer", "Estibaliz Garrote"], "category": "cs.CV", "comment": "Accepted at IEEE ICCVW 2017 - \"Physics Based Vision meets Deep\n  Learning\" Workshop (PBDL 2017). Add...", "img": "/static/thumbs/1709.00265v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.00265v2", "num_discussion": 0, "originally_published_time": "9/1/2017", "pid": "1709.00265v2", "published_time": "3/14/2018", "rawpid": "1709.00265", "tags": ["cs.CV"], "title": "Adversarial Networks for Spatial Context-Aware Spectral Image\n  Reconstruction from RGB"}, {"abstract": "We propose a method for domain adaptation on graphs. Given sufficiently many\nobservations of the label function on a source graph, we study the problem of\ntransferring the label information from the source graph to a target graph for\nestimating the target label function. Our assumption about the relation between\nthe two domains is that the frequency content of the label function, regarded\nas a graph signal, has similar characteristics over the source and the target\ngraphs. We propose a method to learn a pair of coherent bases on the two\ngraphs, such that the corresponding source and target graph basis vectors have\nsimilar spectral content, while \"aligning\" the two graphs at the same time so\nthat the reconstructed source and target label functions have similar\ncoefficients over the bases. Experiments on several types of data sets suggest\nthat the proposed method compares quite favorably to reference domain\nadaptation methods. To the best of our knowledge, our treatment is the first to\nstudy the domain adaptation problem in a purely graph-based setting with no\nneed for embedding the data in an ambient space. This feature is particularly\nconvenient for many problems of interest concerning learning on graphs or\nnetworks.", "authors": ["Mehmet Pilanci", "Elif Vural"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1803.05288v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05288v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05288v1", "published_time": "3/14/2018", "rawpid": "1803.05288", "tags": ["stat.ML", "cs.LG"], "title": "Domain Adaptation on Graphs by Learning Aligned Graph Bases"}, {"abstract": "In this paper we investigate the use of MPC-inspired neural network policies\nfor sequential decision making. We introduce an extension to the DAgger\nalgorithm for training such policies and show how they have improved training\nperformance and generalization capabilities. We take advantage of this\nextension to show scalable and efficient training of complex planning policy\narchitectures in continuous state and action spaces. We provide an extensive\ncomparison of neural network policies by considering feed forward policies,\nrecurrent policies, and recurrent policies with planning structure inspired by\nthe Path Integral control framework. Our results suggest that MPC-type\nrecurrent policies have better robustness to disturbances and modeling error.", "authors": ["Marcus Pereira", "David D. Fan", "Gabriel Nakajima An", "Evangelos Theodorou"], "category": "cs.LG", "comment": "Fixed missing reference to section 4.1", "img": "/static/thumbs/1802.05803v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.05803v2", "num_discussion": 0, "originally_published_time": "2/15/2018", "pid": "1802.05803v2", "published_time": "3/14/2018", "rawpid": "1802.05803", "tags": ["cs.LG"], "title": "MPC-Inspired Neural Network Policies for Sequential Decision Making"}, {"abstract": "In this paper, we introduce a simple but quite effective recognition\nframework dubbed D-PCN, aiming at enhancing feature extracting ability of CNN.\nThe framework consists of two parallel CNNs, a discriminator and an extra\nclassifier which takes integrated features from parallel networks and gives\nfinal prediction. The discriminator is core which drives parallel networks to\nfocus on different regions and learn complementary representations. The\ncorresponding joint training strategy is introduced which ensures the\nutilization of discriminator. We validate D-PCN with several CNN models on two\nbenchmark datasets: CIFAR-100 and ImageNet32x32, D-PCN enhances all models. In\nparticular it yields state of the art performance on CIFAR-100 compared with\nrelated works. We also conduct visualization experiment on fine-grained\nStanford Dogs dataset and verify our motivation. Additionally, we apply D-PCN\nfor segmentation on PASCAL VOC 2012 and also find promotion.", "authors": ["Shiqi Yang", "Gang Peng"], "category": "cs.CV", "comment": "20 pages, 8 figures, 7 tables", "img": "/static/thumbs/1711.04237v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.04237v3", "num_discussion": 0, "originally_published_time": "11/12/2017", "pid": "1711.04237v3", "published_time": "3/14/2018", "rawpid": "1711.04237", "tags": ["cs.CV", "cs.LG"], "title": "D-PCN: Parallel Convolutional Networks for Image Recognition via a\n  Discriminator"}, {"abstract": "In this paper, a novel video classification methodology is presented that\naims to recognize different categories of third-person videos efficiently. The\nidea is to keep track of motion in videos by following optical flow elements\nover time. To classify the resulted motion time series efficiently, the idea is\nletting the machine to learn temporal features along the time dimension. This\nis done by training a multi-channel one dimensional Convolutional Neural\nNetwork (1D-CNN). Since CNNs represent the input data hierarchically, high\nlevel features are obtained by further processing of features in lower level\nlayers. As a result, in the case of time series, long-term temporal features\nare extracted from short-term ones. Besides, the superiority of the proposed\nmethod over most of the deep-learning based approaches is that we only try to\nlearn representative temporal features along the time dimension. This reduces\nthe number of learning parameters significantly which results in trainability\nof our method on even smaller datasets. It is illustrated that the proposed\nmethod could reach state-of-the-art results on two public datasets UCF11 and\njHMDB with the aid of a more efficient feature vector representation.", "authors": ["Ali Javidani", "Ahmad Mahmoudi-Aznaveh"], "category": "cs.CV", "comment": "5 pages", "img": "/static/thumbs/1802.06724v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.06724v2", "num_discussion": 0, "originally_published_time": "2/19/2018", "pid": "1802.06724v2", "published_time": "3/14/2018", "rawpid": "1802.06724", "tags": ["cs.CV"], "title": "Learning Representative Temporal Features for Action Recognition"}, {"abstract": "Visual question answering requires high-order reasoning about an image, which\nis a fundamental capability needed by machine systems to follow complex\ndirectives. Recently, modular networks have been shown to be an effective\nframework for performing visual reasoning tasks. While modular networks were\ninitially designed with a degree of model transparency, their performance on\ncomplex visual reasoning benchmarks was lacking. Current state-of-the-art\napproaches do not provide an effective mechanism for understanding the\nreasoning process. In this paper, we close the performance gap between\ninterpretable models and state-of-the-art visual reasoning methods. We propose\na set of visual-reasoning primitives which, when composed, manifest as a model\ncapable of performing complex reasoning tasks in an explicitly-interpretable\nmanner. The fidelity and interpretability of the primitives\u0027 outputs enable an\nunparalleled ability to diagnose the strengths and weaknesses of the resulting\nmodel. Critically, we show that these primitives are highly performant,\nachieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show\nthat our model is able to effectively learn generalized representations when\nprovided a small amount of data containing novel object attributes. Using the\nCoGenT generalization task, we show more than a 20 percentage point improvement\nover the current state of the art.", "authors": ["David Mascharka", "Philip Tran", "Ryan Soklaski", "Arjun Majumdar"], "category": "cs.CV", "comment": "CVPR 2018 pre-print", "img": "/static/thumbs/1803.05268v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05268v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05268v1", "published_time": "3/14/2018", "rawpid": "1803.05268", "tags": ["cs.CV"], "title": "Transparency by Design: Closing the Gap Between Performance and\n  Interpretability in Visual Reasoning"}, {"abstract": "Estimating the uncertainty in image registration is an area of current\nresearch that is aimed at providing information that will enable surgeons to\nassess the operative risk based on registered image data and the estimated\nregistration uncertainty. If they receive inaccurately calculated registration\nuncertainty and misplace confidence in the alignment solutions, severe\nconsequences may result. For probabilistic image registration (PIR), most\nresearch quantifies the registration uncertainty using summary statistics of\nthe transformation distributions. In this paper, we study a rarely examined\ntopic: whether those summary statistics of the transformation distribution\ntruly represent the registration uncertainty. Using concrete examples, we show\nthat there are two types of uncertainties: the transformation uncertainty, Ut,\nand label uncertainty Ul. Ut indicates the doubt concerning transformation\nparameters and can be estimated by conventional uncertainty measures, while Ul\nis strongly linked to the goal of registration. Further, we show that using Ut\nto quantify Ul is inappropriate and can be misleading. In addition, we present\nsome potentially critical findings regarding PIR.", "authors": ["Jie Luo", "Sarah Frisken", "Karteek Popuri", "Dana Cobzas", "Frank Preiswerk", "Matt Toews", "Miaomiao Zhang", "Hongyi Ding", "Polina Golland", "Alexandra Golby", "Masashi Sugiyama", "William M. Wells III"], "category": "cs.CV", "comment": "arXiv admin note: text overlap with arXiv:1704.08121", "img": "/static/thumbs/1803.05266v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05266v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05266v1", "published_time": "3/14/2018", "rawpid": "1803.05266", "tags": ["cs.CV"], "title": "On the Ambiguity of Registration Uncertainty"}, {"abstract": "Text in natural images is of arbitrary orientations, requiring detection in\nterms of oriented bounding boxes. Normally, a multi-oriented text detector\noften involves two key tasks: 1) text presence detection, which is a\nclassification problem disregarding text orientation; 2) oriented bounding box\nregression, which concerns about text orientation. Previous methods rely on\nshared features for both tasks, resulting in degraded performance due to the\nincompatibility of the two tasks. To address this issue, we propose to perform\nclassification and regression on features of different characteristics,\nextracted by two network branches of different designs. Concretely, the\nregression branch extracts rotation-sensitive features by actively rotating the\nconvolutional filters, while the classification branch extracts\nrotation-invariant features by pooling the rotation-sensitive features. The\nproposed method named Rotation-sensitive Regression Detector (RRD) achieves\nstate-of-the-art performance on three oriented scene text benchmark datasets,\nincluding ICDAR 2015, MSRA-TD500, RCTW-17 and COCO-Text. Furthermore, RRD\nachieves a significant improvement on a ship collection dataset, demonstrating\nits generality on oriented object detection.", "authors": ["Minghui Liao", "Zhen Zhu", "Baoguang Shi", "Gui-song Xia", "Xiang Bai"], "category": "cs.CV", "comment": "accepted by CVPR 2018", "img": "/static/thumbs/1803.05265v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05265v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05265v1", "published_time": "3/14/2018", "rawpid": "1803.05265", "tags": ["cs.CV"], "title": "Rotation-Sensitive Regression for Oriented Scene Text Detection"}, {"abstract": "Deep reinforcement learning (DRL) has proven to be an effective tool for\ncreating general video-game AI. However most current DRL video-game agents\nlearn end-to-end from the video-output of the game, which is superfluous for\nmany applications and creates a number of additional problems. More\nimportantly, directly working on pixel-based raw video data is substantially\ndistinct from what a human player does.In this paper, we present a novel method\nwhich enables DRL agents to learn directly from object information. This is\nobtained via use of an object embedding network (OEN) that compresses a set of\nobject feature vectors of different lengths into a single fixed-length unified\nfeature vector representing the current game-state and fulfills the DRL\nsimultaneously. We evaluate our OEN-based DRL agent by comparing to several\nstate-of-the-art approaches on a selection of games from the GVG-AI\nCompetition. Experimental results suggest that our object-based DRL agent\nyields performance comparable to that of those approaches used in our\ncomparative study.", "authors": ["William Woof", "Ke Chen"], "category": "cs.LG", "comment": "technical report, submitted to IEEE CIG2018", "img": "/static/thumbs/1803.05262v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05262v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05262v1", "published_time": "3/14/2018", "rawpid": "1803.05262", "tags": ["cs.LG", "cs.AI"], "title": "Learning to Play General Video-Games via an Object Embedding Network"}, {"abstract": "In this paper, we introduce the Face Magnifier Network (Face-MageNet), a face\ndetector based on the Faster-RCNN framework which enables the flow of\ndiscriminative information of small scale faces to the classifier without any\nskip or residual connections. To achieve this, Face-MagNet deploys a set of\nConvTranspose, also known as deconvolution, layers in the Region Proposal\nNetwork (RPN) and another set before the Region of Interest (RoI) pooling layer\nto facilitate detection of finer faces. In addition, we also design, train, and\nevaluate three other well-tuned architectures that represent the conventional\nsolutions to the scale problem: context pooling, skip connections, and scale\npartitioning. Each of these three networks achieves comparable results to the\nstate-of-the-art face detectors. With extensive experiments, we show that\nFace-MagNet based on a VGG16 architecture achieves better results than the\nrecently proposed ResNet101-based HR method on the task of face detection on\nWIDER dataset and also achieves similar results on the hard set as our other\nmethod SSH.", "authors": ["Pouya Samangouei", "Mahyar Najibi", "Larry Davis", "Rama Chellappa"], "category": "cs.CV", "comment": "Accepted in WACV18", "img": "/static/thumbs/1803.05258v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05258v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05258v1", "published_time": "3/14/2018", "rawpid": "1803.05258", "tags": ["cs.CV"], "title": "Face-MagNet: Magnifying Feature Maps to Detect Small Faces"}, {"abstract": "The problem of video object segmentation can become extremely challenging\nwhen multiple instances co-exist. While each instance may exhibit large scale\nand pose variations, the problem is compounded when instances occlude each\nother causing failures in tracking. In this study, we formulate a deep\nrecurrent network that is capable of segmenting and tracking objects in video\nsimultaneously by their temporal continuity, yet able to re-identify them when\nthey re-appear after a prolonged occlusion. We combine both temporal\npropagation and re-identification functionalities into a single framework that\ncan be trained end-to-end. In particular, we present a re-identification module\nwith template expansion to retrieve missing objects despite their large\nappearance changes. In addition, we contribute a new attention-based recurrent\nmask propagation approach that is robust to distractors not belonging to the\ntarget segment. Our approach achieves a new state-of-the-art global mean\n(Region Jaccard and Boundary F measure) of 68.2 on the challenging DAVIS 2017\nbenchmark (test-dev set), outperforming the winning solution which achieves a\nglobal mean of 66.1 on the same partition.", "authors": ["Xiaoxiao Li", "Chen Change Loy"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.04242v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04242v2", "num_discussion": 0, "originally_published_time": "3/12/2018", "pid": "1803.04242v2", "published_time": "3/14/2018", "rawpid": "1803.04242", "tags": ["cs.CV"], "title": "Video Object Segmentation with Joint Re-identification and\n  Attention-Aware Mask Propagation"}, {"abstract": "Sparsity exploiting image reconstruction (SER) methods have been extensively\nused with Total Variation (TV) regularization for tomographic reconstructions.\nLocal TV methods fail to preserve texture details and often create additional\nartefacts due to over-smoothing. Non-Local TV (NLTV) methods have been proposed\nas a solution to this but they either lack continuous updates due to\ncomputational constraints or limit the locality to a small region. In this\npaper, we propose Adaptive Graph-based TV (AGTV). The proposed method goes\nbeyond spatial similarity between different regions of an image being\nreconstructed by establishing a connection between similar regions in the\nentire image regardless of spatial distance. As compared to NLTV the proposed\nmethod is computationally efficient and involves updating the graph prior\nduring every iteration making the connection between similar regions stronger.\nMoreover, it promotes sparsity in the wavelet and graph gradient domains. Since\nTV is a special case of graph TV the proposed method can also be seen as a\ngeneralization of SER and TV methods.", "authors": ["Faisal Mahmood", "Nauman Shahid", "Ulf Skoglund", "Pierre Vandergheynst"], "category": "cs.CV", "comment": "8 Pages, 5 page letter, 3 page supplement, 8 Figures, Accepted for\n  publication: IEEE Signal Proces...", "img": "/static/thumbs/1610.00893v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.00893v3", "num_discussion": 0, "originally_published_time": "10/4/2016", "pid": "1610.00893v3", "published_time": "3/14/2018", "rawpid": "1610.00893", "tags": ["cs.CV"], "title": "Adaptive Graph-based Total Variation for Tomographic Reconstructions"}, {"abstract": "In recent years, dynamic vision sensors (DVS), also known as event-based\ncameras or neuromorphic sensors, have seen increased use due to various\nadvantages over conventional frame-based cameras. Using principles inspired by\nthe retina, its high temporal resolution overcomes motion blurring, its high\ndynamic range overcomes extreme illumination conditions and its low power\nconsumption makes it ideal for embedded systems on platforms such as drones and\nself-driving cars. However, event-based data sets are scarce and labels are\neven rarer for tasks such as object detection. We transferred discriminative\nknowledge from a state-of-the-art frame-based convolutional neural network\n(CNN) to the event-based modality via intermediate pseudo-labels, which are\nused as targets for supervised learning. We show, for the first time,\nevent-based car detection under ego-motion in a real environment at 100 frames\nper second with a test average precision of 40.3% relative to our annotated\nground truth. The event-based car detector handles motion blur and poor\nillumination conditions despite not explicitly trained to do so, and even\ncomplements frame-based CNN detectors, suggesting that it has learnt\ngeneralized visual representations.", "authors": ["Nicholas F. Y. Chen"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1709.09323v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.09323v3", "num_discussion": 0, "originally_published_time": "9/27/2017", "pid": "1709.09323v3", "published_time": "3/14/2018", "rawpid": "1709.09323", "tags": ["cs.CV"], "title": "Pseudo-labels for Supervised Learning on Dynamic Vision Sensor Data,\n  Applied to Object Detection under Ego-motion"}, {"abstract": "We introduce a large dataset of narrative texts and questions about these\ntexts, intended to be used in a machine comprehension task that requires\nreasoning using commonsense knowledge. Our dataset complements similar datasets\nin that we focus on stories about everyday activities, such as going to the\nmovies or working in the garden, and that the questions require commonsense\nknowledge, or more specifically, script knowledge, to be answered. We show that\nour mode of data collection via crowdsourcing results in a substantial amount\nof such inference questions. The dataset forms the basis of a shared task on\ncommonsense and script knowledge organized at SemEval 2018 and provides\nchallenging test cases for the broader natural language understanding\ncommunity.", "authors": ["Simon Ostermann", "Ashutosh Modi", "Michael Roth", "Stefan Thater", "Manfred Pinkal"], "category": "cs.CL", "comment": "Accepted at LREC 2018", "img": "/static/thumbs/1803.05223v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05223v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05223v1", "published_time": "3/14/2018", "rawpid": "1803.05223", "tags": ["cs.CL"], "title": "MCScript: A Novel Dataset for Assessing Machine Comprehension Using\n  Script Knowledge"}, {"abstract": "Demosaicking and denoising are among the most crucial steps of modern digital\ncamera pipelines. Meanwhile, joint image denoising-demosaicking is a highly\nill-posed inverse problem where at-least two-thirds of the information are\nmissing and the rest are corrupted by noise. This poses a great challenge in\nobtaining meaningful reconstructions and a special care for the efficient\ntreatment of the problem is required. While there are several machine learning\napproaches that have been recently introduced to solve this problem, in this\nwork we propose a novel deep learning architecture which is inspired by\npowerful classical image regularization methods and large-scale convex\noptimization techniques. Consequently, our derived network is more transparent\nand has a clear interpretation compared to alternative competitive deep\nlearning approaches. Our extensive experiments demonstrate that our network\noutperforms any previous approaches on both noisy and noise-free data. This\nimprovement in reconstruction quality is attributed to the principled way we\ndesign our network architecture, which also requires fewer trainable parameters\nthan the current state-of-the-art deep network solution. Finally, we show that\nour network has the ability to generalize well even when it is trained on small\ndatasets, while keeping the overall number of parameters low.", "authors": ["Filippos Kokkinos", "Stamatios Lefkimmiatis"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05215v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05215v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05215v1", "published_time": "3/14/2018", "rawpid": "1803.05215", "tags": ["cs.CV"], "title": "Deep Image Demosaicking using a Cascade of Convolutional Residual\n  Denoising Networks"}, {"abstract": "Fingerprint Presentation Attack Detection (FPAD) deals with distinguishing\nimages coming from artificial replicas of the fingerprint characteristic, made\nup of materials like silicone, gelatine or latex, and images coming from alive\nfingerprints. Images are captured by modern scanners, typically relying on\nsolid-state or optical technologies. Since from 2009, the Fingerprint Liveness\nDetection Competition (LivDet) aims to assess the performance of the\nstate-of-the-art algorithms according to a rigorous experimental protocol and,\nat the same time, a simple overview of the basic achievements. The competition\nis open to all academics research centers and all companies that work in this\nfield. The positive, increasing trend of the participants number, which\nsupports the success of this initiative, is confirmed even this year: 17\nalgorithms were submitted to the competition, with a larger involvement of\ncompanies and academies. This means that the topic is relevant for both sides,\nand points out that a lot of work must be done in terms of fundamental and\napplied research.", "authors": ["Valerio Mura", "Giulia Orr\u00f9", "Roberto Casula", "Alessandra Sibiriu", "Giulia Loi", "Pierluigi Tuveri", "Luca Ghiani", "Gian Luca Marcialis"], "category": "cs.CV", "comment": "presented at ICB 2018", "img": "/static/thumbs/1803.05210v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05210v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05210v1", "published_time": "3/14/2018", "rawpid": "1803.05210", "tags": ["cs.CV"], "title": "LivDet 2017 Fingerprint Liveness Detection Competition 2017"}, {"abstract": "Sparse connectivity is an important factor behind the success of\nconvolutional neural networks and recurrent neural networks. In this paper, we\nconsider the problem of learning sparse connectivity for feedforward neural\nnetworks (FNNs). The key idea is that a unit should be connected to a small\nnumber of units at the next level below that are strongly correlated. We use\nChow-Liu\u0027s algorithm to learn a tree-structured probabilistic model for the\nunits at the current level, use the tree to identify subsets of units that are\nstrongly correlated, and introduce a new unit with receptive field over the\nsubsets. The procedure is repeated on the new units to build multiple layers of\nhidden units. The resulting model is called a TRF-net. Empirical results show\nthat, when compared to dense FNNs, TRF-net achieves better or comparable\nclassification performance with much fewer parameters and sparser structures.\nThey are also more interpretable.", "authors": ["Xiaopeng Li", "Zhourong Chen", "Nevin L. Zhang"], "category": "cs.LG", "comment": "7 pages", "img": "/static/thumbs/1803.05209v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05209v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05209v1", "published_time": "3/14/2018", "rawpid": "1803.05209", "tags": ["cs.LG"], "title": "Building Sparse Deep Feedforward Networks using Tree Receptive Fields"}, {"abstract": "Recently, deep learning based clustering methods are shown superior to\ntraditional ones by jointly conducting representation learning and clustering.\nThese methods rely on the assumptions that the number of clusters is known, and\nthat there is one single partition over the data and all attributes define that\npartition. However, in real-world applications, prior knowledge of the number\nof clusters is usually unavailable and there are multiple ways to partition the\ndata based on subsets of attributes. To resolve the issues, we propose latent\ntree variational autoencoder (LTVAE), which simultaneously performs\nrepresentation learning and multidimensional clustering. LTVAE learns latent\nembeddings from data, discovers multi-facet clustering structures based on\nsubsets of latent features, and automatically determines the number of clusters\nin each facet. Experiments show that the proposed method achieves\nstate-of-the-art clustering performance and reals reasonable multifacet\nstructures of the data.", "authors": ["Xiaopeng Li", "Zhourong Chen", "Nevin L. Zhang"], "category": "cs.LG", "comment": "9 pages", "img": "/static/thumbs/1803.05206v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05206v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05206v1", "published_time": "3/14/2018", "rawpid": "1803.05206", "tags": ["cs.LG"], "title": "Latent Tree Variational Autoencoder for Joint Representation Learning\n  and Multidimensional Clustering"}, {"abstract": "Neural sequence-to-sequence networks with attention have achieved remarkable\nperformance for machine translation. One of the reasons for their effectiveness\nis their ability to capture relevant source-side contextual information at each\ntime-step prediction through an attention mechanism. However, the target-side\ncontext is solely based on the sequence model which, in practice, is prone to a\nrecency bias and lacks the ability to capture effectively non-sequential\ndependencies among words. To address this limitation, we propose a\ntarget-side-attentive residual recurrent network for decoding, where attention\nover previous words contributes directly to the prediction of the next word.\nThe residual learning facilitates the flow of information from the distant past\nand is able to emphasize any of the previously translated words, hence it gains\naccess to a wider context. The proposed model outperforms a neural MT baseline\nas well as a memory and self-attention network on three language pairs. The\nanalysis of the attention learned by the decoder confirms that it emphasizes a\nwider context, and that it captures syntactic-like structures.", "authors": ["Lesly Miculicich Werlen", "Nikolaos Pappas", "Dhananjay Ram", "Andrei Popescu-Belis"], "category": "cs.CL", "comment": "Accepted on NAACL 2018", "img": "/static/thumbs/1709.04849v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.04849v3", "num_discussion": 0, "originally_published_time": "9/14/2017", "pid": "1709.04849v3", "published_time": "3/14/2018", "rawpid": "1709.04849", "tags": ["cs.CL"], "title": "Self-Attentive Residual Decoder for Neural Machine Translation"}, {"abstract": "We study the problem of maximizing a monotone set function subject to a\ncardinality constraint $k$ in the setting where some number of elements $\\tau$\nis deleted from the returned set. The focus of this work is on the worst-case\nadversarial setting. While there exist constant-factor guarantees when the\nfunction is submodular, there are no guarantees for non-submodular objectives.\nIn this work, we present a new algorithm Oblivious-Greedy and prove the first\nconstant-factor approximation guarantees for a wider class of non-submodular\nobjectives. The obtained theoretical bounds are the first constant-factor\nbounds that also hold in the linear regime, i.e. when the number of deletions\n$\\tau$ is linear in $k$. Our bounds depend on established parameters such as\nthe submodularity ratio and some novel ones such as the inverse curvature. We\nbound these parameters for two important objectives including support selection\nand variance reduction. Finally, we numerically demonstrate the robust\nperformance of Oblivious-Greedy for these two objectives on various datasets.", "authors": ["Ilija Bogunovic", "Junyao Zhao", "Volkan Cevher"], "category": "stat.ML", "comment": "Added references. Accepted by AISTATS 2018", "img": "/static/thumbs/1802.07073v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.07073v2", "num_discussion": 0, "originally_published_time": "2/20/2018", "pid": "1802.07073v2", "published_time": "3/14/2018", "rawpid": "1802.07073", "tags": ["stat.ML", "cs.AI", "cs.DS", "cs.LG"], "title": "Robust Maximization of Non-Submodular Objectives"}, {"abstract": "Modern deep learning algorithms have triggered various image segmentation\napproaches. However most of them deal with pixel based segmentation. However,\nsuperpixels provide a certain degree of contextual information while reducing\ncomputation cost. In our approach, we have performed superpixel level semantic\nsegmentation considering 3 various levels as neighbours for semantic contexts.\nFurthermore, we have enlisted a number of ensemble approaches like max-voting\nand weighted-average. We have also used the Dempster-Shafer theory of\nuncertainty to analyze confusion among various classes. Our method has proved\nto be superior to a number of different modern approaches on the same dataset.", "authors": ["Aritra Das", "Swarnendu Ghosh", "Ritesh Sarkhel", "Sandipan Choudhuri", "Nibaran Das", "Mita Nasipuri"], "category": "cs.CV", "comment": "Accepted for publication in the Proceedings of Second International\n  Conference on Computing and Co...", "img": "/static/thumbs/1803.05200v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05200v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05200v1", "published_time": "3/14/2018", "rawpid": "1803.05200", "tags": ["cs.CV"], "title": "Combining Multi-level Contexts of Superpixel using Convolutional Neural\n  Networks to perform Natural Scene Labeling"}, {"abstract": "Recently convolutional neural network (CNN) promotes the development of\nstereo matching greatly. Especially those end-to-end stereo methods achieve\nbest performance. However less attention is paid on encoding context\ninformation, simplifying two-stage disparity learning pipeline and improving\ndetails in disparity maps. Differently we focus on these problems. Firstly, we\npropose an one-stage context pyramid based residual pyramid network (CP-RPN)\nfor disparity estimation, in which a context pyramid is embedded to encode\nmulti-scale context clues explicitly. Next, we design a CNN based multi-task\nlearning network called EdgeStereo to recover missing details in disparity\nmaps, utilizing mid-level features from edge detection task. In EdgeStereo,\nCP-RPN is integrated with a proposed edge detector HED$_\\beta$ based on\ntwo-fold multi-task interactions. The end-to-end EdgeStereo outputs the edge\nmap and disparity map directly from a stereo pair without any post-processing\nor regularization. We discover that edge detection task and stereo matching\ntask can help each other in our EdgeStereo framework. Comprehensive experiments\non stereo benchmarks such as Scene Flow and KITTI 2015 show that our method\nachieves state-of-the-art performance.", "authors": ["Xiao Song", "Xu Zhao", "Hanwen Hu", "Liangji Fang"], "category": "cs.CV", "comment": "16 pages, 6 figures, 3 tables", "img": "/static/thumbs/1803.05196v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05196v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05196v1", "published_time": "3/14/2018", "rawpid": "1803.05196", "tags": ["cs.CV"], "title": "EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo\n  Matching"}, {"abstract": "PURPOSE: Real-time assessment of ventricular volumes requires high\nacceleration factors. Residual convolutional neural networks (CNN) have shown\npotential for removing artifacts caused by data undersampling. In this study we\ninvestigated the effect of different radial sampling patterns on the accuracy\nof a CNN. We also acquired actual real-time undersampled radial data in\npatients with congenital heart disease (CHD), and compare CNN reconstruction to\nCompressed Sensing (CS).\n  METHODS: A 3D (2D plus time) CNN architecture was developed, and trained\nusing 2276 gold-standard paired 3D data sets, with 14x radial undersampling.\nFour sampling schemes were tested, using 169 previously unseen 3D \u0027synthetic\u0027\ntest data sets. Actual real-time tiny Golden Angle (tGA) radial SSFP data was\nacquired in 10 new patients (122 3D data sets), and reconstructed using the 3D\nCNN as well as a CS algorithm; GRASP.\n  RESULTS: Sampling pattern was shown to be important for image quality, and\naccurate visualisation of cardiac structures. For actual real-time data,\noverall reconstruction time with CNN (including creation of aliased images) was\nshown to be more than 5x faster than GRASP. Additionally, CNN image quality and\naccuracy of biventricular volumes was observed to be superior to GRASP for the\nsame raw data.\n  CONCLUSION: This paper has demonstrated the potential for the use of a 3D CNN\nfor deep de-aliasing of real-time radial data, within the clinical setting.\nClinical measures of ventricular volumes using real-time data with CNN\nreconstruction are not statistically significantly different from the\ngold-standard, cardiac gated, BH techniques.", "authors": ["Andreas Hauptmann", "Simon Arridge", "Felix Lucka", "Vivek Muthurangu", "Jennifer A. Steeden"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05192v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05192v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05192v1", "published_time": "3/14/2018", "rawpid": "1803.05192", "tags": ["cs.CV", "cs.NE"], "title": "Spatio-temporal Deep De-aliasing for Prospective Assessment of Real-time\n  Ventricular Volumes"}, {"abstract": "This article presents an extensive literature review of technology based\nintervention methodologies for individuals facing Autism Spectrum Disorder\n(ASD). Reviewed methodologies include: contemporary Computer Aided Systems\n(CAS), Computer Vision Assisted Technologies (CVAT) and Virtual Reality (VR) or\nArtificial Intelligence-Assisted interventions. The research over the past\ndecade has provided enough demonstrations that individuals of ASD have a strong\ninterest in technology based interventions and can connect with them for longer\ndurations without facing any trouble(s). Theses technology based interventions\nare useful for individuals facing autism in clinical settings as well as at\nhome and classrooms.\n  Despite showing great promise, research in developing an advanced technology\nbased intervention that is clinically quantitative for ASD is minimal.\nMoreover, the clinicians are generally not convinced about the potential of the\ntechnology based interventions due to non-empirical nature of published\nresults. A major reason behind this non-acceptability is a vast majority of\nstudies on distinct intervention methodologies do not follow any specific\nstandard or research design. Consequently, the data produced by these studies\nis minimally appealing to the clinical community.\n  This research domain has a vast social impact as per official statistics\ngiven by the Autism Society of America, autism is the fastest growing\ndevelopmental disability in the United States (US). The estimated annual cost\nin the US for diagnosis and treatment for ASD is 236-262 Billion US Dollars.\nThe cost of up-bringing an ASD individual is estimated to be 1.4 million USD\nwhile statistics show 1% of the worlds\u0027 total population is suffering from ASD.", "authors": ["Muhammad Shoaib Jaliawala", "Rizwan Ahmed Khan"], "category": "cs.HC", "comment": "", "img": "/static/thumbs/1803.05181v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05181v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05181v1", "published_time": "3/14/2018", "rawpid": "1803.05181", "tags": ["cs.HC", "cs.AI", "cs.LG"], "title": "Can Autism be Catered with Artificial Intelligence-Assisted Intervention\n  Technology? A Literature Review"}, {"abstract": "The existing image captioning approaches typically train a one-stage sentence\ndecoder, which is difficult to generate rich fine-grained descriptions. On the\nother hand, multi-stage image caption model is hard to train due to the\nvanishing gradient problem. In this paper, we propose a coarse-to-fine\nmulti-stage prediction framework for image captioning, composed of multiple\ndecoders each of which operates on the output of the previous stage, producing\nincreasingly refined image descriptions. Our proposed learning approach\naddresses the difficulty of vanishing gradients during training by providing a\nlearning objective function that enforces intermediate supervisions.\nParticularly, we optimize our model with a reinforcement learning approach\nwhich utilizes the output of each intermediate decoder\u0027s test-time inference\nalgorithm as well as the output of its preceding decoder to normalize the\nrewards, which simultaneously solves the well-known exposure bias problem and\nthe loss-evaluation mismatch problem. We extensively evaluate the proposed\napproach on MSCOCO and show that our approach can achieve the state-of-the-art\nperformance.", "authors": ["Jiuxiang Gu", "Jianfei Cai", "Gang Wang", "Tsuhan Chen"], "category": "cs.CV", "comment": "AAAI-2018, Oral Presentation", "img": "/static/thumbs/1709.03376v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.03376v3", "num_discussion": 0, "originally_published_time": "9/11/2017", "pid": "1709.03376v3", "published_time": "3/14/2018", "rawpid": "1709.03376", "tags": ["cs.CV"], "title": "Stack-Captioning: Coarse-to-Fine Learning for Image Captioning"}, {"abstract": "In certain complex optimization tasks, it becomes necessary to use multiple\nmeasures to characterize the performance of different algorithms. This paper\npresents a method that combines ordinal effect sizes with Pareto dominance to\nanalyze such cases. Since the method is ordinal, it can also generalize across\ndifferent optimization tasks even when the performance measurements are\ndifferently scaled. Through a case study, we show that this method can discover\nand quantify relations that would be difficult to deduce using a conventional\nmeasure-by-measure analysis. This case study applies the method to the\nevolution of robot controller repertoires using the MAP-Elites algorithm. Here,\nwe analyze the search performance across a large set of parametrizations;\nvarying mutation size and operator type, as well as map resolution, across four\ndifferent robot morphologies. We show that the average magnitude of mutations\nhas a bigger effect on outcomes than their precise distributions.", "authors": ["Eivind Samuelsen", "Kyrre Glette"], "category": "cs.NE", "comment": "As submitted to GECCO\u002718", "img": "/static/thumbs/1803.05174v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05174v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05174v1", "published_time": "3/14/2018", "rawpid": "1803.05174", "tags": ["cs.NE"], "title": "Multi-objective Analysis of MAP-Elites Performance"}, {"abstract": "We present a variational renormalization group approach using deep generative\nmodel composed of bijectors. The model can learn hierarchical transformations\nbetween physical variables and renormalized collective variables. It can\ndirectly generate statistically independent physical configurations by\niterative refinement at various length scales. The generative model has an\nexact and tractable likelihood, which provides renormalized energy function of\nthe collective variables and supports unbiased rejection sampling of the\nphysical variables. To train the neural network, we employ probability density\ndistillation, in which the training loss is a variational upper bound of the\nphysical free energy. The approach could be useful for automatically\nidentifying collective variables and effective field theories.", "authors": ["Shuo-Hui Li", "Lei Wang"], "category": "cond-mat.stat-mech", "comment": "Main text: 4.5 pages, 4 figures. Supplement: 6 pages, 5 figures;\n  Github link: https://github.com/l...", "img": "/static/thumbs/1802.02840v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.02840v2", "num_discussion": 0, "originally_published_time": "2/8/2018", "pid": "1802.02840v2", "published_time": "3/14/2018", "rawpid": "1802.02840", "tags": ["cond-mat.stat-mech", "cs.LG", "stat.ML"], "title": "Neural Network Renormalization Group"}, {"abstract": "We consider Bayesian inference problems with computationally intensive\nlikelihood functions. We propose a Gaussian process (GP) based method to\napproximate the joint distribution of the unknown parameters and the data. In\nparticular, we write the joint density approximately as a product of an\napproximate posterior density and an exponentiated GP surrogate. We then\nprovide an adaptive algorithm to construct such an approximation, where an\nactive learning method is used to choose the design points. With numerical\nexamples, we illustrate that the proposed method has competitive performance\nagainst existing approaches for Bayesian computation.", "authors": ["Hongqiao Wang", "Jinglai Li"], "category": "stat.CO", "comment": "", "img": "/static/thumbs/1703.09930v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.09930v4", "num_discussion": 0, "originally_published_time": "3/29/2017", "pid": "1703.09930v4", "published_time": "3/14/2018", "rawpid": "1703.09930", "tags": ["stat.CO", "stat.ML"], "title": "Adaptive Gaussian process approximation for Bayesian inference with\n  expensive likelihood functions"}, {"abstract": "Deep convolutional neural networks (CNNs) used in practice employ potentially\nhundreds of layers and $10$,$000$s of nodes. Such network sizes entail\nsignificant computational complexity due to the large number of convolutions\nthat need to be carried out; in addition, a large number of parameters needs to\nbe learned and stored. Very deep and wide CNNs may therefore not be well suited\nto applications operating under severe resource constraints as is the case,\ne.g., in low-power embedded and mobile platforms. This paper aims at\nunderstanding the impact of CNN topology, specifically depth and width, on the\nnetwork\u0027s feature extraction capabilities. We address this question for the\nclass of scattering networks that employ either Weyl-Heisenberg filters or\nwavelets, the modulus non-linearity, and no pooling. The exponential feature\nmap energy decay results in Wiatowski et al., 2017, are generalized to\n$\\mathcal{O}(a^{-N})$, where an arbitrary decay factor $a\u003e1$ can be realized\nthrough suitable choice of the Weyl-Heisenberg prototype function or the mother\nwavelet. We then show how networks of fixed (possibly small) depth $N$ can be\ndesigned to guarantee that $((1-\\varepsilon)\\cdot 100)\\%$ of the input signal\u0027s\nenergy are contained in the feature vector. Based on the notion of\noperationally significant nodes, we characterize, partly rigorously and partly\nheuristically, the topology-reducing effects of (effectively) band-limited\ninput signals, band-limited filters, and feature map symmetries. Finally, for\nnetworks based on Weyl-Heisenberg filters, we determine the prototype function\nbandwidth that minimizes---for fixed network depth $N$---the average number of\noperationally significant nodes per layer.", "authors": ["Thomas Wiatowski", "Philipp Grohs", "Helmut B\u00f6lcskei"], "category": "stat.ML", "comment": "Corrected errors in arguments on spectral decay of Sobolev functions.\n  Replaced part of the decay r...", "img": "/static/thumbs/1707.02711v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.02711v2", "num_discussion": 0, "originally_published_time": "7/10/2017", "pid": "1707.02711v2", "published_time": "3/14/2018", "rawpid": "1707.02711", "tags": ["stat.ML", "cs.CV", "cs.IT", "cs.LG", "math.FA", "math.IT"], "title": "Topology Reduction in Deep Convolutional Feature Extraction Networks"}, {"abstract": "When a recurrent neural network language model is used for caption\ngeneration, the image information can be fed to the neural network either by\ndirectly incorporating it in the RNN -- conditioning the language model by\n`injecting\u0027 image features -- or in a layer following the RNN -- conditioning\nthe language model by `merging\u0027 image features. While both options are attested\nin the literature, there is as yet no systematic comparison between the two. In\nthis paper we empirically show that it is not especially detrimental to\nperformance whether one architecture is used or another. The merge architecture\ndoes have practical advantages, as conditioning by merging allows the RNN\u0027s\nhidden state vector to shrink in size by up to four times. Our results suggest\nthat the visual and linguistic modalities for caption generation need not be\njointly encoded by the RNN as that yields large, memory-intensive models with\nfew tangible advantages in performance; rather, the multimodal integration\nshould be delayed to a subsequent stage.", "authors": ["Marc Tanti", "Albert Gatt", "Kenneth P. Camilleri"], "category": "cs.NE", "comment": "Accepted in JNLE Special Issue: Language for Images (24.3) (expanded\n  with content that was removed...", "img": "/static/thumbs/1703.09137v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.09137v2", "num_discussion": 0, "originally_published_time": "3/27/2017", "pid": "1703.09137v2", "published_time": "3/14/2018", "rawpid": "1703.09137", "tags": ["cs.NE", "cs.CL", "cs.CV"], "title": "Where to put the Image in an Image Caption Generator"}, {"abstract": "With the ever increasing application of Convolutional Neural Networks to\ncostumer products the need emerges for models which can efficiently run on\nembedded, mobile hardware. Slimmer models have therefore become a hot research\ntopic with multiple different approaches which vary from binary networks to\nrevised convolution layers. We offer our contribution to the latter and propose\na novel convolution block which significantly reduces the computational burden\nwhile surpassing the current state-of-the-art. Our model, dubbed EffNet, is\noptimised for models which are slim to begin with and is created to tackle\nissues in existing models such as MobileNet and ShuffleNet.", "authors": ["Ido Freeman", "Lutz Roese-Koerner", "Anton Kummert"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1801.06434v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1801.06434v5", "num_discussion": 0, "originally_published_time": "1/19/2018", "pid": "1801.06434v5", "published_time": "3/14/2018", "rawpid": "1801.06434", "tags": ["cs.CV", "cs.NE"], "title": "EffNet: An Efficient Structure for Convolutional Neural Networks"}, {"abstract": "This paper provides an overview of prominent deep learning toolkits and, in\nparticular, reports on recent publications that contributed open source\nsoftware for implementing tasks that are common in intelligent user interfaces\n(IUI). We provide a scientific reference for researchers and software engineers\nwho plan to utilise deep learning techniques within their IUI research and\ndevelopment projects.", "authors": ["Jan Zacharias", "Michael Barz", "Daniel Sonntag"], "category": "cs.HC", "comment": "", "img": "/static/thumbs/1803.04818v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04818v2", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.04818v2", "published_time": "3/14/2018", "rawpid": "1803.04818", "tags": ["cs.HC", "cs.LG", "H.5.2"], "title": "A Survey on Deep Learning Toolkits and Libraries for Intelligent User\n  Interfaces"}, {"abstract": "Social media are becoming an increasingly important source of information\nabout the public mood regarding issues such as elections, Brexit, stock market,\netc. In this paper we focus on sentiment classification of Twitter data.\nConstruction of sentiment classifiers is a standard text mining task, but here\nwe address the question of how to properly evaluate them as there is no settled\nway to do so. Sentiment classes are ordered and unbalanced, and Twitter\nproduces a stream of time-ordered data. The problem we address concerns the\nprocedures used to obtain reliable estimates of performance measures, and\nwhether the temporal ordering of the training and test data matters. We\ncollected a large set of 1.5 million tweets in 13 European languages. We\ncreated 138 sentiment models and out-of-sample datasets, which are used as a\ngold standard for evaluations. The corresponding 138 in-sample datasets are\nused to empirically compare six different estimation procedures: three variants\nof cross-validation, and three variants of sequential validation (where test\nset always follows the training set). We find no significant difference between\nthe best cross-validation and sequential validation. However, we observe that\nall cross-validation variants tend to overestimate the performance, while the\nsequential methods tend to underestimate it. Standard cross-validation with\nrandom selection of examples is significantly worse than the blocked\ncross-validation, and should not be used to evaluate classifiers in\ntime-ordered data scenarios.", "authors": ["Igor Mozeti\u010d", "Luis Torgo", "Vitor Cerqueira", "Jasmina Smailovi\u0107"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1803.05160v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05160v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05160v1", "published_time": "3/14/2018", "rawpid": "1803.05160", "tags": ["cs.CL", "cs.IR", "cs.SI"], "title": "How to evaluate sentiment classifiers for Twitter time-ordered data?"}, {"abstract": "We generalize the convolutional NMF by taking the $\\beta$-divergence as the\nloss function, add a regularizer for sparsity in the form of an elastic net,\nand provide multiplicative update rules for its factors in closed form. The new\nupdate rules embed the $\\beta$-NMF, the standard convolutional NMF, and sparse\ncoding alias basis pursuit. We demonstrate that the originally published update\nrules for the convolutional NMF are suboptimal and that their convergence rate\ndepends on the size of the kernel.", "authors": ["Pedro J. Villasana T.", "Stanislaw Gorlow", "Arvind T. Hariraman"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1803.05159v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05159v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05159v1", "published_time": "3/14/2018", "rawpid": "1803.05159", "tags": ["cs.LG", "cs.DS", "stat.ML"], "title": "Multiplicative Updates for Elastic Net Regularized Convolutional NMF\n  Under $\u03b2$-Divergence"}, {"abstract": "Positive unlabeled (PU) learning is useful in various practical situations,\nwhere there is a need to learn a classifier for a class of interest from an\nunlabeled data set, which may contain anomalies as well as samples from unknown\nclasses. The learning task can be formulated as an optimization problem under\nthe framework of statistical learning theory. Recent studies have theoretically\nanalyzed its properties and generalization performance, nevertheless, little\neffort has been made to consider the problem of scalability, especially when\nlarge sets of unlabeled data are available. In this work we propose a novel\nscalable PU learning algorithm that is theoretically proven to provide the\noptimal solution, while showing superior computational and memory performance.\nExperimental evaluation confirms the theoretical evidence and shows that the\nproposed method can be successfully applied to a large variety of real-world\nproblems involving PU learning.", "authors": ["Emanuele Sansone", "Francesco G. B. De Natale", "Zhi-Hua Zhou"], "category": "cs.LG", "comment": "Submitted to IEEE TPAMI", "img": "/static/thumbs/1608.06807v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.06807v4", "num_discussion": 0, "originally_published_time": "8/24/2016", "pid": "1608.06807v4", "published_time": "3/14/2018", "rawpid": "1608.06807", "tags": ["cs.LG"], "title": "Efficient Training for Positive Unlabeled Learning"}, {"abstract": "This paper presents an overview of the sixth AIBIRDS competition, held at the\n26th International Joint Conference on Artificial Intelligence. This\ncompetition tasked participants with developing an intelligent agent which can\nplay the physics-based puzzle game Angry Birds. This game uses a sophisticated\nphysics engine that requires agents to reason and predict the outcome of\nactions with only limited environmental information. Agents entered into this\ncompetition were required to solve a wide assortment of previously unseen\nlevels within a set time limit. The physical reasoning and planning required to\nsolve these levels are very similar to those of many real-world problems. This\nyear\u0027s competition featured some of the best agents developed so far and even\nincluded several new AI techniques such as deep reinforcement learning. Within\nthis paper we describe the framework, rules, submitted agents and results for\nthis competition. We also provide some background information on related work\nand other video game AI competitions, as well as discussing some potential\nideas for future AIBIRDS competitions and agent improvements.", "authors": ["Matthew Stephenson", "Jochen Renz", "Xiaoyu Ge", "Peng Zhang"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1803.05156v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05156v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05156v1", "published_time": "3/14/2018", "rawpid": "1803.05156", "tags": ["cs.AI"], "title": "The 2017 AIBIRDS Competition"}, {"abstract": "Freight train services in a railway network system are generally divided into\ntwo categories: one is the unscheduled train, whose operating frequency\nfluctuates with origin-destination (OD) demands; the other is the scheduled\ntrain, which is running based on regular timetable just like the passenger\ntrains. The timetable will be released to the public if determined and it would\nnot be influenced by OD demands. Typically, the total capacity of scheduled\ntrains can usually satisfy the predicted demands of express cargos in average.\nHowever, the demands are changing in practice. Therefore, how to distribute the\nshipments between different stations to unscheduled and scheduled train\nservices has become an important research field in railway transportation. This\npaper focuses on the coordinated optimization of the rail express cargos\ndistribution in two service networks. On the premise of fully utilizing the\ncapacity of scheduled service network first, we established a Car-to-Train\n(CTT) assignment model to assign rail express cargos to scheduled and\nunscheduled trains scientifically. The objective function is to maximize the\nnet income of transporting the rail express cargos. The constraints include the\ncapacity restriction on the service arcs, flow balance constraints, logical\nrelationship constraint between two groups of decision variables and the due\ndate constraint. The last constraint is to ensure that the total transportation\ntime of a shipment would not be longer than its predefined due date. Finally,\nwe discuss the linearization techniques to simplify the model proposed in this\npaper, which make it possible for obtaining global optimal solution by using\nthe commercial software.", "authors": ["Boliang Lin"], "category": "cs.AI", "comment": "12 pages, 1 figure", "img": "/static/thumbs/1803.05760v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05760v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05760v1", "published_time": "3/14/2018", "rawpid": "1803.05760", "tags": ["cs.AI", "90Cxx"], "title": "A Study of Car-to-Train Assignment Problem for Rail Express Cargos on\n  Scheduled and Unscheduled Train Service Network"}, {"abstract": "Deep Neural Networks (DNNs) have recently been shown to be vulnerable against\nadversarial examples, which are carefully crafted instances that can mislead\nDNNs to make errors during prediction. To better understand such attacks, a\ncharacterization is needed of the properties of regions (the so-called\n\u0027adversarial subspaces\u0027) in which adversarial examples lie. We tackle this\nchallenge by characterizing the dimensional properties of adversarial regions,\nvia the use of Local Intrinsic Dimensionality (LID). LID assesses the\nspace-filling capability of the region surrounding a reference example, based\non the distance distribution of the example to its neighbors. We first provide\nexplanations about how adversarial perturbation can affect the LID\ncharacteristic of adversarial regions, and then show empirically that LID\ncharacteristics can facilitate the distinction of adversarial examples\ngenerated using state-of-the-art attacks. As a proof-of-concept, we show that a\npotential application of LID is to distinguish adversarial examples, and the\npreliminary results show that it can outperform several state-of-the-art\ndetection measures by large margins for five attack strategies considered in\nthis paper across three benchmark datasets. Our analysis of the LID\ncharacteristic for adversarial regions not only motivates new directions of\neffective adversarial defense, but also opens up more challenges for developing\nnew attacks to better understand the vulnerabilities of DNNs.", "authors": ["Xingjun Ma", "Bo Li", "Yisen Wang", "Sarah M. Erfani", "Sudanthi Wijewickrema", "Grant Schoenebeck", "Dawn Song", "Michael E. Houle", "James Bailey"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1801.02613v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1801.02613v3", "num_discussion": 0, "originally_published_time": "1/8/2018", "pid": "1801.02613v3", "published_time": "3/14/2018", "rawpid": "1801.02613", "tags": ["cs.LG", "cs.CR", "cs.CV"], "title": "Characterizing Adversarial Subspaces Using Local Intrinsic\n  Dimensionality"}, {"abstract": "Recent advances in 3D fully convolutional networks (FCN) have made it\nfeasible to produce dense voxel-wise predictions of volumetric images. In this\nwork, we show that a multi-class 3D FCN trained on manually labeled CT scans of\nseveral anatomical structures (ranging from the large organs to thin vessels)\ncan achieve competitive segmentation results, while avoiding the need for\nhandcrafting features or training class-specific models.\n  To this end, we propose a two-stage, coarse-to-fine approach that will first\nuse a 3D FCN to roughly define a candidate region, which will then be used as\ninput to a second 3D FCN. This reduces the number of voxels the second FCN has\nto classify to ~10% and allows it to focus on more detailed segmentation of the\norgans and vessels.\n  We utilize training and validation sets consisting of 331 clinical CT images\nand test our models on a completely unseen data collection acquired at a\ndifferent hospital that includes 150 CT scans, targeting three anatomical\norgans (liver, spleen, and pancreas). In challenging organs such as the\npancreas, our cascaded approach improves the mean Dice score from 68.5 to\n82.2%, achieving the highest reported average score on this dataset. We compare\nwith a 2D FCN method on a separate dataset of 240 CT scans with 18 classes and\nachieve a significantly higher performance in small organs and vessels.\nFurthermore, we explore fine-tuning our models to different datasets.\n  Our experiments illustrate the promise and robustness of current 3D FCN based\nsemantic segmentation of medical images, achieving state-of-the-art results.\nOur code and trained models are available for download:\nhttps://github.com/holgerroth/3Dunet_abdomen_cascade.", "authors": ["Holger R. Roth", "Hirohisa Oda", "Xiangrong Zhou", "Natsuki Shimizu", "Ying Yang", "Yuichiro Hayashi", "Masahiro Oda", "Michitaka Fujiwara", "Kazunari Misawa", "Kensaku Mori"], "category": "cs.CV", "comment": "Preprint accepted for publication in Computerized Medical Imaging and\n  Graphics. arXiv admin note: ...", "img": "/static/thumbs/1803.05431v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05431v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05431v1", "published_time": "3/14/2018", "rawpid": "1803.05431", "tags": ["cs.CV"], "title": "An application of cascaded 3D fully convolutional networks for medical\n  image segmentation"}, {"abstract": "Binarization of digital documents is the task of classifying each pixel in an\nimage of the document as belonging to the background (parchment/paper) or\nforeground (text/ink). Historical documents are often subject to degradations,\nthat make the task challenging. In the current work a deep neural network\narchitecture is proposed that combines a fully convolutional network with an\nunrolled primal-dual network that can be trained end-to-end in order to achieve\nstate of the art binarization on four out of seven datasets. Document\nbinarization is formulated as a energy minimization problem. A fully\nconvolutional neural network is trained for semantic labeling of pixels to\nprovide class labeling cost associated with each pixel. This cost estimate is\nrefined along the edges to compensate for any over or under estimation of the\nunder represented fore-ground class using a primal-dual approach. We provide\nnecessary overview on proximal operator that facilitates theoretical\nunderpinning in order to train a primal-dual network using a gradient descent\nalgorithm. Numerical instabilities encountered due to the recurrent nature of\nprimal-dual approach are handled. We provide experimental results on document\nbinarization competition dataset along with network changes and hyperparameter\ntuning required for stability and performance of the network. The network when\npre-trained on synthetic dataset performs better as per the competition\nmetrics.", "authors": ["Kalyan Ram Ayyalasomayajula", "Filip Malmberg", "Anders Brun"], "category": "stat.ML", "comment": "Under consideration for Pattern Recognition Letters Special Issue on\n  Graphonomics for e-citizens: ...", "img": "/static/thumbs/1801.08694v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1801.08694v2", "num_discussion": 0, "originally_published_time": "1/26/2018", "pid": "1801.08694v2", "published_time": "3/14/2018", "rawpid": "1801.08694", "tags": ["stat.ML", "cs.LG"], "title": "PDNet: Semantic Segmentation integrated with a Primal-Dual Network for\n  Document binarization"}, {"abstract": "Top-down visual attention mechanisms have been used extensively in image\ncaptioning and visual question answering (VQA) to enable deeper image\nunderstanding through fine-grained analysis and even multiple steps of\nreasoning. In this work, we propose a combined bottom-up and top-down attention\nmechanism that enables attention to be calculated at the level of objects and\nother salient image regions. This is the natural basis for attention to be\nconsidered. Within our approach, the bottom-up mechanism (based on Faster\nR-CNN) proposes image regions, each with an associated feature vector, while\nthe top-down mechanism determines feature weightings. Applying this approach to\nimage captioning, our results on the MSCOCO test server establish a new\nstate-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of\n117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of\nthe method, applying the same approach to VQA we obtain first place in the 2017\nVQA Challenge.", "authors": ["Peter Anderson", "Xiaodong He", "Chris Buehler", "Damien Teney", "Mark Johnson", "Stephen Gould", "Lei Zhang"], "category": "cs.CV", "comment": "CVPR 2018 full oral, winner of the 2017 Visual Question Answering\n  challenge", "img": "/static/thumbs/1707.07998v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.07998v3", "num_discussion": 0, "originally_published_time": "7/25/2017", "pid": "1707.07998v3", "published_time": "3/14/2018", "rawpid": "1707.07998", "tags": ["cs.CV"], "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual\n  Question Answering"}, {"abstract": "Paucity of large curated hand-labeled training data for every\ndomain-of-interest forms a major bottleneck in the deployment of machine\nlearning models in computer vision and other fields. Recent work (Data\nProgramming) has shown how distant supervision signals in the form of labeling\nfunctions can be used to obtain labels for given data in near-constant time. In\nthis work, we present Adversarial Data Programming (ADP), which presents an\nadversarial methodology to generate data as well as a curated aggregated label\nhas given a set of weak labeling functions. We validated our method on the\nMNIST, Fashion MNIST, CIFAR 10 and SVHN datasets, and it outperformed many\nstate-of-the-art models. We conducted extensive experiments to study its\nusefulness, as well as showed how the proposed ADP framework can be used for\ntransfer learning as well as multi-task learning, where data from two domains\nare generated simultaneously using the framework along with the label\ninformation. Our future work will involve understanding the theoretical\nimplications of this new framework from a game-theoretic perspective, as well\nas explore the performance of the method on more complex datasets.", "authors": ["Arghya Pal", "Vineeth N Balasubramanian"], "category": "cs.CV", "comment": "CVPR 2018 main conference paper", "img": "/static/thumbs/1803.05137v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05137v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05137v1", "published_time": "3/14/2018", "rawpid": "1803.05137", "tags": ["cs.CV"], "title": "Adversarial Data Programming: Using GANs to Relax the Bottleneck of\n  Curated Labeled Data"}, {"abstract": "This paper investigates asymptotic behaviors of gradient descent algorithms\n(particularly accelerated gradient descent and stochastic gradient descent) in\nthe context of stochastic optimization arose in statistics and machine learning\nwhere objective functions are estimated from available data. We show that these\nalgorithms can be modeled by continuous-time ordinary or stochastic\ndifferential equations, and their asymptotic dynamic evolutions and\ndistributions are governed by some linear ordinary or stochastic differential\nequations, as the data size goes to infinity. We illustrate that our study can\nprovide a novel unified framework for a joint computational and statistical\nasymptotic analysis on dynamic behaviors of these algorithms with the time (or\nthe number of iterations in the algorithms) and large sample behaviors of the\nstatistical decision rules (like estimators and classifiers) that the\nalgorithms are applied to compute, where the statistical decision rules are the\nlimits of the random sequences generated from these iterative algorithms as the\nnumber of iterations goes to infinity.", "authors": ["Yazhen Wang"], "category": "stat.ML", "comment": "102 pages 2 figure2", "img": "/static/thumbs/1711.09514v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.09514v3", "num_discussion": 0, "originally_published_time": "11/27/2017", "pid": "1711.09514v3", "published_time": "3/14/2018", "rawpid": "1711.09514", "tags": ["stat.ML"], "title": "Asymptotic Analysis via Stochastic Differential Equations of Gradient\n  Descent Algorithms in Statistical and Computational Paradigms"}, {"abstract": "Mapping neuro-inspired algorithms to sensor backplanes of on-chip hardware\nrequire shifting the signal processing from digital to the analog domain,\ndemanding memory technologies beyond conventional CMOS binary storage units.\nUsing memristors for building analog data storage is one of the promising\napproaches amongst emerging non-volatile memory technologies. Recently, a\nmemristive multi-level memory (MLM) cell for storing discrete analog values has\nbeen developed in which memory system is implemented combining memristors in\nvoltage divider configuration. In given example, the memory cell of 3 sub-cells\nwith a memristor in each was programmed to store ternary bits which overall\nachieved 10 and 27 discrete voltage levels. However, for further use of\nproposed memory cell in analog signal processing circuits data encoder is\nrequired to generate control voltages for programming memristors to store\ndiscrete analog values. In this paper, we present the design and performance\nanalysis of data encoder that generates write pattern signals for 10 level\nmemristive memory.", "authors": ["Aidana Irmanova", "Alex Pappachen James"], "category": "cs.ET", "comment": "", "img": "/static/thumbs/1803.05132v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05132v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05132v1", "published_time": "3/14/2018", "rawpid": "1803.05132", "tags": ["cs.ET", "cs.AR", "cs.NE"], "title": "Neuron inspired data encoding memristive multi-level memory cell"}, {"abstract": "Hierarchical Temporal Memory (HTM) is a neuromorphic algorithm that emulates\nsparsity, hierarchy and modularity resembling the working principles of\nneocortex. Feature encoding is an important step to create sparse binary\npatterns. This sparsity is introduced by the binary weights and random weight\nassignment in the initialization stage of the HTM. We propose the alternative\ndeterministic method for the HTM initialization stage, which connects the HTM\nweights to the input data and preserves natural sparsity of the input\ninformation. Further, we introduce the hardware implementation of the\ndeterministic approach and compare it to the traditional HTM and existing\nhardware implementation. We test the proposed approach on the face recognition\nproblem and show that it outperforms the conventional HTM approach.", "authors": ["Olga Krestinskaya", "Alex Pappachen James"], "category": "cs.ET", "comment": "", "img": "/static/thumbs/1803.05131v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05131v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05131v1", "published_time": "3/14/2018", "rawpid": "1803.05131", "tags": ["cs.ET", "cs.AI", "cs.AR", "cs.NE"], "title": "Feature extraction without learning in an analog Spatial Pooler\n  memristive-CMOS circuit design of Hierarchical Temporal Memory"}, {"abstract": "In the event of sensor failure, autonomous vehicles need to safely execute\nemergency maneuvers while avoiding other vehicles on the road. To accomplish\nthis, the sensor-failed vehicle must predict the future semantic behaviors of\nother drivers, such as lane changes, as well as their future trajectories given\na recent window of past sensor observations. We address the first issue of\nsemantic behavior prediction in this paper, which is a precursor to trajectory\nprediction, by introducing a framework that leverages the power of recurrent\nneural networks (RNNs) and graphical models. Our goal is to predict the future\ncategorical driving intent, for lane changes, of neighboring vehicles up to\nthree seconds into the future given as little as a one-second window of past\nLIDAR, GPS, inertial, and map data.\n  We collect real-world data containing over 20 hours of highway driving using\nan autonomous Toyota vehicle. We propose a composite RNN model by adopting the\nmethodology of Structural Recurrent Neural Networks (RNNs) to learn factor\nfunctions and take advantage of both the high-level structure of graphical\nmodels and the sequence modeling power of RNNs, which we expect to afford more\ntransparent modeling and activity than opaque, single RNN models. To\ndemonstrate our approach, we validate our model using authentic interstate\nhighway driving to predict the future lane change maneuvers of other vehicles\nneighboring our autonomous vehicle. We find that our composite Structural RNN\noutperforms baselines by as much as 12% in balanced accuracy metrics.", "authors": ["Sajan Patel", "Brent Griffin", "Kristofer Kusano", "Jason J. Corso"], "category": "cs.RO", "comment": "Preprint of submission to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IRO...", "img": "/static/thumbs/1801.04340v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1801.04340v3", "num_discussion": 0, "originally_published_time": "1/12/2018", "pid": "1801.04340v3", "published_time": "3/14/2018", "rawpid": "1801.04340", "tags": ["cs.RO", "cs.LG"], "title": "Predicting Future Lane Changes of Other Highway Vehicles using RNN-based\n  Deep Models"}, {"abstract": "Deep neural network (DNNs) has shown impressive performance on hard\nperceptual problems. However, researchers found that DNN-based systems are\nvulnerable to adversarial examples which contain specially crafted\nhumans-imperceptible perturbations. Such perturbations cause DNN-based systems\nto misclassify the adversarial examples, with potentially disastrous\nconsequences where safety or security is crucial. As a major security concern,\nstate-of-the-art attacks can still bypass the existing defensive methods. In\nthis paper, we propose a novel defensive framework based on collaborative\nmulti-task training to address the above problem. The proposed defence first\nincorporates specific label pairs into adversarial training process to enhance\nmodel robustness in black-box setting. Then a novel collaborative multi-task\ntraining framework is proposed to construct a detector which identifies\nadversarial examples based on the pairwise relationship of the label pairs. The\ndetector can identify and reject high confidence adversarial examples that\nbypass black-box defence. The model whose robustness has been enhanced work\nreciprocally with the detector on the false-negative adversarial examples.\nImportantly, the proposed collaborative architecture can prevent the adversary\nfrom finding valid adversarial examples in a nearly-white-box setting.", "authors": ["Derek Wang", "Chaoran Li", "Sheng Wen", "Yang Xiang", "Wanlei Zhou", "Surya Nepal"], "category": "cs.LG", "comment": "15 pages, 12 figures", "img": "/static/thumbs/1803.05123v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05123v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05123v1", "published_time": "3/14/2018", "rawpid": "1803.05123", "tags": ["cs.LG", "cs.CR"], "title": "Defensive Collaborative Multi-task Training - Defending against\n  Adversarial Attack towards Deep Neural Networks"}, {"abstract": "Optical coherence tomography (OCT) is a noninvasive imaging modality which\ncan be used to obtain depth images of the retina. The changing layer\nthicknesses can thus be quantified by analyzing these OCT images, moreover\nthese changes have been shown to correlate with disease progression in multiple\nsclerosis. Recent automated retinal layer segmentation tools use machine\nlearning methods to perform pixel-wise labeling and graph methods to guarantee\nthe layer hierarchy or topology. However, graph parameters like distance and\nsmoothness constraints must be experimentally assigned by retinal region and\npathology, thus degrading the flexibility and time efficiency of the whole\nframework. In this paper, we develop cascaded deep networks to provide a\ntopologically correct segmentation of the retinal layers in a single feed\nforward propagation. The first network (S-Net) performs pixel-wise labeling and\nthe second regression network (R-Net) takes the topologically unconstrained\nS-Net results and outputs layer thicknesses for each layer and each position.\nRelu activation is used as the final operation of the R-Net which guarantees\nnon-negativity of the output layer thickness. Since the segmentation boundary\nposition is acquired by summing up the corresponding non-negative layer\nthicknesses, the layer ordering (i.e., topology) of the reconstructed\nboundaries is guaranteed even at the fovea where the distances between\nboundaries can be zero. The R-Net is trained using simulated masks and thus can\nbe generalized to provide topology guaranteed segmentation for other layered\nstructures. This deep network has achieved comparable mean absolute boundary\nerror (2.82 {\\mu}m) to state-of-the-art graph methods (2.83 {\\mu}m).", "authors": ["Yufan He", "Aaron Carass", "Bruno M. Jedynak", "Sharon D. Solomon", "Shiv Saidha", "Peter A. Calabresi", "Jerry L. Prince"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05120v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05120v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05120v1", "published_time": "3/14/2018", "rawpid": "1803.05120", "tags": ["cs.CV"], "title": "Topology guaranteed segmentation of the human retina from OCT using\n  convolutional neural networks"}, {"abstract": "Modern deep learning enabled artificial neural networks, such as Deep Neural\nNetwork (DNN) and Convolutional Neural Network (CNN), have achieved a series of\nbreaking records on a broad spectrum of recognition applications. However, the\nenormous computation and storage requirements associated with such deep and\ncomplex neural network models greatly challenge their implementations on\nresource-limited platforms. Time-based spiking neural network has recently\nemerged as a promising solution in Neuromorphic Computing System designs for\nachieving remarkable computing and power efficiency within a single chip.\nHowever, the relevant research activities have been narrowly concentrated on\nthe biological plausibility and theoretical learning approaches, causing\ninefficient neural processing and impracticable multilayer extension thus\nsignificantly limitations on speed and accuracy when handling the realistic\ncognitive tasks. In this work, a practical multilayer time-based spiking\nneuromorphic architecture, namely \"MT-Spike\", is developed to fill this gap.\nWith the proposed practical time-coding scheme, average delay response model,\ntemporal error backpropagation algorithm, and heuristic loss function,\n\"MT-Spike\" achieves more efficient neural processing through flexible neural\nmodel size reduction while offering very competitive classification accuracy\nfor realistic recognition tasks. Simulation results well validated that the\nalgorithmic power of deep multi-layer learning can be seamlessly merged with\nthe efficiency of time-based spiking neuromorphic architecture, demonstrating\ngreat potentials of \"MT-Spike\" in resource and power constrained embedded\nplatforms.", "authors": ["Tao Liu", "Zihao Liu", "Fuhong Lin", "Yier Jin", "Gang Quan", "Wujie Wen"], "category": "cs.NE", "comment": "36th International Conference On Computer Aided Design (ICCAD 2017)", "img": "/static/thumbs/1803.05117v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05117v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05117v1", "published_time": "3/14/2018", "rawpid": "1803.05117", "tags": ["cs.NE", "q-bio.NC"], "title": "MT-Spike: A Multilayer Time-based Spiking Neuromorphic Architecture with\n  Temporal Error Backpropagation"}, {"abstract": "Uplift modeling is aimed at estimating the incremental impact of an action on\nan individual\u0027s behavior, which is useful in various application domains such\nas targeted marketing (advertisement campaigns) and personalized medicine\n(medical treatments). Conventional methods of uplift modeling require every\ninstance to be jointly equipped with two types of labels: the taken action and\nits outcome. However, obtaining two labels for each instance at the same time\nis difficult or expensive in many real-world problems. In this paper, we\npropose a novel method of uplift modeling that is applicable to a more\npractical setting where only one type of labels is available for each instance.\nWe demonstrate the effectiveness of the proposed method through experiments.", "authors": ["Ikko Yamane", "Florian Yger", "Masashi Sugiyama"], "category": "stat.ML", "comment": "18 pages, 17 figures", "img": "/static/thumbs/1803.05112v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05112v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05112v1", "published_time": "3/14/2018", "rawpid": "1803.05112", "tags": ["stat.ML"], "title": "Uplift Modeling from Separate Labels"}, {"abstract": "One of the most exciting advancements in AI over the last decade is the wide\nadoption of ANNs, such as DNN and CNN, in many real-world applications.\nHowever, the underlying massive amounts of computation and storage requirement\ngreatly challenge their applicability in resource-limited platforms like the\ndrone, mobile phone, and IoT devices etc. The third generation of neural\nnetwork model--Spiking Neural Network (SNN), inspired by the working mechanism\nand efficiency of human brain, has emerged as a promising solution for\nachieving more impressive computing and power efficiency within light-weighted\ndevices (e.g. single chip). However, the relevant research activities have been\nnarrowly carried out on conventional rate-based spiking system designs for\nfulfilling the practical cognitive tasks, underestimating SNN\u0027s energy\nefficiency, throughput, and system flexibility. Although the time-based SNN can\nbe more attractive conceptually, its potentials are not unleashed in realistic\napplications due to lack of efficient coding and practical learning schemes. In\nthis work, a Precise-Time-Dependent Single Spike Neuromorphic Architecture,\nnamely \"PT-Spike\", is developed to bridge this gap. Three constituent\nhardware-favorable techniques: precise single-spike temporal encoding,\nefficient supervised temporal learning, and fast asymmetric decoding are\nproposed accordingly to boost the energy efficiency and data processing\ncapability of the time-based SNN at a more compact neural network model size\nwhen executing real cognitive tasks. Simulation results show that \"PT-Spike\"\ndemonstrates significant improvements in network size, processing efficiency\nand power consumption with marginal classification accuracy degradation when\ncompared with the rate-based SNN and ANN under the similar network\nconfiguration.", "authors": ["Tao Liu", "Lei Jiang", "Yier Jin", "Gang Quan", "Wujie Wen"], "category": "cs.NE", "comment": "23rd Asia and South Pacific Design Automation Conference (ASP-DAC\n  2018)", "img": "/static/thumbs/1803.05109v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05109v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05109v1", "published_time": "3/14/2018", "rawpid": "1803.05109", "tags": ["cs.NE", "q-bio.NC"], "title": "PT-Spike: A Precise-Time-Dependent Single Spike Neuromorphic\n  Architecture with Efficient Supervised Learning"}, {"abstract": "Deep Neural Networks (DNNs) have achieved remarkable performance in a myriad\nof realistic applications. However, recent studies show that well-trained DNNs\ncan be easily misled by adversarial examples (AE) -- the maliciously crafted\ninputs by introducing small and imperceptible input perturbations. Existing\nmitigation solutions, such as adversarial training and defensive distillation,\nsuffer from expensive retraining cost and demonstrate marginal robustness\nimprovement against the state-of-the-art attacks like CW family adversarial\nexamples. In this work, we propose a novel low-cost \"feature distillation\"\nstrategy to purify the adversarial input perturbations of AEs by redesigning\nthe popular image compression framework \"JPEG\". The proposed \"feature\ndistillation\" wisely maximizes the malicious feature loss of AE perturbations\nduring image compression while suppressing the distortions of benign features\nessential for high accurate DNN classification. Experimental results show that\nour method can drastically reduce the success rate of various state-of-the-art\nAE attacks by ~60% on average for both CIFAR-10 and ImageNet benchmarks without\nharming the testing accuracy, outperforming existing solutions like default\nJPEG compression and \"feature squeezing\".", "authors": ["Zihao Liu", "Qi Liu", "Tao Liu", "Yanzhi Wang", "Wujie Wen"], "category": "cs.CV", "comment": "27th International Joint Conference on Artificial Intelligence\n  (IJCAI-18)", "img": "/static/thumbs/1803.05787v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05787v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05787v1", "published_time": "3/14/2018", "rawpid": "1803.05787", "tags": ["cs.CV", "cs.CR"], "title": "Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial\n  Examples"}, {"abstract": "Retrieving the most similar objects in a large-scale database for a given\nquery is a fundamental building block in many application domains, ranging from\nweb searches, visual, cross media, and document retrievals. State-of-the-art\napproaches have mainly focused on capturing the underlying geometry of the data\nmanifolds. Graph-based approaches, in particular, define various diffusion\nprocesses on weighted data graphs. Despite success, these approaches rely on\nfixed-weight graphs, making ranking sensitive to the input affinity matrix. In\nthis study, we propose a new ranking algorithm that simultaneously learns the\ndata affinity matrix and the ranking scores. The proposed optimization\nformulation assigns adaptive neighbors to each point in the data based on the\nlocal connectivity, and the smoothness constraint assigns similar ranking\nscores to similar data points. We develop a novel and efficient algorithm to\nsolve the optimization problem. Evaluations using synthetic and real datasets\nsuggest that the proposed algorithm can outperform the existing methods.", "authors": ["Muge Li", "Liangyue Li", "Feiping Nie"], "category": "cs.LG", "comment": "published at Tsinghua Science and Technology 22(6), 2017", "img": "/static/thumbs/1803.05105v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05105v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05105v1", "published_time": "3/14/2018", "rawpid": "1803.05105", "tags": ["cs.LG", "stat.ML"], "title": "Ranking with Adaptive Neighbors"}, {"abstract": "As one of most fascinating machine learning techniques, deep neural network\n(DNN) has demonstrated excellent performance in various intelligent tasks such\nas image classification. DNN achieves such performance, to a large extent, by\nperforming expensive training over huge volumes of training data. To reduce the\ndata storage and transfer overhead in smart resource-limited Internet-of-Thing\n(IoT) systems, effective data compression is a \"must-have\" feature before\ntransferring real-time produced dataset for training or classification. While\nthere have been many well-known image compression approaches (such as JPEG), we\nfor the first time find that a human-visual based image compression approach\nsuch as JPEG compression is not an optimized solution for DNN systems,\nespecially with high compression ratios. To this end, we develop an image\ncompression framework tailored for DNN applications, named \"DeepN-JPEG\", to\nembrace the nature of deep cascaded information process mechanism of DNN\narchitecture. Extensive experiments, based on \"ImageNet\" dataset with various\nstate-of-the-art DNNs, show that \"DeepN-JPEG\" can achieve ~3.5x higher\ncompression rate over the popular JPEG solution while maintaining the same\naccuracy level for image recognition, demonstrating its great potential of\nstorage and power efficiency in DNN-based smart IoT system design.", "authors": ["Zihao Liu", "Tao Liu", "Wujie Wen", "Lei Jiang", "Jie Xu", "Yanzhi Wang", "Gang Quan"], "category": "cs.CV", "comment": "55th Design Automation Conference (DAC2018)", "img": "/static/thumbs/1803.05788v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05788v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05788v1", "published_time": "3/14/2018", "rawpid": "1803.05788", "tags": ["cs.CV", "cs.GR", "cs.PF"], "title": "DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression\n  Framework"}, {"abstract": "We design differentially private learning algorithms that are agnostic to the\nlearning model. Our algorithms are interactive in nature, i.e., instead of\noutputting a model based on the training data, they provide predictions for a\nset of $m$ feature vectors that arrive online. We show that, for the feature\nvectors on which an ensemble of models (trained on random disjoint subsets of a\ndataset) makes consistent predictions, there is almost no-cost of privacy in\ngenerating accurate predictions for those feature vectors. To that end, we\nprovide a novel coupling of the distance to instability framework with the\nsparse vector technique. We provide algorithms with formal privacy and utility\nguarantees for both binary/multi-class classification, and soft-label\nclassification. For binary classification in the standard (agnostic) PAC model,\nwe show how to bootstrap from our privately generated predictions to construct\na computationally efficient private learner that outputs a final accurate\nhypothesis. Our construction - to the best of our knowledge - is the first\ncomputationally efficient construction for a label-private learner. We prove\nsample complexity upper bounds for this setting. As in non-private sample\ncomplexity bounds, the only relevant property of the given concept class is its\nVC dimension. For soft-label classification, our techniques are based on\nexploiting the stability properties of traditional learning algorithms, like\nstochastic gradient descent (SGD). We provide a new technique to boost the\naverage-case stability properties of learning algorithms to strong (worst-case)\nstability properties, and then exploit them to obtain private classification\nalgorithms. In the process, we also show that a large class of SGD methods\nsatisfy average-case stability properties, in contrast to a smaller class of\nSGD methods that are uniformly stable as shown in prior work.", "authors": ["Raef Bassily", "Om Thakkar", "Abhradeep Thakurta"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1803.05101v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05101v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05101v1", "published_time": "3/14/2018", "rawpid": "1803.05101", "tags": ["cs.LG"], "title": "Model-Agnostic Private Learning via Stability"}, {"abstract": "Social and behavioral interventions are a critical tool for governments and\ncommunities to tackle deep-rooted societal challenges such as homelessness,\ndisease, and poverty. However, real-world interventions are almost always\nplagued by limited resources and limited data, which creates a computational\nchallenge: how can we use algorithmic techniques to enhance the targeting and\ndelivery of social and behavioral interventions? The goal of my thesis is to\nprovide a unified study of such questions, collectively considered under the\nname \"algorithmic social intervention\". This proposal introduces algorithmic\nsocial intervention as a distinct area with characteristic technical\nchallenges, presents my published research in the context of these challenges,\nand outlines open problems for future work. A common technical theme is\ndecision making under uncertainty: how can we find actions which will impact a\nsocial system in desirable ways under limitations of knowledge and resources?\nThe primary application area for my work thus far is public health, e.g. HIV or\ntuberculosis prevention. For instance, I have developed a series of algorithms\nwhich optimize social network interventions for HIV prevention. Two of these\nalgorithms have been pilot-tested in collaboration with LA-area service\nproviders for homeless youth, with preliminary results showing substantial\nimprovement over status-quo approaches. My work also spans other topics in\ninfectious disease prevention and underlying algorithmic questions in robust\nand risk-aware submodular optimization.", "authors": ["Bryan Wilder"], "category": "cs.AI", "comment": "Thesis proposal. 21 pages, 4 figures", "img": "/static/thumbs/1803.05098v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05098v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05098v1", "published_time": "3/14/2018", "rawpid": "1803.05098", "tags": ["cs.AI", "cs.CY", "cs.SI"], "title": "Algorithmic Social Intervention"}, {"abstract": "This paper introduces a nonparametric copula-based index for detecting the\nstrength and monotonicity structure of linear and nonlinear statistical\ndependence between pairs of random variables or stochastic signals. Our index,\ntermed Copula Index for Detecting Dependence and Monotonicity (CIM), satisfies\nseveral desirable properties of measures of association, including R\\\u0027enyi\u0027s\nproperties, the data processing inequality (DPI), and consequently\nself-equitability. Synthetic data simulations reveal that the statistical power\nof CIM compares favorably to other state-of-the-art measures of association\nthat are proven to satisfy the DPI. Simulation results with real-world data\nreveal the CIM\u0027s unique ability to detect the monotonicity structure among\nstochastic signals to find interesting dependencies in large datasets.\nAdditionally, simulations show that the CIM shows favorable performance to\nestimators of mutual information when discovering Markov network structure.", "authors": ["Kiran Karra", "Lamine Mili"], "category": "stat.ML", "comment": "Keywords: copula, statistical dependency, monotonic, equitability,\n  discrete 40 pages", "img": "/static/thumbs/1703.06686v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.06686v3", "num_discussion": 0, "originally_published_time": "3/20/2017", "pid": "1703.06686v3", "published_time": "3/14/2018", "rawpid": "1703.06686", "tags": ["stat.ML", "q-bio.QM"], "title": "Copula Index for Detecting Dependence and Monotonicity between\n  Stochastic Signals"}, {"abstract": "Robust model fitting plays a vital role in computer vision, and research into\nalgorithms for robust fitting continues to be active. Arguably the most popular\nparadigm for robust fitting in computer vision is consensus maximisation, which\nstrives to find the model parameters that maximise the number of inliers.\nDespite the significant developments in algorithms for consensus maximisation,\nthere has been a lack of fundamental analysis of the problem in the computer\nvision literature. In particular, whether consensus maximisation is \"tractable\"\nremains a question that has not been rigorously dealt with, thus making it\ndifficult to assess and compare the performance of proposed algorithms,\nrelative to what is theoretically achievable. To shed light on these issues, we\npresent several computational hardness results for consensus maximisation. Our\nresults underline the fundamental intractability of the problem, and resolve\nseveral ambiguities existing in the literature.", "authors": ["Tat-Jun Chin", "Zhipeng Cai", "Frank Neumann"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1802.06464v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.06464v2", "num_discussion": 0, "originally_published_time": "2/18/2018", "pid": "1802.06464v2", "published_time": "3/14/2018", "rawpid": "1802.06464", "tags": ["cs.CV", "cs.CC"], "title": "Robust Fitting in Computer Vision: Easy or Hard?"}, {"abstract": "Salient object detection is a problem that has been considered in detail and\nmany solutions proposed. In this paper, we argue that work to date has\naddressed a problem that is relatively ill-posed. Specifically, there is not\nuniversal agreement about what constitutes a salient object when multiple\nobservers are queried. This implies that some objects are more likely to be\njudged salient than others, and implies a relative rank exists on salient\nobjects. The solution presented in this paper solves this more general problem\nthat considers relative rank, and we propose data and metrics suitable to\nmeasuring success in a relative objects saliency landscape. A novel deep\nlearning solution is proposed based on a hierarchical representation of\nrelative saliency and stage-wise refinement. We also show that the problem of\nsalient object subitizing can be addressed with the same network, and our\napproach exceeds performance of any prior work across all metrics considered\n(both traditional and newly proposed).", "authors": ["Md Amirul Islam", "Mahmoud Kalash", "Neil D. B. Bruce"], "category": "cs.CV", "comment": "To appear in CVPR 2018", "img": "/static/thumbs/1803.05082v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05082v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05082v1", "published_time": "3/14/2018", "rawpid": "1803.05082", "tags": ["cs.CV"], "title": "Revisiting Salient Object Detection: Simultaneous Detection, Ranking,\n  and Subitizing of Multiple Salient Objects"}, {"abstract": "In this work, we propose a new language modeling paradigm that has the\nability to perform both prediction and moderation of information flow at\nmultiple granularities: neural lattice language models. These models construct\na lattice of possible paths through a sentence and marginalize across this\nlattice to calculate sequence probabilities or optimize parameters. This\napproach allows us to seamlessly incorporate linguistic intuitions - including\npolysemy and existence of multi-word lexical items - into our language model.\nExperiments on multiple language modeling tasks show that English neural\nlattice language models that utilize polysemous embeddings are able to improve\nperplexity by 9.95% relative to a word-level baseline, and that a Chinese model\nthat handles multi-character tokens is able to improve perplexity by 20.94%\nrelative to a character-level baseline.", "authors": ["Jacob Buckman", "Graham Neubig"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1803.05071v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05071v1", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.05071v1", "published_time": "3/13/2018", "rawpid": "1803.05071", "tags": ["cs.CL"], "title": "Neural Lattice Language Models"}, {"abstract": "The group affect or emotion in an image of people can be inferred by\nextracting features about both the people in the picture and the overall makeup\nof the scene. The state-of-the-art on this problem investigates a combination\nof facial features, scene extraction and even audio tonality. This paper\ncombines three additional modalities, namely, human pose, text-based tagging\nand CNN extracted features / predictions. To the best of our knowledge, this is\nthe first time all of the modalities were extracted using deep neural networks.\nWe evaluate the performance of our approach against baselines and identify\ninsights throughout this paper.", "authors": ["Ashok Sundaresan", "Sugumar Murugesan", "Sean Davis", "Karthik Kappaganthu", "ZhongYi Jin", "Divya Jain", "Anurag Maunder"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1803.05070v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05070v1", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.05070v1", "published_time": "3/13/2018", "rawpid": "1803.05070", "tags": ["cs.CV", "cs.LG", "stat.ML"], "title": "A Multi-Modal Approach to Infer Image Affect"}, {"abstract": "Popular generative model learning methods such as Generative Adversarial\nNetworks (GANs), and Variational Autoencoders (VAE) enforce the latent\nrepresentation to follow simple distributions such as isotropic Gaussian. In\nthis paper, we argue that learning a complicated distribution over the latent\nspace of an auto-encoder enables more accurate modeling of complicated data\ndistributions. Based on this observation, we propose a two stage optimization\nprocedure which maximizes an approximate implicit density model. We\nexperimentally verify that our method outperforms GANs and VAEs on two image\ndatasets (MNIST, CELEB-A). We also show that our approach is amenable to\nlearning generative model for sequential data, by learning to generate speech\nand music.", "authors": ["Cem Subakan", "Oluwasanmi Koyejo", "Paris Smaragdis"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1803.04357v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.04357v2", "num_discussion": 0, "originally_published_time": "3/12/2018", "pid": "1803.04357v2", "published_time": "3/13/2018", "rawpid": "1803.04357", "tags": ["cs.LG", "cs.NE"], "title": "Learning the Base Distribution in Implicit Generative Models"}, {"abstract": "Image identification is one of the most challenging tasks in different areas\nof computer vision. Scale-invariant feature transform is an algorithm to detect\nand describe local features in images to further use them as an image matching\ncriteria. In this paper, the performance of the SIFT matching algorithm against\nvarious image distortions such as rotation, scaling, fisheye and motion\ndistortion are evaluated and false and true positive rates for a large number\nof image pairs are calculated and presented. We also evaluate the distribution\nof the matched keypoint orientation difference for each image deformation.", "authors": ["Ebrahim Karami", "Mohamed Shehata", "Andrew Smith"], "category": "cs.CV", "comment": "4 pages, 11 figures, In Proceedings of the 2015 Newfoundland\n  Electrical and Computer Engineering C...", "img": "/static/thumbs/1710.02728v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.02728v2", "num_discussion": 0, "originally_published_time": "10/7/2017", "pid": "1710.02728v2", "published_time": "3/13/2018", "rawpid": "1710.02728", "tags": ["cs.CV"], "title": "Image Identification Using SIFT Algorithm: Performance Analysis against\n  Different Image Deformations"}, {"abstract": "Background music in social interaction settings can hinder conversation. Yet,\nlittle is known of how specific properties of music impact speech processing.\nThis paper addresses this knowledge gap by investigating 1) whether the masking\neffect of background music with lyrics is larger than that of music without\nlyrics, and 2) whether the masking effect is larger for more complex music. To\nanswer these questions, a word identification experiment was run in which Dutch\nparticipants listened to Dutch CVC words embedded in stretches of background\nmusic in two conditions, with and without lyrics, and at three SNRs. Three\nsongs were used of different genres and complexities. Music stretches with and\nwithout lyrics were sampled from the same song in order to control for factors\nbeyond the presence of lyrics. The results showed a clear negative impact of\nthe presence of lyrics in background music on spoken-word recognition. This\nimpact is independent of complexity. The results suggest that social spaces\n(e.g., restaurants, caf\\\u0027es and bars) should make careful choices of music to\npromote conversation, and open a path for future work.", "authors": ["Odette Scharenborg", "Martha Larson"], "category": "cs.SD", "comment": "Preliminary study", "img": "/static/thumbs/1803.05058v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05058v1", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.05058v1", "published_time": "3/13/2018", "rawpid": "1803.05058", "tags": ["cs.SD", "cs.CL", "eess.AS"], "title": "Investigating the Effect of Music and Lyrics on Spoken-Word Recognition"}, {"abstract": "Many practical environments contain catastrophic states that an optimal agent\nwould visit infrequently or never. Even on toy problems, Deep Reinforcement\nLearning (DRL) agents tend to periodically revisit these states upon forgetting\ntheir existence under a new policy. We introduce intrinsic fear (IF), a learned\nreward shaping that guards DRL agents against periodic catastrophes. IF agents\npossess a fear model trained to predict the probability of imminent\ncatastrophe. This score is then used to penalize the Q-learning objective. Our\ntheoretical analysis bounds the reduction in average return due to learning on\nthe perturbed objective. We also prove robustness to classification errors. As\na bonus, IF models tend to learn faster, owing to reward shaping. Experiments\ndemonstrate that intrinsic-fear DQNs solve otherwise pathological environments\nand improve on several Atari games.", "authors": ["Zachary C. Lipton", "Kamyar Azizzadenesheli", "Abhishek Kumar", "Lihong Li", "Jianfeng Gao", "Li Deng"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1611.01211v8.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.01211v8", "num_discussion": 0, "originally_published_time": "11/3/2016", "pid": "1611.01211v8", "published_time": "3/13/2018", "rawpid": "1611.01211", "tags": ["cs.LG", "cs.NE", "stat.ML"], "title": "Combating Reinforcement Learning\u0027s Sisyphean Curse with Intrinsic Fear"}, {"abstract": "Fractal AI is a theory for general artificial intelligence. It allows to\nderive new mathematical tools that constitute the foundations for a new kind of\nstochastic calculus, by modelling information using cellular automaton-like\nstructures instead of smooth functions.\n  In the repository included we are presenting a new Agent, derived from the\nfirst principles of the theory, which is capable of solving Atari games several\norders of magnitude more efficiently than other similar techniques, like Monte\nCarlo Tree Search.\n  The code provided shows how it is now possible to beat some of the current\nstate of the art benchmarks on Atari games, without previous learning and using\nless than 1000 samples to calculate each one of the actions when standard MCTS\nuses 3 Million samples. Among other things, Fractal AI makes it possible to\ngenerate a huge database of top performing examples with very little amount of\ncomputation required, transforming Reinforcement Learning into a supervised\nproblem.\n  The algorithm presented is capable of solving the exploration vs exploitation\ndilemma on both the discrete and continuous cases, while maintaining control\nover any aspect of the behavior of the Agent. From a general approach, new\ntechniques presented here have direct applications to other areas such as:\nNon-equilibrium thermodynamics, chemistry, quantum physics, economics,\ninformation theory, and non-linear control theory.", "authors": ["Sergio Hernandez Cerezo", "Guillem Duran Ballester"], "category": "cs.AI", "comment": "55 pages, python code on https://github.com/FragileTheory/FractalAI", "img": "/static/thumbs/1803.05049v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05049v1", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.05049v1", "published_time": "3/13/2018", "rawpid": "1803.05049", "tags": ["cs.AI"], "title": "Fractal AI: A fragile theory of intelligence"}, {"abstract": "The Variational Autoencoder (VAE) has proven to be an effective model for\nproducing semantically meaningful latent representations for natural data.\nHowever, it has thus far seen limited application to sequential data, and, as\nwe demonstrate, existing recurrent VAE models have difficulty modeling\nsequences with long-term structure. To address this issue, we propose the use\nof a hierarchical decoder, which first outputs embeddings for subsequences of\nthe input and then uses these embeddings to generate each subsequence\nindependently. This structure encourages the model to utilize its latent code,\nthereby avoiding the \"posterior collapse\" problem which remains an issue for\nrecurrent VAEs. We apply this architecture to modeling sequences of musical\nnotes and find that it exhibits dramatically better sampling, interpolation,\nand reconstruction performance than a \"flat\" baseline model. An implementation\nof our \"MusicVAE\" is available online at http://g.co/magenta/musicvae-colab.", "authors": ["Adam Roberts", "Jesse Engel", "Colin Raffel", "Curtis Hawthorne", "Douglas Eck"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1803.05428v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05428v1", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.05428v1", "published_time": "3/13/2018", "rawpid": "1803.05428", "tags": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "title": "A Hierarchical Latent Vector Model for Learning Long-Term Structure in\n  Music"}, {"abstract": "This paper explores the use of Visual Saliency to Classify Age, Gender and\nFacial Expression for Facial Images. For multi-task classification, we propose\nour method VEGAC, which is based on Visual Saliency. Using the Deep Multi-level\nNetwork [1] and off-the-shelf face detector [2], our proposed method first\ndetects the face in the test image and extracts the CNN predictions on the\ncropped face. The CNN of VEGAC were fine-tuned on the collected dataset from\ndifferent benchmarks. Our convolutional neural network (CNN) uses the VGG-16\narchitecture [3] and is pre-trained on ImageNet for image classification. We\ndemonstrate the usefulness of our method for Age Estimation, Gender\nClassification, and Facial Expression Classification. We show that we obtain\nthe competitive result with our method on selected benchmarks. All our models\nand code will be publically available.", "authors": ["Ayesha Gurnani", "Vandit Gajjar", "Viraj Mavani", "Yash Khandhediya"], "category": "cs.CV", "comment": "10 Pages, 4 Figures, 7 Tables; Submitted to AMFG Workshop in\n  Conjunction with CVPR-2018", "img": "/static/thumbs/1803.05719v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05719v1", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.05719v1", "published_time": "3/13/2018", "rawpid": "1803.05719", "tags": ["cs.CV"], "title": "VEGAC: Visual Saliency-based Age, Gender, and Facial Expression\n  Classification Using Convolutional Neural Networks"}, {"abstract": "Recent advances in policy gradient methods and deep learning have\ndemonstrated their applicability for complex reinforcement learning problems.\nHowever, the variance of the performance gradient estimates obtained from the\nsimulation is often excessive, leading to poor sample efficiency. In this\npaper, we apply the stochastic variance reduced gradient descent (SVRG) to\nmodel-free policy gradient to significantly improve the sample-efficiency. The\nSVRG estimation is incorporated into a trust-region Newton conjugate gradient\nframework for the policy optimization. On several Mujoco tasks, our method\nachieves significantly better performance compared to the state-of-the-art\nmodel-free policy gradient methods in robotic continuous control such as trust\nregion policy optimization (TRPO)", "authors": ["Tianbing Xu", "Qiang Liu", "Jian Peng"], "category": "cs.LG", "comment": "8 pages, 3 figures", "img": "/static/thumbs/1710.06034v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.06034v2", "num_discussion": 0, "originally_published_time": "10/17/2017", "pid": "1710.06034v2", "published_time": "3/13/2018", "rawpid": "1710.06034", "tags": ["cs.LG", "stat.ML"], "title": "Stochastic Variance Reduction for Policy Gradient Estimation"}, {"abstract": "Generative adversarial networks are used to generate images but still their\nconvergence properties are not well understood. There have been a few studies\nwho intended to investigate the stability properties of GANs as a dynamical\nsystem. This short writing can be seen in that direction. Among the proposed\nmethods for stabilizing training of GANs, {\\ss}-GAN was the first who proposed\na complete annealing strategy to change high-level conditions of the GAN\nobjective. In this note, we show by a simple example how annealing strategy\nworks in GANs. The theoretical analysis is supported by simple simulations.", "authors": ["Arash Mehrjou"], "category": "stat.ML", "comment": "5 pages", "img": "/static/thumbs/1803.05045v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05045v1", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.05045v1", "published_time": "3/13/2018", "rawpid": "1803.05045", "tags": ["stat.ML", "cs.LG"], "title": "Analysis of Nonautonomous Adversarial Systems"}, {"abstract": "The performance of off-policy learning, including deep Q-learning and deep\ndeterministic policy gradient (DDPG), critically depends on the choice of the\nexploration policy. Existing exploration methods are mostly based on adding\nnoise to the on-going actor policy and can only explore \\emph{local} regions\nclose to what the actor policy dictates. In this work, we develop a simple\nmeta-policy gradient algorithm that allows us to adaptively learn the\nexploration policy in DDPG. Our algorithm allows us to train flexible\nexploration behaviors that are independent of the actor policy, yielding a\n\\emph{global exploration} that significantly speeds up the learning process.\nWith an extensive study, we show that our method significantly improves the\nsample-efficiency of DDPG on a variety of reinforcement learning tasks.", "authors": ["Tianbing Xu", "Qiang Liu", "Liang Zhao", "Wei Xu", "Jian Peng"], "category": "cs.LG", "comment": "10 pages", "img": "/static/thumbs/1803.05044v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05044v1", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.05044v1", "published_time": "3/13/2018", "rawpid": "1803.05044", "tags": ["cs.LG", "cs.AI"], "title": "Learning to Explore with Meta-Policy Gradient"}, {"abstract": "Zero-inflated datasets, which have an excess of zero outputs, are commonly\nencountered in problems such as climate or rare event modelling. Conventional\nmachine learning approaches tend to overestimate the non-zeros leading to poor\nperformance. We propose a novel model family of zero-inflated Gaussian\nprocesses (ZiGP) for such zero-inflated datasets, produced by sparse kernels\nthrough learning a latent probit Gaussian process that can zero out kernel rows\nand columns whenever the signal is absent. The ZiGPs are particularly useful\nfor making the powerful Gaussian process networks more interpretable. We\nintroduce sparse GP networks where variable-order latent modelling is achieved\nthrough sparse mixing signals. We derive the non-trivial stochastic variational\ninference tractably for scalable learning of the sparse kernels in both models.\nThe novel output-sparse approach improves both prediction of zero-inflated data\nand interpretability of latent mixing models.", "authors": ["Pashupati Hegde", "Markus Heinonen", "Samuel Kaski"], "category": "stat.ML", "comment": "10 pages, 8 pages appendix", "img": "/static/thumbs/1803.05036v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05036v1", "num_discussion": 0, "originally_published_time": "3/13/2018", "pid": "1803.05036v1", "published_time": "3/13/2018", "rawpid": "1803.05036", "tags": ["stat.ML"], "title": "Variational zero-inflated Gaussian processes with sparse kernels"}, {"abstract": "Thanks to the advances in the technology of low-cost digital cameras and the\npopularity of the self-recording culture, the amount of visual data on the\nInternet is going to the opposite side of the available time and patience of\nthe users. Thus, most of the uploaded videos are doomed to be forgotten and\nunwatched in a computer folder or website. In this work, we address the problem\nof creating smooth fast-forward videos without losing the relevant content. We\npresent a new adaptive frame selection formulated as a weighted minimum\nreconstruction problem, which combined with a smoothing frame transition method\naccelerates first-person videos emphasizing the relevant segments and avoids\nvisual discontinuities. The experiments show that our method is able to\nfast-forward videos to retain as much relevant information and smoothness as\nthe state-of-the-art techniques in less time. We also present a new 80-hour\nmultimodal (RGB-D, IMU, and GPS) dataset of first-person videos with\nannotations for recorder profile, frame scene, activities, interaction, and\nattention.", "authors": ["Michel Melo Silva", "Washington Luis Souza Ramos", "Joao Klock Ferreira", "Felipe Cadar Chamone", "Mario Fernando Montenegro Campos", "Erickson Rangel Nascimento"], "category": "cs.CV", "comment": "Accepted for publication in the IEEE Conference on Computer Vision\n  and Pattern Recognition (CVPR) ...", "img": "/static/thumbs/1802.08722v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.08722v3", "num_discussion": 0, "originally_published_time": "2/23/2018", "pid": "1802.08722v3", "published_time": "3/13/2018", "rawpid": "1802.08722", "tags": ["cs.CV"], "title": "A Weighted Sparse Sampling and Smoothing Frame Transition Approach for\n  Semantic Fast-Forward First-Person Videos"}];
var pid_to_users = {};
var msg = "Showing most recent Arxiv papers:";
var render_format = "recent";
var username = "";
var numresults = "43477";
var show_prompt = "no";

var urlq = ''; // global will be read in to QueryString when load is done

// when page loads...
$(document).ready(function(){

	urlq = QueryString.q;

  // display message, if any
  if(msg !== '') { d3.select("#rtable").append('div').classed('msg', true).html(msg); }

  // add papers to #rtable
	var done = addPapers(10, false);
  if(done) { $("#loadmorebtn").hide(); }

  // set up inifinite scrolling for adding more papers
  $(window).on('scroll', function(){
    var scrollTop = $(document).scrollTop();
    var windowHeight = $(window).height();
    var bodyHeight = $(document).height() - windowHeight;
    var scrollPercentage = (scrollTop / bodyHeight);
    if(scrollPercentage > 0.9) {
      var done = addPapers(5, true);
      if(done) { $("#loadmorebtn").hide(); }
    }
  });

  // just in case scrolling is broken somehow, provide a button handler explicit
  $("#loadmorebtn").on('click', function(){
    var done = addPapers(5, true);
    if(done) { $("#loadmorebtn").hide(); }
  });

  if(papers.length === 0) { $("#loadmorebtn").hide(); }

	if(!(typeof urlq == 'undefined')) {
		d3.select("#qfield").attr('value', urlq.replace(/\+/g, " "));
	}

  var vf = QueryString.vfilter; if(typeof vf === 'undefined') { vf = 'all'; }
  var tf = QueryString.timefilter; if(typeof tf === 'undefined') { tf = 'week'; }
  var link_endpoint = '/';
  if(render_format === 'recent') { link_endpoint = ''; }
  if(render_format === 'top') { link_endpoint = 'top'; }
  if(render_format === 'recommend') { link_endpoint = 'recommend'; }
  if(render_format === 'friends') { link_endpoint = 'friends'; }
  if(render_format === 'toptwtr') { link_endpoint = 'toptwtr'; }
  if(render_format === 'discussions') { link_endpoint = 'discussions'; }

  var time_ranges = ['day', '3days', 'week', 'month', 'year', 'alltime'];
  var time_txt = {'day':'Last day', '3days': 'Last 3 days', 'week': 'Last week', 'month': 'Last month', 'year': 'Last year', 'alltime': 'All time'}
  var time_range = tf;

  // set up time filtering options
  if(render_format === 'recommend' || render_format === 'top' || render_format === 'recent' || render_format === 'friends') {
    // insert version filtering options for these views
    var elt = d3.select('#recommend-time-choice');
    var vflink = vf === 'all' ? '1' : 'all'; // toggle only showing v1 or not
    if(render_format === 'recent') {
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'&vfilter='+vflink); // leave out timefilter from this page
    } else {
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range+'&vfilter='+vflink);
    }
    var delt = aelt.append('div').classed('vchoice', true).html('Only show v1');
    if(vf === '1') { delt.classed('vchoice-selected', true); }
  }

  // time choices for recommend/top
  if(render_format === 'recommend' || render_format === 'top' || render_format === 'friends') {
    // insert time filtering options for these two views
    var elt = d3.select('#recommend-time-choice');
    elt.append('div').classed('fdivider', true).html('|');
    for(var i=0;i<time_ranges.length;i++) {
      var time_range = time_ranges[i];
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range+'&vfilter='+vf);
      var delt = aelt.append('div').classed('timechoice', true).html(time_txt[time_range]);
      if(tf == time_range) { delt.classed('timechoice-selected', true); } // also render as chosen
    }
  }

  // time choices for top tweets
  if(render_format === 'toptwtr') {
    var tf = QueryString.timefilter; if(typeof tf === 'undefined') { tf = 'day'; } // default here is day
    var time_ranges = ['day', 'week', 'month'];
    var elt = d3.select('#recommend-time-choice');
    for(var i=0;i<time_ranges.length;i++) {
      var time_range = time_ranges[i];
      var aelt = elt.append('a').attr('href', '/'+link_endpoint+'?'+'timefilter='+time_range);
      var delt = aelt.append('div').classed('timechoice', true).html(time_txt[time_range]);
      if(tf == time_range) { delt.classed('timechoice-selected', true); } // also render as chosen
    }
  }

  var xb = $("#xbanner");
  if(xb.length !== 0) {
    xb.click(function(){ $("#banner").slideUp('fast'); })
  }

  // in top tab: color current choice
  if( render_format === 'recent') { d3.select('#tabrecent').classed('tab-selected', true); }
  if( render_format === 'top') { d3.select('#tabtop').classed('tab-selected', true); }
  if( render_format === 'toptwtr') { d3.select('#tabtwtr').classed('tab-selected', true); }
  if( render_format === 'friends') { d3.select('#tabfriends').classed('tab-selected', true); }
  if( render_format === 'discussions') { d3.select('#tabdiscussions').classed('tab-selected', true); }
  if( render_format === 'recommend') { d3.select('#tabrec').classed('tab-selected', true); }
  if( render_format === 'library') { d3.select('#tablib').classed('tab-selected', true); }

  $("#goaway").on('click', function(){
    $("#prompt").slideUp('fast');
    $.post("/goaway", {}).done(function(data){ });
  });
});

</script>
</head>

<body>
<a href="https://github.com/karpathy/arxiv-sanity-preserver"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

<div id ="titdiv">

  <!-- User account information on top right -->
  <div id="userinfo">
    
    <form action="/login" method="post">
      User:
      <input type="text" name="username" class="input-no-border">
      Pass:
      <input type="password" name="password" class="input-no-border">
      <input type="submit" value="Login or Create" class="btn-fancy">
    </form>
    
  </div>

  <!-- Site information/banner on top left -->
	<a href="/">
	<div id="tittxt">
		<h1>Arxiv Sanity Preserver</h1>
		Built in spare time by <a href="https://twitter.com/karpathy">@karpathy</a> to accelerate research.<br>
		Serving last 43477 papers from cs.[CV|CL|LG|AI|NE]/stat.ML
	</div>
	</a>
</div>

<div id="flashesdiv">

    

</div>


<div id="banner">
  <div style="float:right;cursor:pointer;" id="xbanner">X</div>
  New to arxiv-sanity? Check out the <a href="https://youtu.be/S2GY3gh6qC8" target="_blank">introduction video</a>.
</div>


<div id="sbox">
  <form action="/search" method="get">
  	<input name="q" type="text" id="qfield">
  </form>
  <div id="search_hint"></div>
</div>



<div id="pagebar">
  <div class="pagelink" id="tabrecent"><a href="/">most recent</a></div>
  <div class="pagelink" id="tabtop"><a href="/top">top recent</a></div>
  <div class="pagelink" id="tabtwtr"><a href="/toptwtr">top hype</a></div>
  <div class="pagelink" id="tabfriends"><a href="/friends">friends</a></div>
  <div class="pagelink" id="tabdiscussions"><a href="/discussions">discussions</a></div>
  <div class="pagelink" id="tabrec"><a href="/recommend">recommended</a></div>
  <div class="pagelink" id="tablib"><a href="/library">library</a></div>
</div>

<!-- this div will be rendered into dynamcially at init with JS -->
<div id="recommend-time-choice" class="centerdiv"></div>

<div id="maindiv">

<div id="rtable"></div>

<div id="loadmore">
  <button id="loadmorebtn">Load more</button>
</div>

</div>

<br><br><br><br><br><br>
</body>

</html>