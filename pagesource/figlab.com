<!doctype html>
<html xmlns:og="http://opengraphprotocol.org/schema/" xmlns:fb="http://www.facebook.com/2008/fbml" xmlns:website="http://ogp.me/ns/website" lang="en-US" itemscope itemtype="http://schema.org/WebPage" >

<head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  
  <meta name="viewport" content="width=device-width,initial-scale=1">
  
  <!-- This is Squarespace. --><!-- figlab -->
<base href="">
<meta charset="utf-8" />
<title>Future Interfaces Group</title>
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico"/>
<link rel="canonical" href="http://www.figlab.com/"/>
<meta property="og:site_name" content="Future Interfaces Group"/>
<meta property="og:title" content="Research"/>
<meta property="og:url" content="http://www.figlab.com/"/>
<meta property="og:type" content="website"/>
<meta property="og:image" content="http://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1450454441649/figlogo_letterhead_gray.png?format=1000w"/>
<meta property="og:image:width" content="805"/>
<meta property="og:image:height" content="300"/>
<meta itemprop="name" content="Research"/>
<meta itemprop="url" content="http://www.figlab.com/"/>
<meta itemprop="thumbnailUrl" content="http://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1450454441649/figlogo_letterhead_gray.png?format=1000w"/>
<link rel="image_src" href="http://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1450454441649/figlogo_letterhead_gray.png?format=1000w" />
<meta itemprop="image" content="http://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1450454441649/figlogo_letterhead_gray.png?format=1000w"/>
<meta name="twitter:title" content="Research"/>
<meta name="twitter:image" content="http://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1450454441649/figlogo_letterhead_gray.png?format=1000w"/>
<meta name="twitter:url" content="http://www.figlab.com/"/>
<meta name="twitter:card" content="summary"/>
<meta name="description" content="" />
<script type="text/javascript" src="//use.typekit.net/ik/3AS4yc83T8EMl8_b1d6UujCy8nQO0PeJtz9486tZ3GIfenG2fFHN4UJLFRbh52jhWD9hFeJuwQMaZQsKw26a52boF29kZAIXjyTQiaiaOcZTZhUyjKoRdhXCZc81deBKOcFzdPUCdhFydeyzSabCZc81deBKO1FUiABkZWF3jAF8OcFzdP37OcZTZhUyjKoDSWmyScmDSeBRZPoRdhXK2YgkdayTdAIldcNhjPJ4Z1mXiW4yOWgXH6qJ73IbMg6gJMJ7fbKCMsMMeMC6MKG4f5J7IMMjMkMfH6qJtkGbMg6FJMJ7fbKzMsMMeMb6MKG4fOMgIMMj2KMfH6GJCwbgIMMjgPMfH6GJCSbgIMMj2kMfH6qJnbIbMg6eJMJ7fbK0MsMMegM6MKG4fJCgIMMjgkMfH6qJRMIbMg6sJMJ7fbKTMsMMeM66MKG4fHGgIMMjIKMfH6qJKbIbMg64JMJ7fbKHMsMMegw6MKG4fJZmIMIjMkMfH6qJ6u9bMs6FJMJ7fbKImsMgeMb6MKG4fJmmIMIj2KMfH6qJxubbMs6BJMJ7fbKMmsMgeMv6MKG4fJBmIMIjgkMfH6qJ689bMs6sJMJ7fbKYmsMgeM66MKG4fJymIMIjIKMfqMYBTirXgb.js"></script>
<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
<script type="text/javascript">SQUARESPACE_ROLLUPS = {};</script>
<script>(function(rollups, name) { if (!rollups[name]) { rollups[name] = {}; } rollups[name].js = ["//static.squarespace.com/universal/scripts-compressed/common-c65675cff5476772de69-min.en-US.js"]; })(SQUARESPACE_ROLLUPS, 'squarespace-common');</script>
<script crossorigin="anonymous" src="//static.squarespace.com/universal/scripts-compressed/common-c65675cff5476772de69-min.en-US.js" ></script><script data-name="static-context">Static = window.Static || {}; Static.SQUARESPACE_CONTEXT = {"facebookAppId":"314192535267336","rollups":{"squarespace-announcement-bar":{"css":"//static.squarespace.com/universal/styles-compressed/announcement-bar-d41d8cd98f00b204e9800998ecf8427e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/announcement-bar-a630df7af3b4c16afd8d-min.en-US.js"},"squarespace-audio-player":{"css":"//static.squarespace.com/universal/styles-compressed/audio-player-6c42d60d26f4e09ab1ac335b4bc55b7e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/audio-player-0f389a0ca3c62ef4dc26-min.en-US.js"},"squarespace-blog-collection-list":{"css":"//static.squarespace.com/universal/styles-compressed/blog-collection-list-d41d8cd98f00b204e9800998ecf8427e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/blog-collection-list-8dd1834c9b90222a0fd0-min.en-US.js"},"squarespace-calendar-block-renderer":{"css":"//static.squarespace.com/universal/styles-compressed/calendar-block-renderer-56713b1663346e1fe5b31a3adb8f31fc-min.css","js":"//static.squarespace.com/universal/scripts-compressed/calendar-block-renderer-ad4def75735babd8721b-min.en-US.js"},"squarespace-chartjs-helpers":{"css":"//static.squarespace.com/universal/styles-compressed/chartjs-helpers-9935a41d63cf08ca108505d288c1712e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/chartjs-helpers-3aa476d98d6190fffea7-min.en-US.js"},"squarespace-comments":{"css":"//static.squarespace.com/universal/styles-compressed/comments-c32adaffffa61675d5f238e2ea399b2b-min.css","js":"//static.squarespace.com/universal/scripts-compressed/comments-d1114c43cd239f5273fb-min.en-US.js"},"squarespace-dialog":{"css":"//static.squarespace.com/universal/styles-compressed/dialog-a5f874591a318ca818816c527ef525b3-min.css","js":"//static.squarespace.com/universal/scripts-compressed/dialog-97fd9d514b092231474c-min.en-US.js"},"squarespace-events-collection":{"css":"//static.squarespace.com/universal/styles-compressed/events-collection-56713b1663346e1fe5b31a3adb8f31fc-min.css","js":"//static.squarespace.com/universal/scripts-compressed/events-collection-11f0600e899a394547a2-min.en-US.js"},"squarespace-form-rendering-utils":{"js":"//static.squarespace.com/universal/scripts-compressed/form-rendering-utils-d2caaa218d7bbd716311-min.en-US.js"},"squarespace-gallery-collection-list":{"css":"//static.squarespace.com/universal/styles-compressed/gallery-collection-list-d41d8cd98f00b204e9800998ecf8427e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/gallery-collection-list-e02b7618282c52497d9b-min.en-US.js"},"squarespace-image-zoom":{"css":"//static.squarespace.com/universal/styles-compressed/image-zoom-ae974921915aeccaff8ad60c60e19c31-min.css","js":"//static.squarespace.com/universal/scripts-compressed/image-zoom-595732587c981d3b80ee-min.en-US.js"},"squarespace-pinterest":{"css":"//static.squarespace.com/universal/styles-compressed/pinterest-d41d8cd98f00b204e9800998ecf8427e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/pinterest-755ccfaa56dd12bff0f9-min.en-US.js"},"squarespace-popup-overlay":{"css":"//static.squarespace.com/universal/styles-compressed/popup-overlay-6b891e5f689f032ce65af5855c067b4f-min.css","js":"//static.squarespace.com/universal/scripts-compressed/popup-overlay-7bfe1b2ddaff2136f9cd-min.en-US.js"},"squarespace-product-quick-view":{"css":"//static.squarespace.com/universal/styles-compressed/product-quick-view-9abaaa4dfff182aa8d4ccf3b6ffdbe8d-min.css","js":"//static.squarespace.com/universal/scripts-compressed/product-quick-view-a3ecaad00375f46008f5-min.en-US.js"},"squarespace-products-collection-item-v2":{"css":"//static.squarespace.com/universal/styles-compressed/products-collection-item-v2-ae974921915aeccaff8ad60c60e19c31-min.css","js":"//static.squarespace.com/universal/scripts-compressed/products-collection-item-v2-d0de406ebeb8803208d5-min.en-US.js"},"squarespace-products-collection-list-v2":{"css":"//static.squarespace.com/universal/styles-compressed/products-collection-list-v2-ae974921915aeccaff8ad60c60e19c31-min.css","js":"//static.squarespace.com/universal/scripts-compressed/products-collection-list-v2-c5fde9651b15bf295b50-min.en-US.js"},"squarespace-search-page":{"css":"//static.squarespace.com/universal/styles-compressed/search-page-9c747eeaabe96dacfea4932a63336f54-min.css","js":"//static.squarespace.com/universal/scripts-compressed/search-page-446616453fb982fd2668-min.en-US.js"},"squarespace-search-preview":{"js":"//static.squarespace.com/universal/scripts-compressed/search-preview-29ecb29e74ff23484866-min.en-US.js"},"squarespace-share-buttons":{"js":"//static.squarespace.com/universal/scripts-compressed/share-buttons-d8e11eaa1c332db06117-min.en-US.js"},"squarespace-simple-liking":{"css":"//static.squarespace.com/universal/styles-compressed/simple-liking-09fa291ec2800c97714f0d157fd0a6ca-min.css","js":"//static.squarespace.com/universal/scripts-compressed/simple-liking-4dcce2aee81e528ce98e-min.en-US.js"},"squarespace-social-buttons":{"css":"//static.squarespace.com/universal/styles-compressed/social-buttons-7a696232d1cd101fd62b5f174f9ae6ff-min.css","js":"//static.squarespace.com/universal/scripts-compressed/social-buttons-5bc2a12f878b80d75d5c-min.en-US.js"},"squarespace-tourdates":{"css":"//static.squarespace.com/universal/styles-compressed/tourdates-d41d8cd98f00b204e9800998ecf8427e-min.css","js":"//static.squarespace.com/universal/scripts-compressed/tourdates-6694acde8c21be28c83f-min.en-US.js"},"squarespace-website-overlays-manager":{"css":"//static.squarespace.com/universal/styles-compressed/website-overlays-manager-923311a5ffb05ecb734ddfc7d9be08bc-min.css","js":"//static.squarespace.com/universal/scripts-compressed/website-overlays-manager-1d0f660ee6b3ab347f51-min.en-US.js"}},"pageType":1,"website":{"id":"564b5107e4b087849452ea4b","identifier":"figlab","websiteType":1,"contentModifiedOn":1515097876522,"cloneable":false,"developerMode":true,"siteStatus":{},"language":"en-US","timeZone":"America/New_York","machineTimeZoneOffset":-18000000,"timeZoneOffset":-18000000,"timeZoneAbbr":"EST","siteTitle":"Future Interfaces Group","siteTagLine":"","siteDescription":"","logoImageId":"56742da94bf118e324766841","shareButtonOptions":{"3":true,"8":true,"1":true,"4":true,"7":true,"2":true,"6":true,"5":true},"logoImageUrl":"//static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1515097876522/","authenticUrl":"http://www.figlab.com","internalUrl":"http://figlab.squarespace.com","baseUrl":"http://www.figlab.com","primaryDomain":"www.figlab.com","typekitId":"","statsMigrated":false,"imageMetadataProcessingEnabled":false,"screenshotId":"a93b0601"},"websiteSettings":{"id":"564b5107e4b087849452ea4e","websiteId":"564b5107e4b087849452ea4b","type":"Personal","subjects":[],"country":"US","state":"PA","simpleLikingEnabled":true,"mobileInfoBarSettings":{"isContactEmailEnabled":false,"isContactPhoneNumberEnabled":false,"isLocationEnabled":false,"isBusinessHoursEnabled":false},"commentLikesAllowed":true,"commentAnonAllowed":true,"commentThreaded":true,"commentApprovalRequired":false,"commentAvatarsOn":true,"commentSortType":2,"commentFlagThreshold":0,"commentFlagsAllowed":true,"commentEnableByDefault":true,"commentDisableAfterDaysDefault":0,"disqusShortname":"","commentsEnabled":false,"contactPhoneNumber":"","storeSettings":{"returnPolicy":null,"termsOfService":null,"privacyPolicy":null,"paymentSettings":{},"expressCheckout":false,"useLightCart":false,"showNoteField":false,"shippingCountryDefaultValue":"US","billToShippingDefaultValue":false,"showShippingPhoneNumber":true,"isShippingPhoneRequired":false,"showBillingPhoneNumber":true,"isBillingPhoneRequired":false,"multipleQuantityAllowedForServices":true,"currenciesSupported":["CHF","HKD","MXN","EUR","DKK","USD","CAD","MYR","NOK","THB","AUD","SGD","ILS","PLN","GBP","CZK","SEK","NZD","PHP","RUB"],"defaultCurrency":"USD","selectedCurrency":"USD","measurementStandard":1,"showCustomCheckoutForm":false,"enableMailingListOptInByDefault":true,"sameAsRetailLocation":false,"isLive":false},"useEscapeKeyToLogin":true,"ssBadgeType":1,"ssBadgePosition":4,"ssBadgeVisibility":1,"ssBadgeDevices":1,"pinterestOverlayOptions":{"mode":"disabled"},"ampEnabled":false},"cookieSettings":{"isRestrictiveCookiePolicyEnabled":false},"websiteCloneable":false,"collection":{"title":"Research","id":"564b5638e4b0cfc22513bc9c","fullUrl":"/","type":1},"subscribed":false,"appDomain":"squarespace.com","templateTweakable":true,"tweakJSON":{"TGutter":"3%","TMaxWidth":"400","TPerRow":"3","gallery-auto-play":"false","galleryPlaySpeed":"3","hide-thumbnail-titles":"false","outerPadding":"75px","pagePadding":"50px","product-gallery-auto-crop":"false","product-image-auto-crop":"true","thumbnail-layout":"Basic Grid","topPadding":"10px"},"templateId":"564b9dfce4b06621e2bb38d8","pageFeatures":[1,2,4],"impersonatedSession":false,"isFacebookTab":false,"tzData":{"zones":[[-300,"US","E%sT",null]],"rules":{"US":[[1967,2006,null,"Oct","lastSun","2:00","0","S"],[1987,2006,null,"Apr","Sun>=1","2:00","1:00","D"],[2007,"max",null,"Mar","Sun>=8","2:00","1:00","D"],[2007,"max",null,"Nov","Sun>=1","2:00","0","S"]]}},"useNewImageLoader":true};</script><script type="text/javascript"> SquarespaceFonts.loadViaContext(); Squarespace.load(window);</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="http://www.figlab.com/research?format=RSS" />
<script type="application/ld+json">{"@context":"http://schema.org","@type":"WebSite","url":"http://www.figlab.com","name":"Future Interfaces Group","description":"","image":"//static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1515097876522/"}</script><!--[if gte IE 9]> <link rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/564b5107e4b087849452ea4b/21/564b9dfce4b06621e2bb38d8/564b9dfce4b06621e2bb38ff/0-05142015/1511968426189/site.css?&filterFeatures=false&part=1"/><link rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/564b5107e4b087849452ea4b/21/564b9dfce4b06621e2bb38d8/564b9dfce4b06621e2bb38ff/0-05142015/1511968426189/site.css?&filterFeatures=false&part=2"/><link rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/564b5107e4b087849452ea4b/21/564b9dfce4b06621e2bb38d8/564b9dfce4b06621e2bb38ff/0-05142015/1511968426189/site.css?&filterFeatures=false&part=3"/><link rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/564b5107e4b087849452ea4b/21/564b9dfce4b06621e2bb38d8/564b9dfce4b06621e2bb38ff/0-05142015/1511968426189/site.css?&filterFeatures=false&part=4"/><![endif]-->
<!--[if lt IE 9]><link rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/564b5107e4b087849452ea4b/21/564b9dfce4b06621e2bb38d8/564b9dfce4b06621e2bb38ff/0-05142015/1511968426189/site.css?&filterFeatures=false&noMedia=true&part=1"/><link rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/564b5107e4b087849452ea4b/21/564b9dfce4b06621e2bb38d8/564b9dfce4b06621e2bb38ff/0-05142015/1511968426189/site.css?&filterFeatures=false&noMedia=true&part=2"/><link rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/564b5107e4b087849452ea4b/21/564b9dfce4b06621e2bb38d8/564b9dfce4b06621e2bb38ff/0-05142015/1511968426189/site.css?&filterFeatures=false&noMedia=true&part=3"/><link rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/564b5107e4b087849452ea4b/21/564b9dfce4b06621e2bb38d8/564b9dfce4b06621e2bb38ff/0-05142015/1511968426189/site.css?&filterFeatures=false&noMedia=true&part=4"/><![endif]-->
<!--[if !IE]> --><link rel="stylesheet" type="text/css" href="//static1.squarespace.com/static/sitecss/564b5107e4b087849452ea4b/21/564b9dfce4b06621e2bb38d8/564b9dfce4b06621e2bb38ff/0-05142015/1511968426189/site.css?&filterFeatures=false"/><!-- <![endif]-->
<!-- End of Squarespace Headers -->
</head>

<body class="blog-sidebar-display-auto index-thumb-title-position-below index-item-layout-auto thumbnail-layout-basic-grid thumbnails-on-open-page-show-all   layout-style-left page-borders-none underline-body-links gallery-page-controls-tiny-thumbnails  social-icon-style-normal  show-category-navigation product-list-titles-under product-list-alignment-center product-item-size-11-square product-image-auto-crop product-gallery-size-11-square  show-product-price show-product-item-nav product-social-sharing  event-show-past-events event-thumbnails event-thumbnail-size-32-standard event-date-label  event-list-show-cats event-list-date event-list-time event-list-address   event-icalgcal-links  event-excerpts  event-item-back-link     hide-opentable-icons opentable-style-dark newsletter-style-dark small-button-style-solid small-button-shape-square medium-button-style-solid medium-button-shape-square large-button-style-solid large-button-shape-square button-style-solid button-corner-style-square tweak-product-quick-view-button-style-floating tweak-product-quick-view-button-position-bottom tweak-product-quick-view-lightbox-excerpt-display-truncate tweak-product-quick-view-lightbox-show-arrows tweak-product-quick-view-lightbox-show-close-button tweak-product-quick-view-lightbox-controls-weight-light native-currency-code-usd collection-564b5638e4b0cfc22513bc9c collection-type-index collection-layout-default homepage view-list mobile-style-available logo-image" id="collection-564b5638e4b0cfc22513bc9c">

  <div id="canvas">

      
  <div id="mobileNav" class="">
    <div id="mobileNavWrapper" class="wrapper">
      <nav class="main-nav mobileNav">
        <ul>
          

            
              <li class="index-collection active-link">
                
                  <a href="/">Research</a>
                
              </li>
            

          

            
              <li class="page-collection">
                
                  <a href="/about-us/">About Us</a>
                
                
              </li>
            

          

            
              <li class="page-collection">
                
                  <a href="/sponsors/">Sponsors</a>
                
                
              </li>
            

          

            
              <li class="page-collection">
                
                  <a href="/facilities/">Contact</a>
                
                
              </li>
            

          
        </ul>
      </nav>
    </div>
  </div>
  <div id="mobileMenuLink"><a>Menu</a></div>





    <header id="header">
      <div id="logo" data-content-field="site-title">
        
        <h1 class="logo"><a href="/"><img src="//static1.squarespace.com/static/564b5107e4b087849452ea4b/t/56742da94bf118e324766841/1515097876522/?format=1000w" alt="Future Interfaces Group" /></a></h1>
        
        
      </div>
      <script>
        Y.use('squarespace-ui-base', function(Y) {
          Y.all("#header .logo, #header .logo-subtitle").each(function (text) {
            text.plug(Y.Squarespace.TextShrink, {
              parentEl: Y.one('#header'),
              triggerWidth: 750
            });
          });
        });

        // Show spinner on page load if loading bookmarked url
        if (window.location.hash && window.location.hash !== '#') {
          document.querySelector('body').className += ' index-loading';
        }
      </script>
      <div id="topNav" data-content-field="navigation">
<nav class="main-nav"><ul>
  

      <li class="index-collection active-link">

        
          
            <a href="/">Research</a>
          
        

      </li>

  

      <li class="page-collection">

        

          
            <a href="/about-us/">About Us</a>
          

          


        

      </li>

  

      <li class="page-collection">

        

          
            <a href="/sponsors/">Sponsors</a>
          

          


        

      </li>

  

      <li class="page-collection">

        

          
            <a href="/facilities/">Contact</a>
          

          


        

      </li>

  
</ul>
</nav>
</div>

    </header>

    <div class="page-divider"></div>

    <div class="extra-wrapper page-header">
      <div class="sqs-layout sqs-grid-12 columns-12" data-layout-label="Header Content: Research" data-type="block-field" data-updated-on="1515096097490" id="page-header-564b5638e4b0cfc22513bc9c"><div class="row sqs-row"><div class="col sqs-col-12 span-12"><div class="sqs-block html-block sqs-block-html" data-block-type="2" id="block-db0dcc93556dbb1eb264"><div class="sqs-block-content"><h2>The Future Interfaces Group is an interdisciplinary research lab within the <a target="_blank" href="http://hcii.cmu.edu">Human-Computer Interaction Institute</a> at <a target="_blank" href="http://cmu.edu">Carnegie Mellon University</a>.&nbsp;We create new sensing and interface technologies that foster powerful and delightful interactions between humans and computers. These efforts often lie in emerging use modalities, such as wearable computing, augmented reality, smart environments and gestural interfaces.</h2></div></div></div></div></div>
    </div>

    <section id="page" role="main" data-content-field="main-content">
      <!-- CATEGORY NAV -->
      
      <div id="projectPages" data-collection-id="">
  
    
      <div class="project gallery-project" data-url="/desktopography/" >
      
        <div class="project-meta">
          <h2 class="project-title">Desktopography (2017)</h2>
          <div class="project-description"><p>Systems for providing mixed physical-virtual interaction on desktop surfaces have been proposed for decades, though no such systems have achieved widespread use. One major factor contributing to this lack of acceptance may be that these systems are not designed for the variety and complexity of actual work surfaces, which are often in flux and cluttered with physical objects. In this project, we use an elicitation study and interviews to synthesize a list of ten interactive behaviors that desk-bound, digital interfaces should implement to support responsive cohabitation with physical objects. As a proof of concept, we implemented these interactive behaviors in a working augmented desk system, demonstrating their imminent feasibility.</p><p><a href="/s/desktopography.pdf">Download Paper</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/L5mCxfjk6hc?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;&gt;
&lt;/iframe&gt;" data-provider-name="YouTube"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/5a1ecb9ee4966b13429fe985/5a1eccc08165f542d646844a/1511968115190/Screen+Shot+2017-11-29+at+10.02.39+AM.png" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/5a1ecb9ee4966b13429fe985/5a1eccc08165f542d646844a/1511968115190/Screen+Shot+2017-11-29+at+10.02.39+AM.png" data-image-dimensions="1761x805" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              <span class="image-title"><strong>Desktopography: Supporting Responsive Cohabitation Between Virtual Interfaces and Physical Objects</strong></span>
              <span class="image-desc"><p>Xiao, R., Hudson, S.E. and Harrison, C. 2017. Supporting Responsive Cohabitation Between Virtual Interfaces and Physical Objects on Everyday Surfaces. In Proceedings of the 9th ACM SIGCHI Symposium on Engineering Interactive Computing Systems (Lisbon, Portugal, June 26 – 29, 2017). EICS ’17. ACM, New York, NY. Article 11.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/synthetic-sensors-2017/" >
      
        <div class="project-meta">
          <h2 class="project-title">Synthetic Sensors (2017)</h2>
          <div class="project-description"><p>The promise of smart environments and the Internet of Things (IoT) relies on robust sensing of diverse environmental facets.&nbsp;In this work, we explore the notion of general-purpose sensing, wherein a single, highly capable sensor can indirectly monitor a large context, without direct instrumentation of objects. Further, through what we call Synthetic Sensors, we can virtualize raw sensor data into actionable feeds, whilst simultaneously mitigating immediate privacy issues. We deployed our system across many months and environments, the results of which show the versatility, accuracy and potential of this approach.</p><p><a href="/s/syntheticsensors.pdf">Download Paper</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/aqbKrrru2co?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;&gt;
&lt;/iframe&gt;" data-provider-name="YouTube"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/591b664903596e76396b5c69/591b665e9f7456f0991c4d60/1494967968936/00_a_hardware_closeup.jpg" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/591b664903596e76396b5c69/591b665e9f7456f0991c4d60/1494967968936/00_a_hardware_closeup.jpg" data-image-dimensions="2500x1667" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Laput, G., Zhang, Y. and Harrison, C. 2017. Synthetic Sensors: Towards General-Purpose Sensing. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 3986-3999.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/electrick-2017/" >
      
        <div class="project-meta">
          <h2 class="project-title">Electrick (2017)</h2>
          <div class="project-description"><p>Electrick is a low-cost and versatile sensing technique that enables touch input on a wide variety of objects and surfaces, whether small or large, flat or irregular. This is achieved by using electric field tomography in concert with an electrically conductive material, which can be easily and cheaply added to objects and surfaces. We show that our technique is compatible with commonplace manufacturing methods, such as spray/brush coating, vacuum forming, and casting/molding – enabling a wide range of possible uses and outputs. Published at CHI 2017.</p><p><a href="/s/electrick.pdf">Download paper</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/38h4-5FDdV4?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;&gt;
&lt;/iframe&gt;" data-provider-name="YouTube"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/591b652e2994ca8dc9494099/591b65462e69cfc59cade40b/1495033812433/electrick2.png" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/591b652e2994ca8dc9494099/591b65462e69cfc59cade40b/1495033812433/electrick2.png" data-image-dimensions="2500x1400" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Zhang, Y., Laput, G. and Harrison, C. 2017. Electrick: Low-Cost Touch Sensing Using Electric Field Tomography. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 1-14.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/deus-em-machina-2017/" >
      
        <div class="project-meta">
          <h2 class="project-title">Deus EM Machina (2017)</h2>
          <div class="project-description"><p>Homes, offices and many other environments will be increasingly saturated with connected, computational appliances, forming the “Internet of Things” (IoT). At present, most of these devices rely on mechanical inputs, webpages, or smartphone apps for control. We propose an approach where users simply tap a smartphone to an appliance to discover and rapidly utilize contextual functionality. To achieve this, our prototype smartphone recognizes physical contact with uninstrumented appliances, and summons appliance-specific interfaces and contextually relevant functionality. Published at CHI 2017.</p><p><a href="/s/emphone.pdf">Download paper</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/eInfzdZ-9fE?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;&gt;
&lt;/iframe&gt;" data-provider-name="YouTube"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/591b646c5016e1e1e7c74f67/591b6499d1758e26255927d7/1494967509672/lightApp+copy.JPG" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/591b646c5016e1e1e7c74f67/591b6499d1758e26255927d7/1494967509672/lightApp+copy.JPG" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Xiao, R., Laput, G., Zhang, Y. and Harrison, C. 2017. Deus EM Machina: On-Touch Contextual Functionality for Smart IoT Appliances. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 4000-4008.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/direct-2016/" >
      
        <div class="project-meta">
          <h2 class="project-title">DIRECT (2016)</h2>
          <div class="project-description"><p>Many research systems have demonstrated that depth cameras, combined with projectors for output, can turn nearly any reasonably flat surface into an ad hoc, touch-sensitive display. However, even with the latest generation of depth cameras, it has been difficult to obtain sufficient sensing fidelity across a table-sized surface to get much beyond a proof-of-concept demonstration. In this research, we present DIRECT, a novel touch-tracking algorithm that merges depth and infrared imagery captured by a commodity sensor. Our results show that our technique boosts touch detection accuracy by 15% and reduces positional error by 55% compared to the next best-performing technique in the literature.</p><p><a href="/s/direct.pdf">Download paper</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/w6RtfrczmYQ?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;&gt;
&lt;/iframe&gt;" data-provider-name="YouTube"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/591b6364414fb5f242500876/591b64022e69cfc59cadcc0f/1494967387312/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/591b6364414fb5f242500876/591b64022e69cfc59cadcc0f/1494967387312/" data-image-dimensions="477x265" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Xiao, R., Hudson, S. E. and Harrison, C. 2016. DIRECT: Making Touch Tracking on Ordinary Surfaces Practical with Hybrid Depth-Infrared Sensing. In Proceedings of the 11th ACM International Conference on Interactive Surfaces and Spaces (Niagara Falls, Canada, November 6 - 9, 2016). ISS '16. ACM, New York, NY. 85-94.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/capcam-2016/" >
      
        <div class="project-meta">
          <h2 class="project-title">CapCam (2016)</h2>
          <div class="project-description"><p>We present CapCam, a novel technique that enables smartphones (and similar devices) to establish quick, ad-hoc connections with a host touchscreen device, simply by pressing a device to the screen’s surface. Pairing data, used to bootstrap a conventional wireless connection, is transmitted optically to the phone’s rear camera. This approach utilizes the near-ubiquitous rear camera on smart devices, making it applicable to a wide range of devices, both new and old. CapCam also tracks phones’ physical positions on the host capacitive touchscreen without any instrumentation, enabling a wide range of targeted and spatial interactions.&nbsp;</p><p><a href="/s/capcam.pdf">Download paper</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe scrolling=&quot;no&quot; allowfullscreen=&quot;&quot; src=&quot;//www.youtube.com/embed/9ERZaT0X-6M?wmode=opaque&amp;amp;enablejsapi=1&quot; width=&quot;854&quot; frameborder=&quot;0&quot; height=&quot;480&quot;&gt;
&lt;/iframe&gt;" data-provider-name="YouTube"><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/591b6118c534a54e1e947d12/591b6169bebafb95dd1bfc2e/1494967119478/kbd.jpg" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/591b6118c534a54e1e947d12/591b6169bebafb95dd1bfc2e/1494967119478/kbd.jpg" data-image-dimensions="2371x1200" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Xiao, R., Hudson, S. E. and Harrison, C. 2016. CapCam: Enabling Rapid, Ad-Hoc, Position-Tracked Interactions Between Devices. In Proceedings of the 11th ACM International Conference on Interactive Surfaces and Spaces (Niagara Falls, Canada, November 6 - 9, 2016). ISS '16. ACM, New York, NY. 169-178.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/viband-2016/" >
      
        <div class="project-meta">
          <h2 class="project-title">ViBand (2016)</h2>
          <div class="project-description"><p>Smartwatches and wearables are unique in that they reside on the body, presenting great potential for always-available input and interaction. Additionally, their position on the wrist makes them ideal for capturing bio-acoustic signals. We developed a custom smartwatch kernel that boosts the sampling rate of a smartwatch’s existing accelerometer, enabling many new applications. For example, we can use bio-acoustic data to classify hand gestures such as flicks, claps, scratches, and taps. Bio-acoustic sensing can also detect the vibrations of grasped mechanical or motor-powered objects, enabling object recognition. Finally, we can generate structured vibrations using a transducer, and show that data can be transmitted through the human body.</p><p><a href="/s/aurasense.pdf">Download paper</a>.</p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/Poi0MeASmuY?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/5820a8002e69cf3e25fcbf26/5820a9b56b8f5b2e55606226/1478535805547/VideoThumbnail.jpg" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/5820a8002e69cf3e25fcbf26/5820a9b56b8f5b2e55606226/1478535805547/VideoThumbnail.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Laput, G., Xiao, R. and Harrison, C. 2016. ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers. To appear in Proceedings of the 29th Annual ACM Symposium on User interface Software and Technology (Tokyo, Japan, October 16 - 19, 2016). UIST '16. ACM, New York, NY.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/aurasense-2016/" >
      
        <div class="project-meta">
          <h2 class="project-title">AuraSense (2016)</h2>
          <div class="project-description"><p>AuraSense enables rich, around-device, smartwatch interactions using electric field sensing. To explore how this sensing approach could enhance smartwatch interactions, we considered different antenna configurations and how they could enable useful interaction modalities. We identified four configurations that can support six well-known modalities of particular interest and utility, including gestures above the watchface and touchscreen-like finger tracking on the skin. We quantify the feasibility of these input modalities in a series of user studies, which suggest that AuraSense can be low latency and robust across both users and environments.&nbsp;</p><p><a href="/s/aurasense.pdf">Download paper</a>.</p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/gZGqkpuwzrA?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/5820a67e37c581a003a519cd/5820a74cd482e9a9a7024518/1478535046235/thumbnail.jpg" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/5820a67e37c581a003a519cd/5820a74cd482e9a9a7024518/1478535046235/thumbnail.jpg" data-image-dimensions="668x676" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Zhou, J., Yang, C., Laput, G., Sample, A. and Harrison, C. 2016. AuraSense: Enabling Expressive Around-Smartwatch Interactions with Electric Field Sensing. To appear in Proceedings of the 29th Annual ACM Symposium on User interface Software and Technology (Tokyo, Japan, October 16 - 19, 2016). UIST '16. ACM, New York, NY.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/tomo2/" >
      
        <div class="project-meta">
          <h2 class="project-title">Tomo 2 (2016)</h2>
          <div class="project-description"><p>Electrical Impedance Tomography (EIT) can be used to detect hand gestures using an instrumented smartwatch (see Tomo), demonstrating great promise for non-invasive, high accuracy recognition of gestures for interactive control. In Tomo 2, we introduce a new system that offers improved sampling speed and resolution. This, in turn, enables superior interior reconstruction and gesture recognition. More importantly, we use our new system as a vehicle for experimentation – we compare two EIT sensing methods and three different electrode resolutions. Results from in-depth empirical evaluations and a user study shed light on the future feasibility of EIT for sensing human input.</p><p><a href="/s/Tomo2.pdf">Download paper</a>.</p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/6a8q7HON4_c?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/58209e84e6f2e1714cf6d9b1/5820a5179de4bb43a9c5a7e6/1478535888777/Tomo2.png" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/58209e84e6f2e1714cf6d9b1/5820a5179de4bb43a9c5a7e6/1478535888777/Tomo2.png" data-image-dimensions="398x398" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Zhang, Y., Xiao, R., and Harrison, C. 2016. Advancing Hand Gesture Recognition with High Resolution Electrical Impedance Tomography. To appear in Proceedings of the 29th Annual ACM Symposium on User interface Software and Technology (Tokyo, Japan, October 16 - 19, 2016). UIST '16. ACM, New York, NY.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/skintrack-2016/" >
      
        <div class="project-meta">
          <h2 class="project-title">SkinTrack (2016)</h2>
          <div class="project-description"><p>SkinTrack enables continuous touch tracking on the skin. It consists of a ring, which emits a continuous high frequency AC signal, and a sensing wristband with multiple electrodes. SkinTrack measures phase differences to compute a 2D finger touch coordinate. Our approach is compact, non-invasive, low-cost and low-powered. We envision the technology being integrated into future smartwatches, supporting rich touch interactions beyond the confines of the small touchscreen.</p><p><a href="/s/skintrack.pdf">Download paper</a>.</p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/9hu8MNuvCHE?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/572bed7db654f95c0056bc93/572bed96b654f95c0056bd0b/1462496864423/Thumbnail.jpg" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/572bed7db654f95c0056bc93/572bed96b654f95c0056bd0b/1462496864423/Thumbnail.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Zhang, Y., Zhou, J., Laput, G., Harrison, C. SkinTrack: Using the Body as an Electrical Waveguide for Continuous Finger Tracking on the SkinIn Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16). ACM, New York.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/sweepsense/" >
      
        <div class="project-meta">
          <h2 class="project-title">SweepSense (2016)</h2>
          <div class="project-description"><p>We use speakers and microphones already present in a wide variety of devices to open new sensing opportunities. Our technique sweeps through a range of inaudible frequencies and measures the intensity of reflected sound to deduce information about the immediate environment, chiefly the materials and geometry of proximate surfaces. We offer several example uses, two of which we implemented as self-contained demos.</p><p><a href="http://www.gierad.com/">Download Paper</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/iPRdL9K-ZQQ?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;
" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/572beafdf8baf35017577c20/572bec3a2b8dded39ca866a8/1462496985996/02_Right_Out.jpg" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/572beafdf8baf35017577c20/572bec3a2b8dded39ca866a8/1462496985996/02_Right_Out.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Laput, G., Xiao, A.C., Harrison, C.&nbsp;SweepSense: Ad-Hoc Configuration Sensing Using Swept-Frequency Ultrasonics. In Proceedings of the ACM International Conference on Intelligent User Interfaces (IUI ‘16). ACM, New York, NY.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/fingerpose/" >
      
        <div class="project-meta">
          <h2 class="project-title">FingerPose (2015)</h2>
          <div class="project-description"><p>A new method that estimates a finger’s angle relative to the screen. &nbsp;Our approach works in tandem with conventional multitouch finger tracking, offering two additional analog degrees of freedom for a single touch point. We prototyped our solution on two platforms—&nbsp;a smartphone and smartwatch—each fully self-contained and operating in real-time.&nbsp;</p><p><a href="http://chrisharrison.net/projects/3dfingerangle/Qeexo3DFingerAngle.pdf">Download Paper</a><br /><a target="_blank" href="http://chrisharrison.net/index.php/Research/3DFingerAngle">More Info</a></p><p><strong>Research conducted at:</strong><br /><a target="_blank" href="http://www.qeexo.com">Qeexo</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/hLYBEBJHFYY?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8811e4b0e5a6e497ce53/564b8869e4b080e7107d4dd4/1450895641888/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8811e4b0e5a6e497ce53/564b8869e4b080e7107d4dd4/1450895641888/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Xiao, R. Schwarz, J. and Harrison, C. Estimating 3D Finger Angle on Commodity Touchscreens. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (ITS ‘15). ACM, New York, NY.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/capauth/" >
      
        <div class="project-meta">
          <h2 class="project-title">CapAuth (2015)</h2>
          <div class="project-description"><p>A technique that uses existing, low-level touchscreen data, combined with machine learning classifiers, to provide real-time authentication and even identification of users. Our user study demonstrates twenty-participant authentication accuracies of 99.6%. For twenty-user identification, our software achieved 94.0% accuracy and 98.2% on groups of four, simulating family use.</p><p><a href="http://chrisharrison.net/projects/capauth/CapAuth.pdf">Download Paper</a><br /><a target="_blank" href="http://chrisharrison.net/index.php/Research/CapAuth">More Info</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/jGH78KWH_yM?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b88aae4b0a01007d21a00/564b8904e4b0a6a54bed4434/1450895971957/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b88aae4b0a01007d21a00/564b8904e4b0a6a54bed4434/1450895971957/" data-image-dimensions="3957x3456" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Guo, A., Xiao, R. and Harrison, C. 2015. CapAuth: Identifying and Differentiating User Handprints on Commodity Capacitive Touchscreens. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (Madeira, Portugal, November 15 - 18, 2015). ITS ‘15. ACM, New York, NY. 59-62.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/gazegesture/" >
      
        <div class="project-meta">
          <h2 class="project-title">Gaze+Gesture (2015)</h2>
          <div class="project-description"><p>By fusing gaze and gesture into a unified and fluid interaction modality, we can enable rapid, precise and expressive free-space interactions that mirror natural use. Although both approaches are independently poor for pointing tasks, combining them can achieve pointing performance superior to either method alone. This opens new interaction opportunities for gaze and gesture systems alike.</p><p><a href="http://chrisharrison.net/projects/gazegesture/GazePlusGesture.pdf">Download Paper</a><br /><a target="_blank" href="http://chrisharrison.net/index.php/Research/GazePlusGesture">More Info</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/IjV2D69Gpio?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8929e4b066933563ba73/564b89a0e4b00c2918a10d1a/1450896179093/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8929e4b066933563ba73/564b89a0e4b00c2918a10d1a/1450896179093/" data-image-dimensions="1800x1200" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Chatterjee, I., Xiao, R. and Harrison, C. 2015. Gaze+Gesture: Expressive, Precise and Targeted Free-Space Interactions. In Proceedings of the 17th ACM International Conference on Multimodal Interaction (Seattle, Washington, November 9 - 13, 2015). ICMI '15. ACM, New York, NY. 131-138.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/emsense/" >
      
        <div class="project-meta">
          <h2 class="project-title">EM-Sense (2015)</h2>
          <div class="project-description"><p>A sensing technology that allows a smartwatch to know what object the user is touching. &nbsp;When the user operates an electrical or electro-mechanical object, the electro-magnetic signals (EM) propagate through the user. &nbsp;These characteristic signals flow through the user and detected by the watch, which can be used for on-touch object detection.</p><p><a href="http://www.disneyresearch.com/wp-content/uploads/EMSense-Recognizing-Handled-Uninstrumented-Electro-Mechanical-Objects-Using-Software-Defined-Radio-Paper.pdf">Download Paper</a><br /><a href="http://gierad.com/projects/emsense">More Info on Gierad's Webpage</a></p><p><strong>In collaboration with:</strong><br />Disney Research</p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/fpKDNle6ia4?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b5854e4b0c59293140754/564b58f4e4b02250913fc8b8/1450896343070/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b5854e4b0c59293140754/564b58f4e4b02250913fc8b8/1450896343070/" data-image-dimensions="2136x1536" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Laput, G., Yang, C. Xiao, R, Sample, A. and Harrison, C. 2015. EM-Sense: Touch Recognition of Uninstrumented, Electrical and Electromechanical Objects. In Proceedings of the 28th Annual ACM Symposium on User interface Software and Technology (Charlotte, North Carolina, November 8 - 11, 2015). UIST '15. ACM, New York, NY. 157-166.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/tomo/" >
      
        <div class="project-meta">
          <h2 class="project-title">Tomo (2015)</h2>
          <div class="project-description"><p>Tomo is a wearable, low-cost system using Electrical Impedance Tomography (EIT) to recover the interior impedance geometry of a user’s arm. &nbsp;We ultimately envision this technique being integrated into future smartwatches, allowing hand gestures and direct touch manipulation to work synergistically to support interactive tasks on small screens.&nbsp;</p><p><a href="http://www.yang-zhang.me/paper/Tomo.pdf">Download Paper</a><br /><a href="http://chrisharrison.net/index.php/Research/Tomo">More Info</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/N9c4hINa2Bk?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b604ae4b09ca8e659e1b6/564b6146e4b041eae9e3d21d/1450896680522/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b604ae4b09ca8e659e1b6/564b6146e4b041eae9e3d21d/1450896680522/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Zhang, Y. and Harrison, C. 2015. Tomo: Wearable, Low-Cost, Electrical Impedance Tomography for Hand Gesture Recognition. In Proceedings of the 28th Annual ACM Symposium on User interface Software and Technology (Charlotte, North Carolina, November 8 - 11, 2015). UIST '15. ACM, New York, NY. 167-173.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/furbrication/" >
      
        <div class="project-meta">
          <h2 class="project-title">3D-Printed Hair (2015)</h2>
          <div class="project-description"><p>A technique for 3D printing hair, fibers and bristles, by exploiting the stringing phenomena inherent in 3D printers using fused deposition modeling. This technique extends the capabilities of 3D printing in a new and interesting way, without requiring any new hardware</p><p><a href="http://gierad.com/assets/3dprintedhair/furbrication.pdf">Download Paper</a><br /><a href="http://www.gierad.com/projects/furbrication/">More Info</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/FB9YZLO0-AI?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b7cc2e4b05d8b3373fac9/564b7d44e4b0669335631608/1450821114446/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b7cc2e4b05d8b3373fac9/564b7d44e4b0669335631608/1450821114446/" data-image-dimensions="2000x1334" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Laput, G., Chen, X. and Harrison, C. 3D Printed Hair: Fused Deposition Modeling of Soft Strands, Fibers and Bristles. In Proceedings of the 28th Annual ACM Symposium on User interface Software and Technology (UIST '15). ACM, New York, NY.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/zensors/" >
      
        <div class="project-meta">
          <h2 class="project-title">Zensors (2015)</h2>
          <div class="project-description"><p>Zensors is a new sensing approach that fuses real-time human intelligence from online crowd workers with automatic approaches to provide robust, adaptive, and readily deployable intelligent sensors. With Zensors, users can go from question to live sensor feed in less than 60 seconds. Through our API, Zensors can enable a variety of rich end-user applications and moves us closer to the vision of responsive, intelligent environments.&nbsp;</p><p><a href="http://www.gierad.com/assets/zensors/zensors.pdf">Download Paper</a><br /><a href="http://www.gierad.com/projects/zensors/">More Info at Gierad's Webpage</a></p><p><strong>Sponsor:</strong><br />Yahoo! InMind</p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/VVP9emuFsQI?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b7f2ce4b053665f716f80/564b7fbde4b053665f71776c/1450897315708/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b7f2ce4b053665f716f80/564b7fbde4b053665f71776c/1450897315708/" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Laput, G., Lasecki, W., Wiese, J., Xiao, R., Bigham, J. and Harrison, C. 2015. Zensors: Adaptive, Rapidly Deployable, Human-Intelligent Sensor Feeds. In Proceedings of the 33nd Annual SIGCHI Conference on Human Factors in Computing Systems (CHI '15). 1935-1944.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/new-gallery/" >
      
        <div class="project-meta">
          <h2 class="project-title">Acoustruments (2015)</h2>
          <div class="project-description"><p>Acoustruments are low-cost, passive, and powerless mechanisms, made from plastic, that can bring tangible functionality to handheld devices. The operational principles were inspired by wind instruments, which produce expressive musical output despite being simple in physical design. Through a structured exploration, we built an expansive vocabulary of design primitives, providing building blocks for the construction of tangible interfaces utilizing smartphonesʼ existing audio functionality (the speaker and microphone).&nbsp;</p><p><a href="http://www.gierad.com/assets/acoustruments/acoustruments.pdf">Download Paper</a><br /><a href="http://www.gierad.com/projects/acoustruments/">More Info at Gierad's Webpage</a></p><p><strong>In collaboration with:</strong><br />Disney Research</p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/llOKDcr1gsY?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8005e4b053665f717ab5/564b8060e4b065c2f5d5d70d/1450897470199/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8005e4b053665f717ab5/564b8060e4b065c2f5d5d70d/1450897470199/" data-image-dimensions="1920x1280" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Laput, G., Brockmeyer, E., Hudson, S. and Harrison, C. 2015. Acoustruments: Passive, Acoustically-Driven, Interactive Controls for Handheld Devices. In Proceedings of the 33nd Annual SIGCHI Conference on Human Factors in Computing Systems (Seoul, Korea, April 18 - 23, 2015). CHI '15. ACM, New York, NY. 2161-2170.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/skinbuttons/" >
      
        <div class="project-meta">
          <h2 class="project-title">Skin Buttons (2014)</h2>
          <div class="project-description"><p>Tiny projectors integrated into the smartwatch to render icons on the user’s skin. These icons can be made touch sensitive, significantly expanding the interactive region without increasing device size. Through a series of experiments, we show that these “skin buttons” can have high touch accuracy and recognizability, while being low cost and power-efficient.</p><p><a href="http://www.gierad.com/assets/skinbuttons/skinbuttons.pdf">Download Paper</a><br /><a href="http://www.gierad.com/projects/skinbuttons/">More Info</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/OJVBuI3LFqU?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b80cee4b065df532cad0f/564b8142e4b05f4c07c6c1cf/1450897799296/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b80cee4b065df532cad0f/564b8142e4b05f4c07c6c1cf/1450897799296/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Laput, G., Xiao, R., Chen, X., Hudson, S. and Harrison, C. 2014. Skin Buttons: Cheap, Small, Low-Power and Clickable Fixed-Icon Laser Projections. In Proceedings of the 27th Annual ACM Symposium on User interface Software and Technology (Honolulu, Hawaii, October 5 - 8, 2014). UIST '14. ACM, New York, NY. 389-394.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/airplustouch/" >
      
        <div class="project-meta">
          <h2 class="project-title">Air+Touch (2014)</h2>
          <div class="project-description"><p>Air+Touch is a new class of interactions that interweave touch events with in-air gestures, offering a unified input modality with expressiveness greater than each input modality alone. We demonstrate how air and touch are highly complementary: touch is used to designate targets and segment in-air gestures, while in-air gestures add expressivity to touch events.&nbsp;</p><p><a href="http://chrisharrison.net/projects/airplustouch/airplustouchCMU.pdf">Download Paper</a><br /><a href="http://chrisharrison.net/index.php/Research/AirPlusTouch">More </a><a href="#">Info</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe mozallowfullscreen allowfullscreen src=&quot;https://player.vimeo.com/video/92972949?wmode=opaque&quot; webkitallowfullscreen frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b827ee4b0669335635941/564b83a9e4b04cb84dafaa87/1452484940165/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b827ee4b0669335635941/564b83a9e4b04cb84dafaa87/1452484940165/" data-image-dimensions="750x500" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Chen, X., Schwarz, J. Harrison, C., Mankoff, J. and Hudson, S. 2014. Air+Touch: Interweaving Touch &amp; In-Air Gestures. In Proceedings of the 27th Annual ACM Symposium on User interface Software and Technology (Honolulu, Hawaii, October 5 - 8, 2014). UIST '14. ACM, New York, NY. 519-525.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/teslatouch/" >
      
        <div class="project-meta">
          <h2 class="project-title">Toffee (2014)</h2>
          <div class="project-description"><p>Toffee is a sensing approach that extends touch interaction beyond the small confines of a mobile device and onto ad hoc adjacent surfaces, most notably tabletops. This is achieved using a novel application of acoustic time differences of arrival (TDOA) correlation. This enables radial interactions in an area many times larger than a mobile device.</p><p><a href="http://chrisharrison.net/projects/toffee/ToffeeCMU.pdf">Download Paper</a><br /><a href="http://chrisharrison.net/index.php/Research/Toffee">More I</a><a href="http://chrisharrison.net/index.php/Research/Toffee">nfo</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/_Edph1kb4ck?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b83fbe4b065df532cd5b6/564b8460e4b053665f71b29e/1450898642292/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b83fbe4b065df532cd5b6/564b8460e4b053665f71b29e/1450898642292/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Xiao, R., Lew, G., Marsanico, J., Hariharan, D., Hudson, S., and Harrison, C. 2014. Toffee: Enabling Ad Hoc, Around-Device Interaction with Acoustic Time-of-Arrival Correlation. In Proceedings of the 16th International Conference on Human-Computer Interaction with Mobile Devices and Services (Toronto, Canada, September 23 - 26, 2014). MobileHCI ’14. ACM, New York, NY. 67-76.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/touchtools/" >
      
        <div class="project-meta">
          <h2 class="project-title">TouchTools (2014)</h2>
          <div class="project-description"><p>We propose that touch gesture design be inspired by the manipulation of physical tools from the real world. In this way, we can leverage user familiarity and fluency with such tools to build a rich set of gestures for touch interaction. With only a few minutes of training on a proof-of-concept system, users were able to summon a variety of virtual tools by replicating their corresponding real-world grasps.&nbsp;</p><p><a href="http://chrisharrison.net/projects/touchtools/TouchTools.pdf">Download Paper</a><br /><a href="http://chrisharrison.net/index.php/Research/Touchtools">More Info</a></p><p><strong>Commercialized by:</strong><br />Qeexo</p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/N8s8NJf34fM?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8492e4b0ca87c62d9b6a/564b84d3e4b04f83f2f0ffbb/1450821705088/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8492e4b0ca87c62d9b6a/564b84d3e4b04f83f2f0ffbb/1450821705088/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Harrison, C., Xiao, R., Schwarz, J., and Hudson, S. TouchTools: Leveraging Familiarity and Skill with Physical Tools to Augment Touch Interaction. In Proceedings of the 32nd Annual SIGCHI Conference on Human Factors in Computing Systems (Toronto, Canada, April 26 - May 1, 2014). CHI '14. ACM, New York, NY. 2913-2916.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/smartwatch-5dof/" >
      
        <div class="project-meta">
          <h2 class="project-title">Smartwatch 5DOF (2014)</h2>
          <div class="project-description"><p>We propose using the face of a smartwatch as a multi-degree-of-freedom mechanical interface. This enables rich interaction without occluding the screen with fingers, and can operate in concert with touch interaction and physical buttons. We developed a series of example applications, many of which are cumbersome – or even impossible – on today’s smartwatch devices.</p><p><a href="http://www.gierad.com/assets/smartwatchface/smartwatchface.pdf">Download Paper</a><br /><a href="http://www.gierad.com/projects/smartwatch5dof/">More Info</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/rLQtqTpZBOU?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8508e4b04f83f2f10272/564b8580e4b04cb84dafc371/1450899058596/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8508e4b04f83f2f10272/564b8580e4b04cb84dafc371/1450899058596/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Xiao, R., Laput, G., and Harrison, C. Expanding the Input Expressivity of Smartwatches with Mechanical Pan, Twist, Tilt and Click. In Proceedings of 32nd Annual SIGCHI Conference on Human Factors in Computing Systems (Toronto, Canada, April 26 - May 1, 2014). CHI '14. ACM, New York, NY. 193-196.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/tapsense/" >
      
        <div class="project-meta">
          <h2 class="project-title">TapSense (2011)</h2>
          <div class="project-description"><p>TapSense enables touchscreens to know how users are touching the screen - with finger tips, knuckles and nails, or even a passive stylus. TapSense moves beyond just counting the number of fingers on the screen, revolutionizing the way we interact with touch-enabled devices. By distinguishing between different parts of the hand, TapSense takes the pain out of performing these actions, making mobile devices faster and easier to use than ever.</p><p><a href="http://chrisharrison.net/projects/tapsense/tapsense.pdf">Download Paper</a><br /><a href="http://chrisharrison.net/index.php/Research/TapSense">More Info</a></p><p><strong>Commercialized by:</strong><br /><a target="_blank" href="http://www.qeexo.com">Qeexo</a></p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/-oN96cucBr4?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b85dfe4b0ca87c62dabe9/564b862de4b0a2e58a09edef/1450821958887/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b85dfe4b0ca87c62dabe9/564b862de4b0a2e58a09edef/1450821958887/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Harrison, C., Schwarz, J. and Hudson S. E. 2011. TapSense: Enhancing Finger Interaction on Touch Surfaces. In Proceedings of the 24th Annual ACM Symposium on User interface Software and Technology. UIST '11. ACM, New York, NY. 627-636.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
      <div class="project gallery-project" data-url="/teslatouch-1/" >
      
        <div class="project-meta">
          <h2 class="project-title">TeslaTouch (2010)</h2>
          <div class="project-description"><p>TeslaTouch brings rich, dynamic physical feedback to otherwise flat, featureless touchscreens. The technology is based on the electrovibration principle, which can programmatically vary the electrostatic friction between fingers and a touch panel. When combined with an interactive graphical display, this approach enables touch experiences with rich textures and physical affordances.</p><p><a href="http://chrisharrison.net/projects/teslatouch/teslatouchUIST2010.pdf">Download Paper</a><br /><a href="http://www.disneyresearch.com/project/teslatouch/">More Info at Disney Research</a></p><p><strong>In collaboration with:</strong><br />Disney Research</p></div>
        </div>
        <div class="image-list">
        

          <div class="image">
            
            <div class="sqs-video-wrapper"  data-load="false" data-html="&lt;iframe allowfullscreen src=&quot;https://www.youtube.com/embed/3l3MDNZk-3I?wmode=opaque&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;" data-provider-name=""><div class="sqs-video-overlay" style="opacity: 0;"><img data-load="false" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8681e4b0a2e58a09f286/564b86c2e4b0f792373d716b/1450822112263/" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/564b8681e4b0a2e58a09f286/564b86c2e4b0f792373d716b/1450822112263/" data-image-dimensions="1000x350" data-image-focal-point="0.5,0.5" /><div class="sqs-video-opaque"> </div><div class="sqs-video-icon"></div></div></div>
            <div class="image-meta">
              
              <span class="image-desc"><p>Bau, O., Poupyrev, I., Israr, A., and Harrison, C. 2010. TeslaTouch: Electrovibration for Touch Surfaces. In Proceedings of the 23rd Annual ACM Symposium on User interface Software and Technology (New York, New York, October 3 - 6, 2010). UIST '10. ACM, New York, NY. 283-292.</p></span>
            </div>
          </div>
        
        </div>
      
      </div>
    
  

  <div class="project-controls">
    <div>
      <div id="projectNav"><span class="prev-project">prev</span> / <span class="next-project">next</span></div>
      <a href="#">Back to Research</a>
    </div>
  </div>

</div>



<div id="projectThumbs" >
  <div class="wrapper">
    
      
        <a class="project "href="/desktopography/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5a1ecd73f9619afa6aeb95d5/1511968115190/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5a1ecd73f9619afa6aeb95d5/1511968115190/" data-image-dimensions="1761x805" data-image-focal-point="0.5,0.5" alt="Desktopography: Supporting Responsive Cohabitation Between Virtual Interfaces and Physical Objects"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5a1ecd73f9619afa6aeb95d5/1511968115190/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Desktopography (2017)</div>
          </div>
        </a>
      
        <a class="project "href="/synthetic-sensors-2017/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b66a017bffcb364fa0a4f/1494967968936/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b66a017bffcb364fa0a4f/1494967968936/" data-image-dimensions="2500x1667" data-image-focal-point="0.5,0.5" alt=" Laput, G., Zhang, Y. and Harrison, C. 2017. Synthetic Sensors: Towards General-Purpose Sensing. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 3986-3999. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b66a017bffcb364fa0a4f/1494967968936/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Synthetic Sensors (2017)</div>
          </div>
        </a>
      
        <a class="project "href="/electrick-2017/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591c67d486e6c092bca477fd/1495033812433/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591c67d486e6c092bca477fd/1495033812433/" data-image-dimensions="2500x1400" data-image-focal-point="0.5,0.5" alt=" Zhang, Y., Laput, G. and Harrison, C. 2017. Electrick: Low-Cost Touch Sensing Using Electric Field Tomography. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 1-14. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591c67d486e6c092bca477fd/1495033812433/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Electrick (2017)</div>
          </div>
        </a>
      
        <a class="project "href="/deus-em-machina-2017/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b64d5893fc0b4477a6f5c/1494967509672/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b64d5893fc0b4477a6f5c/1494967509672/" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" alt=" Xiao, R., Laput, G., Zhang, Y. and Harrison, C. 2017. Deus EM Machina: On-Touch Contextual Functionality for Smart IoT Appliances. In Proceedings of the 35th Annual SIGCHI Conference on Human Factors in Computing Systems (Denver, Colorado, USA, May 6 - 11, 2017). CHI '17. ACM, New York, NY. 4000-4008. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b64d5893fc0b4477a6f5c/1494967509672/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Deus EM Machina (2017)</div>
          </div>
        </a>
      
        <a class="project "href="/direct-2016/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b63ec1b10e3072d05dce7/1494967281686/compare.jpg" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b63ec1b10e3072d05dce7/1494967281686/compare.jpg" data-image-dimensions="2000x2050" data-image-focal-point="0.5,0.5" alt="compare.jpg"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b63ec1b10e3072d05dce7/1494967281686/compare.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">DIRECT (2016)</div>
          </div>
        </a>
      
        <a class="project "href="/capcam-2016/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b62d2cd0f68313a2b88bc/1494967016474/kbd.jpg" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b62d2cd0f68313a2b88bc/1494967016474/kbd.jpg" data-image-dimensions="2371x1200" data-image-focal-point="0.5,0.5" alt="kbd.jpg"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/591b62d2cd0f68313a2b88bc/1494967016474/kbd.jpg?format=500w"></noscript></div></div><div class="project-item-count">2</div></div>
            <div class="project-title">CapCam (2016)</div>
          </div>
        </a>
      
        <a class="project "href="/viband-2016/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5820aa8be3df287bcc9434be/1478535820737/VideoThumbnail.jpg" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5820aa8be3df287bcc9434be/1478535820737/VideoThumbnail.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" alt="VideoThumbnail.jpg"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5820aa8be3df287bcc9434be/1478535820737/VideoThumbnail.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">ViBand (2016)</div>
          </div>
        </a>
      
        <a class="project "href="/aurasense-2016/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5820a72fb3db2b4ee66d1df4/1478534960032/thumbnail.jpg" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5820a72fb3db2b4ee66d1df4/1478534960032/thumbnail.jpg" data-image-dimensions="668x676" data-image-focal-point="0.5,0.5" alt="thumbnail.jpg"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5820a72fb3db2b4ee66d1df4/1478534960032/thumbnail.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">AuraSense (2016)</div>
          </div>
        </a>
      
        <a class="project "href="/tomo2/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5820aaf31b631b164240988e/1478535925172/Tomo2.png" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5820aaf31b631b164240988e/1478535925172/Tomo2.png" data-image-dimensions="398x398" data-image-focal-point="0.5,0.5" alt="Tomo2.png"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5820aaf31b631b164240988e/1478535925172/Tomo2.png?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Tomo 2 (2016)</div>
          </div>
        </a>
      
        <a class="project "href="/skintrack-2016/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/573f87dcb654f953eb26c29e/1463781348497/k.JPG" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/573f87dcb654f953eb26c29e/1463781348497/k.JPG" data-image-dimensions="5184x2912" data-image-focal-point="0.5,0.5" alt="k.JPG"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/573f87dcb654f953eb26c29e/1463781348497/k.JPG?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">SkinTrack (2016)</div>
          </div>
        </a>
      
        <a class="project "href="/sweepsense/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/572beedac2ea515568e08fe0/1462496086881/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/572beedac2ea515568e08fe0/1462496086881/" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" alt=""  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/572beedac2ea515568e08fe0/1462496086881/?format=500w"></noscript></div></div><div class="project-item-count">2</div></div>
            <div class="project-title">SweepSense (2016)</div>
          </div>
        </a>
      
        <a class="project "href="/fingerpose/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567ae919d82d5ed06c138dcf/1450895641888/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567ae919d82d5ed06c138dcf/1450895641888/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" alt=" Xiao, R. Schwarz, J. and Harrison, C. Estimating 3D Finger Angle on Commodity Touchscreens. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (ITS ‘15). ACM, New York, NY. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567ae919d82d5ed06c138dcf/1450895641888/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">FingerPose (2015)</div>
          </div>
        </a>
      
        <a class="project "href="/capauth/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aea637086d7d3f8037107/1450895971957/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aea637086d7d3f8037107/1450895971957/" data-image-dimensions="3957x3456" data-image-focal-point="0.5,0.5" alt=" Guo, A., Xiao, R. and Harrison, C. 2015. CapAuth: Identifying and Differentiating User Handprints on Commodity Capacitive Touchscreens. In Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces (Madeira, Portugal, November 15 - 18, 2015). ITS ‘15. ACM, New York, NY. 59-62. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aea637086d7d3f8037107/1450895971957/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">CapAuth (2015)</div>
          </div>
        </a>
      
        <a class="project "href="/gazegesture/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aeb33d8af10452c77a013/1450896179093/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aeb33d8af10452c77a013/1450896179093/" data-image-dimensions="1800x1200" data-image-focal-point="0.5,0.5" alt=" Chatterjee, I., Xiao, R. and Harrison, C. 2015. Gaze+Gesture: Expressive, Precise and Targeted Free-Space Interactions. In Proceedings of the 17th ACM International Conference on Multimodal Interaction (Seattle, Washington, November 9 - 13, 2015). ICMI '15. ACM, New York, NY. 131-138. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aeb33d8af10452c77a013/1450896179093/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Gaze+Gesture (2015)</div>
          </div>
        </a>
      
        <a class="project "href="/emsense/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aebd70ab377dd6039a5dc/1450896343070/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aebd70ab377dd6039a5dc/1450896343070/" data-image-dimensions="2136x1536" data-image-focal-point="0.5,0.5" alt=" Laput, G., Yang, C. Xiao, R, Sample, A. and Harrison, C. 2015. EM-Sense: Touch Recognition of Uninstrumented, Electrical and Electromechanical Objects. In Proceedings of the 28th Annual ACM Symposium on User interface Software and Technology (Charlotte, North Carolina, November 8 - 11, 2015). UIST '15. ACM, New York, NY. 157-166. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aebd70ab377dd6039a5dc/1450896343070/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">EM-Sense (2015)</div>
          </div>
        </a>
      
        <a class="project "href="/tomo/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aed9a7086d7d3f8038b5b/1450896804713/IMG_0259.jpg" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aed9a7086d7d3f8038b5b/1450896804713/IMG_0259.jpg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" alt="IMG_0259.jpg"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aed9a7086d7d3f8038b5b/1450896804713/IMG_0259.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Tomo (2015)</div>
          </div>
        </a>
      
        <a class="project "href="/furbrication/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aeea7cbced61f5a81fe02/1450897063806/horse.jpg" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aeea7cbced61f5a81fe02/1450897063806/horse.jpg" data-image-dimensions="920x614" data-image-focal-point="0.5,0.5" alt="horse.jpg"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aeea7cbced61f5a81fe02/1450897063806/horse.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">3D-Printed Hair (2015)</div>
          </div>
        </a>
      
        <a class="project "href="/zensors/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aefb669492ea506c919a8/1450897339025/zensors3.jpg" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aefb669492ea506c919a8/1450897339025/zensors3.jpg" data-image-dimensions="1920x1080" data-image-focal-point="0.5,0.5" alt="zensors3.jpg"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567aefb669492ea506c919a8/1450897339025/zensors3.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Zensors (2015)</div>
          </div>
        </a>
      
        <a class="project "href="/new-gallery/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af03ea12f444c58f15a98/1450897470199/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af03ea12f444c58f15a98/1450897470199/" data-image-dimensions="1920x1280" data-image-focal-point="0.5,0.5" alt=" Laput, G., Brockmeyer, E., Hudson, S. and Harrison, C. 2015. Acoustruments: Passive, Acoustically-Driven, Interactive Controls for Handheld Devices. In Proceedings of the 33nd Annual SIGCHI Conference on Human Factors in Computing Systems (Seoul, Korea, April 18 - 23, 2015). CHI '15. ACM, New York, NY. 2161-2170. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af03ea12f444c58f15a98/1450897470199/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Acoustruments (2015)</div>
          </div>
        </a>
      
        <a class="project "href="/skinbuttons/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af18757eb8d11d1839245/1450897799296/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af18757eb8d11d1839245/1450897799296/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" alt=" Laput, G., Xiao, R., Chen, X., Hudson, S. and Harrison, C. 2014. Skin Buttons: Cheap, Small, Low-Power and Clickable Fixed-Icon Laser Projections. In Proceedings of the 27th Annual ACM Symposium on User interface Software and Technology (Honolulu, Hawaii, October 5 - 8, 2014). UIST '14. ACM, New York, NY. 389-394. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af18757eb8d11d1839245/1450897799296/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Skin Buttons (2014)</div>
          </div>
        </a>
      
        <a class="project "href="/airplustouch/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af300df40f39145353760/1450898179250/generic.jpg" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af300df40f39145353760/1450898179250/generic.jpg" data-image-dimensions="2575x1716" data-image-focal-point="0.5,0.5" alt="generic.jpg"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af300df40f39145353760/1450898179250/generic.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Air+Touch (2014)</div>
          </div>
        </a>
      
        <a class="project "href="/teslatouch/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af49ba2bab88cc7e79498/1450898593854/toffee.jpg" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af49ba2bab88cc7e79498/1450898593854/toffee.jpg" data-image-dimensions="3456x3456" data-image-focal-point="0.5,0.5" alt="toffee.jpg"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af49ba2bab88cc7e79498/1450898593854/toffee.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Toffee (2014)</div>
          </div>
        </a>
      
        <a class="project "href="/touchtools/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5679c849a12f445fc5bbd3ab/1450821705088/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5679c849a12f445fc5bbd3ab/1450821705088/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" alt=" Harrison, C., Xiao, R., Schwarz, J., and Hudson, S. TouchTools: Leveraging Familiarity and Skill with Physical Tools to Augment Touch Interaction. In Proceedings of the 32nd Annual SIGCHI Conference on Human Factors in Computing Systems (Toronto, Canada, April 26 - May 1, 2014). CHI '14. ACM, New York, NY. 2913-2916. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5679c849a12f445fc5bbd3ab/1450821705088/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">TouchTools (2014)</div>
          </div>
        </a>
      
        <a class="project "href="/smartwatch-5dof/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af6dcbfe8733d31fe8371/1450899180155/IMG_3566+X.jpg" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af6dcbfe8733d31fe8371/1450899180155/IMG_3566+X.jpg" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" alt="IMG_3566 X.jpg"  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/567af6dcbfe8733d31fe8371/1450899180155/IMG_3566+X.jpg?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">Smartwatch 5DOF (2014)</div>
          </div>
        </a>
      
        <a class="project "href="/tapsense/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5679c946b204d58613bae3b6/1450821958887/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5679c946b204d58613bae3b6/1450821958887/" data-image-dimensions="5184x3456" data-image-focal-point="0.5,0.5" alt=" Harrison, C., Schwarz, J. and Hudson S. E. 2011. TapSense: Enhancing Finger Interaction on Touch Surfaces. In Proceedings of the 24th Annual ACM Symposium on User interface Software and Technology. UIST '11. ACM, New York, NY. 627-636. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5679c946b204d58613bae3b6/1450821958887/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">TapSense (2011)</div>
          </div>
        </a>
      
        <a class="project "href="/teslatouch-1/" >
          <div>
            <div class="project-image"><div class="intrinsic"><div class="content-fill"><img data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5679c9e069a91af2c88a53b6/1450822112263/" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5679c9e069a91af2c88a53b6/1450822112263/" data-image-dimensions="1000x350" data-image-focal-point="0.5,0.5" alt=" Bau, O., Poupyrev, I., Israr, A., and Harrison, C. 2010. TeslaTouch: Electrovibration for Touch Surfaces. In Proceedings of the 23rd Annual ACM Symposium on User interface Software and Technology (New York, New York, October 3 - 6, 2010). UIST '10. ACM, New York, NY. 283-292. "  data-load="false"/><noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5679c9e069a91af2c88a53b6/1450822112263/?format=500w"></noscript></div></div><div class="project-item-count">1</div></div>
            <div class="project-title">TeslaTouch (2010)</div>
          </div>
        </a>
      
    

  </div>
</div>
    </section>

    <div class="extra-wrapper page-footer">
      <div class="sqs-layout sqs-grid-12 columns-12" data-layout-label="Footer Content: Research" data-type="block-field" data-updated-on="1515097656775" id="page-footer-564b5638e4b0cfc22513bc9c"><div class="row sqs-row"><div class="col sqs-col-12 span-12"><div class="sqs-block horizontalrule-block sqs-block-horizontalrule" data-block-type="47" id="block-yui_3_17_2_15_1447816420419_5558"><div class="sqs-block-content"><hr /></div></div><div class="row sqs-row"><div class="col sqs-col-2 span-2"><div class="sqs-block image-block sqs-block-image" data-aspect-ratio="56.875" data-block-type="5" id="block-yui_3_17_2_24_1450453695280_16504"><div class="sqs-block-content">

  

  	
      <div class="image-block-outer-wrapper layout-caption-hidden design-layout-inline">
      
        <div class="intrinsic" style="max-width:1667.0px;">
          
            <a href="http://cmu.edu"  target="_blank">
          
            <div style="padding-bottom:56.875%;" class="image-block-wrapper   has-aspect-ratio" data-description="" >
              <noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5a4e8c790d9297b7533f8cbd/1515097224003/cmu2.png"  alt="cmu2.png"  /></noscript><img class="thumb-image" alt="cmu2.png" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5a4e8c790d9297b7533f8cbd/1515097224003/cmu2.png" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5a4e8c790d9297b7533f8cbd/1515097224003/cmu2.png" data-image-dimensions="1667x946" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a4e8c790d9297b7533f8cbd" data-type="image" />
            </div>
          
            </a>
          

          

        </div>
      
      </div>
    

  


</div></div></div><div class="col sqs-col-10 span-10"><div class="sqs-block image-block sqs-block-image sqs-col-2 span-2 float float-right" data-aspect-ratio="56.875" data-block-type="5" id="block-yui_3_17_2_24_1450453695280_17453"><div class="sqs-block-content">

  

  	
      <div class="image-block-outer-wrapper layout-caption-hidden design-layout-inline">
      
        <div class="intrinsic" style="max-width:1667.0px;">
          
            <a href="http://hcii.cmu.edu"  target="_blank">
          
            <div style="padding-bottom:56.875%;" class="image-block-wrapper   has-aspect-ratio" data-description="" >
              <noscript><img src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5a4e8c9071c10b1be8992c02/1515097245693/hcii2.png"  alt="hcii2.png"  /></noscript><img class="thumb-image" alt="hcii2.png" data-src="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5a4e8c9071c10b1be8992c02/1515097245693/hcii2.png" data-image="https://static1.squarespace.com/static/564b5107e4b087849452ea4b/t/5a4e8c9071c10b1be8992c02/1515097245693/hcii2.png" data-image-dimensions="1667x946" data-image-focal-point="0.5,0.5" data-load="false" data-image-id="5a4e8c9071c10b1be8992c02" data-type="image" />
            </div>
          
            </a>
          

          

        </div>
      
      </div>
    

  


</div></div><div class="sqs-block html-block sqs-block-html" data-block-type="2" id="block-yui_3_17_2_24_1450453695280_16023"><div class="sqs-block-content"><p class="text-align-center">Future Interfaces Group<br /><a target="_blank" href="http://hcii.cmu.edu">Human-Computer Interaction Institute</a><br /><a href="http://cs.cmu.edu">School of Computer Science</a>&nbsp;<br /><a href="http://cmu.edu">Carnegie Mellon University</a>&nbsp;</p></div></div></div></div></div></div></div>
    </div>

    <div class="page-divider"></div>

    <footer id="footer">
      <div class="sqs-layout sqs-grid-1 columns-1 empty" data-layout-label="Footer Content" data-type="block-field" data-updated-on="1450819064801" id="footerBlock"><div class="row sqs-row"><div class="col sqs-col-1 span-1"></div></div></div>
      
        
      
    </footer>

  </div>

  <div></div>

  <script type="text/javascript" src="https://static1.squarespace.com/static/ta/564b5107e4b087849452ea4b/0/scripts/combo/?dynamic-data.js&site.js"></script>
  

  <script type="text/javascript" data-sqs-type="imageloader-bootstraper">(function() {if(window.ImageLoader) { window.ImageLoader.bootstrap({}, document); }})();</script><script>Squarespace.afterBodyLoad(Y);</script>



</body>

</html>